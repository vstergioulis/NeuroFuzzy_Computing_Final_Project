{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45d7fcbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:34:24.903742Z",
     "start_time": "2024-02-10T14:34:24.897501Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer , PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "212369e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:34:28.616155Z",
     "start_time": "2024-02-10T14:34:27.984670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_level_1\n",
       "arts, culture, entertainment and media        300\n",
       "conflict, war and peace                       800\n",
       "crime, law and justice                        500\n",
       "disaster, accident and emergency incident     500\n",
       "economy, business and finance                 400\n",
       "education                                     607\n",
       "environment                                   600\n",
       "health                                        700\n",
       "human interest                                600\n",
       "labour                                        703\n",
       "lifestyle and leisure                         300\n",
       "politics                                      900\n",
       "religion and belief                           800\n",
       "science and technology                        800\n",
       "society                                      1100\n",
       "sport                                         907\n",
       "weather                                       400\n",
       "Name: data_id, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/news_class.csv\")\n",
    "\n",
    "df = data.copy() # to be safe and avoid errors\n",
    "\n",
    "df = df.loc[:,[\"data_id\" , \"content\" , \"category_level_1\" , \"category_level_2\"]]\n",
    "df.groupby(['category_level_1'])['data_id'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c58d90bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:34:29.693468Z",
     "start_time": "2024-02-10T14:34:29.689496Z"
    }
   },
   "outputs": [],
   "source": [
    "def labeler(dataframe_column):\n",
    "    encoder=LabelEncoder()\n",
    "    \n",
    "    labels = encoder.fit_transform(dataframe_column)\n",
    "    print(encoder.classes_)\n",
    "    print(\"We did it boys , labels have been created\")\n",
    "    \n",
    "    return(pd.DataFrame(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15441b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:35:05.001821Z",
     "start_time": "2024-02-10T14:35:04.989249Z"
    }
   },
   "outputs": [],
   "source": [
    "extra_stop = ['said', 'would','even','according','could','year','years','also','new','people','old,''one','two','time','first','last','say','make','best','get','three','make','year old','told','made','like','take','many','set','number','month','week','well','back' , 'post', 'http', 'www', 'presstv' , 'ir']\n",
    "total_stop = stopwords.words(\"english\") + extra_stop\n",
    "\n",
    "# Function for removing ASCII characters\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "# Function for converting to lower case\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function for removing stop words\n",
    "def remove_stop_words(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text =  tokenizer.tokenize(text)\n",
    "    stops = total_stop\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "# Function for removing html\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "# Function for removing punctuation\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def lemm_text(text):\n",
    "    lemm=WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    tokens = text\n",
    "    return ' '.join([lemm.lemmatize(t) for t in tokens])\n",
    "\n",
    "def text_stemmer(text):\n",
    "    stemmer=PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    tokens = text\n",
    "    return ' '.join([stemmer.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba881001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:38:55.324491Z",
     "start_time": "2024-02-10T14:37:04.435092Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Cleaned'] = df['content'].apply(_removeNonAscii)\n",
    "df['Cleaned'] = df.Cleaned.apply(func = make_lower_case)\n",
    "df['Cleaned'] = df.Cleaned.apply(func = remove_stop_words)\n",
    "df['Cleaned'] = df.Cleaned.apply(func = remove_punctuation)\n",
    "df['Cleaned'] = df.Cleaned.apply(func = remove_html)\n",
    "df['Cleaned'] = df.Cleaned.apply(func = text_stemmer)\n",
    "df = df.drop('content' , axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "617d05fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:39:50.679756Z",
     "start_time": "2024-02-10T14:39:50.666641Z"
    }
   },
   "outputs": [],
   "source": [
    "io = df.copy()\n",
    "\n",
    "io = io.drop([374]).reset_index(drop=True)\n",
    "io = io.drop([6527]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d3222676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:06.691523Z",
     "start_time": "2024-02-10T14:40:06.285422Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_full = []\n",
    "for words in io['Cleaned']:\n",
    "    corpus_full.append(words.split())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76df5979",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:07.476319Z",
     "start_time": "2024-02-10T14:40:07.469574Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorizer():\n",
    "    \n",
    "    \n",
    "    # Creating a list for storing the vectors ('Description' into vectors)\n",
    "    #global word_embeddings\n",
    "    word_embeddings = []\n",
    "    i = 0\n",
    "    # Reading the each 'Description'\n",
    "    for line in io['Cleaned']:\n",
    "        avgword2vec = None\n",
    "        count = 0\n",
    "        for word in line.split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                count += 1\n",
    "                if avgword2vec is None:\n",
    "                    avgword2vec = model.wv[word]\n",
    "                else:\n",
    "                    avgword2vec = avgword2vec + model.wv[word]\n",
    "                \n",
    "        if avgword2vec is not None:\n",
    "            avgword2vec = avgword2vec / count\n",
    "            word_embeddings.append(avgword2vec)\n",
    "        else:\n",
    "            print(\"I found it , the error occurs at line:\" , i)\n",
    "          \n",
    "        i +=1 \n",
    "\n",
    "    return(pd.DataFrame(word_embeddings))  # Returning our Data as a Dataframe (aesthetic reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62ad7b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:08.461220Z",
     "start_time": "2024-02-10T14:40:08.175203Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97fb4f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:16.393683Z",
     "start_time": "2024-02-10T14:40:08.853114Z"
    }
   },
   "outputs": [],
   "source": [
    "vect = vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8c89c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:23.810113Z",
     "start_time": "2024-02-10T14:40:23.784249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001459</td>\n",
       "      <td>-0.078473</td>\n",
       "      <td>-0.092210</td>\n",
       "      <td>0.052347</td>\n",
       "      <td>0.102376</td>\n",
       "      <td>-0.002754</td>\n",
       "      <td>-0.056390</td>\n",
       "      <td>0.037248</td>\n",
       "      <td>-0.053454</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058640</td>\n",
       "      <td>0.173090</td>\n",
       "      <td>-0.018010</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>0.258875</td>\n",
       "      <td>0.031823</td>\n",
       "      <td>-0.007939</td>\n",
       "      <td>-0.085078</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>-0.109643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072767</td>\n",
       "      <td>-0.063284</td>\n",
       "      <td>-0.092942</td>\n",
       "      <td>0.015908</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>-0.067774</td>\n",
       "      <td>0.043507</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.186417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022937</td>\n",
       "      <td>0.108182</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.064129</td>\n",
       "      <td>0.210500</td>\n",
       "      <td>0.040490</td>\n",
       "      <td>-0.036917</td>\n",
       "      <td>-0.046180</td>\n",
       "      <td>-0.024272</td>\n",
       "      <td>-0.064851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040674</td>\n",
       "      <td>0.019691</td>\n",
       "      <td>-0.046176</td>\n",
       "      <td>0.082028</td>\n",
       "      <td>0.135475</td>\n",
       "      <td>-0.008119</td>\n",
       "      <td>-0.007749</td>\n",
       "      <td>0.072441</td>\n",
       "      <td>-0.066983</td>\n",
       "      <td>0.221950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025967</td>\n",
       "      <td>0.072008</td>\n",
       "      <td>-0.051875</td>\n",
       "      <td>0.114976</td>\n",
       "      <td>0.170444</td>\n",
       "      <td>0.098256</td>\n",
       "      <td>-0.050679</td>\n",
       "      <td>-0.022611</td>\n",
       "      <td>-0.050753</td>\n",
       "      <td>-0.058318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100426</td>\n",
       "      <td>-0.135048</td>\n",
       "      <td>-0.067858</td>\n",
       "      <td>0.048585</td>\n",
       "      <td>0.099610</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>-0.076442</td>\n",
       "      <td>0.033414</td>\n",
       "      <td>-0.030683</td>\n",
       "      <td>0.204774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048312</td>\n",
       "      <td>0.127591</td>\n",
       "      <td>-0.027277</td>\n",
       "      <td>0.057416</td>\n",
       "      <td>0.185428</td>\n",
       "      <td>0.101191</td>\n",
       "      <td>-0.045935</td>\n",
       "      <td>-0.063575</td>\n",
       "      <td>-0.013838</td>\n",
       "      <td>-0.118549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040711</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.070330</td>\n",
       "      <td>0.057477</td>\n",
       "      <td>0.106131</td>\n",
       "      <td>-0.021223</td>\n",
       "      <td>-0.070114</td>\n",
       "      <td>0.045974</td>\n",
       "      <td>-0.020043</td>\n",
       "      <td>0.177833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067081</td>\n",
       "      <td>0.080117</td>\n",
       "      <td>-0.014415</td>\n",
       "      <td>0.098188</td>\n",
       "      <td>0.157175</td>\n",
       "      <td>0.078036</td>\n",
       "      <td>-0.014283</td>\n",
       "      <td>-0.087883</td>\n",
       "      <td>-0.085483</td>\n",
       "      <td>-0.048128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10910</th>\n",
       "      <td>0.072800</td>\n",
       "      <td>-0.021963</td>\n",
       "      <td>-0.076825</td>\n",
       "      <td>-0.089859</td>\n",
       "      <td>0.062972</td>\n",
       "      <td>-0.039061</td>\n",
       "      <td>-0.005116</td>\n",
       "      <td>-0.047534</td>\n",
       "      <td>-0.084299</td>\n",
       "      <td>0.020017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125978</td>\n",
       "      <td>0.118477</td>\n",
       "      <td>-0.050200</td>\n",
       "      <td>0.117858</td>\n",
       "      <td>0.248817</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>-0.001825</td>\n",
       "      <td>-0.123827</td>\n",
       "      <td>0.012053</td>\n",
       "      <td>-0.021778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10911</th>\n",
       "      <td>0.114847</td>\n",
       "      <td>-0.079585</td>\n",
       "      <td>-0.117073</td>\n",
       "      <td>-0.018464</td>\n",
       "      <td>0.158374</td>\n",
       "      <td>-0.127964</td>\n",
       "      <td>-0.093952</td>\n",
       "      <td>-0.101651</td>\n",
       "      <td>-0.051418</td>\n",
       "      <td>0.090127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107329</td>\n",
       "      <td>0.046495</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.071641</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.066662</td>\n",
       "      <td>-0.049829</td>\n",
       "      <td>-0.263125</td>\n",
       "      <td>-0.038419</td>\n",
       "      <td>0.042634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912</th>\n",
       "      <td>0.109188</td>\n",
       "      <td>0.045075</td>\n",
       "      <td>-0.084913</td>\n",
       "      <td>-0.135421</td>\n",
       "      <td>0.069145</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>-0.064714</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>-0.052627</td>\n",
       "      <td>0.014560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008871</td>\n",
       "      <td>0.046193</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.234333</td>\n",
       "      <td>0.179953</td>\n",
       "      <td>0.029830</td>\n",
       "      <td>-0.012984</td>\n",
       "      <td>-0.046922</td>\n",
       "      <td>0.042794</td>\n",
       "      <td>-0.046787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913</th>\n",
       "      <td>0.035525</td>\n",
       "      <td>-0.023847</td>\n",
       "      <td>-0.118682</td>\n",
       "      <td>-0.085456</td>\n",
       "      <td>0.090627</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.010420</td>\n",
       "      <td>0.026654</td>\n",
       "      <td>-0.094493</td>\n",
       "      <td>0.038901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008083</td>\n",
       "      <td>0.086267</td>\n",
       "      <td>0.029471</td>\n",
       "      <td>0.199954</td>\n",
       "      <td>0.192337</td>\n",
       "      <td>0.017063</td>\n",
       "      <td>-0.037731</td>\n",
       "      <td>-0.130126</td>\n",
       "      <td>0.075293</td>\n",
       "      <td>-0.019874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10914</th>\n",
       "      <td>0.088829</td>\n",
       "      <td>-0.144162</td>\n",
       "      <td>-0.122658</td>\n",
       "      <td>-0.003350</td>\n",
       "      <td>0.067955</td>\n",
       "      <td>-0.052278</td>\n",
       "      <td>-0.085135</td>\n",
       "      <td>-0.031870</td>\n",
       "      <td>-0.054439</td>\n",
       "      <td>0.076158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109578</td>\n",
       "      <td>0.066075</td>\n",
       "      <td>-0.056231</td>\n",
       "      <td>0.102821</td>\n",
       "      <td>0.201457</td>\n",
       "      <td>0.022220</td>\n",
       "      <td>-0.033358</td>\n",
       "      <td>-0.165434</td>\n",
       "      <td>-0.011135</td>\n",
       "      <td>-0.026443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10915 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.001459 -0.078473 -0.092210  0.052347  0.102376 -0.002754 -0.056390   \n",
       "1      0.072767 -0.063284 -0.092942  0.015908  0.083422  0.004476 -0.067774   \n",
       "2      0.040674  0.019691 -0.046176  0.082028  0.135475 -0.008119 -0.007749   \n",
       "3      0.100426 -0.135048 -0.067858  0.048585  0.099610  0.015120 -0.076442   \n",
       "4      0.040711 -0.018188 -0.070330  0.057477  0.106131 -0.021223 -0.070114   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10910  0.072800 -0.021963 -0.076825 -0.089859  0.062972 -0.039061 -0.005116   \n",
       "10911  0.114847 -0.079585 -0.117073 -0.018464  0.158374 -0.127964 -0.093952   \n",
       "10912  0.109188  0.045075 -0.084913 -0.135421  0.069145  0.031762 -0.064714   \n",
       "10913  0.035525 -0.023847 -0.118682 -0.085456  0.090627  0.014579 -0.010420   \n",
       "10914  0.088829 -0.144162 -0.122658 -0.003350  0.067955 -0.052278 -0.085135   \n",
       "\n",
       "            7         8         9    ...       190       191       192  \\\n",
       "0      0.037248 -0.053454  0.207812  ... -0.058640  0.173090 -0.018010   \n",
       "1      0.043507 -0.000405  0.186417  ... -0.022937  0.108182 -0.009431   \n",
       "2      0.072441 -0.066983  0.221950  ... -0.025967  0.072008 -0.051875   \n",
       "3      0.033414 -0.030683  0.204774  ... -0.048312  0.127591 -0.027277   \n",
       "4      0.045974 -0.020043  0.177833  ... -0.067081  0.080117 -0.014415   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "10910 -0.047534 -0.084299  0.020017  ... -0.125978  0.118477 -0.050200   \n",
       "10911 -0.101651 -0.051418  0.090127  ... -0.107329  0.046495  0.006817   \n",
       "10912  0.009175 -0.052627  0.014560  ...  0.008871  0.046193  0.000713   \n",
       "10913  0.026654 -0.094493  0.038901  ... -0.008083  0.086267  0.029471   \n",
       "10914 -0.031870 -0.054439  0.076158  ... -0.109578  0.066075 -0.056231   \n",
       "\n",
       "            193       194       195       196       197       198       199  \n",
       "0      0.045516  0.258875  0.031823 -0.007939 -0.085078  0.036334 -0.109643  \n",
       "1      0.064129  0.210500  0.040490 -0.036917 -0.046180 -0.024272 -0.064851  \n",
       "2      0.114976  0.170444  0.098256 -0.050679 -0.022611 -0.050753 -0.058318  \n",
       "3      0.057416  0.185428  0.101191 -0.045935 -0.063575 -0.013838 -0.118549  \n",
       "4      0.098188  0.157175  0.078036 -0.014283 -0.087883 -0.085483 -0.048128  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "10910  0.117858  0.248817  0.005598 -0.001825 -0.123827  0.012053 -0.021778  \n",
       "10911  0.071641  0.254237  0.066662 -0.049829 -0.263125 -0.038419  0.042634  \n",
       "10912  0.234333  0.179953  0.029830 -0.012984 -0.046922  0.042794 -0.046787  \n",
       "10913  0.199954  0.192337  0.017063 -0.037731 -0.130126  0.075293 -0.019874  \n",
       "10914  0.102821  0.201457  0.022220 -0.033358 -0.165434 -0.011135 -0.026443  \n",
       "\n",
       "[10915 rows x 200 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "886f0fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:24:31.505378Z",
     "start_time": "2024-02-10T14:24:31.481378Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(vect)\n",
    "\n",
    "vect_trans = scaler.transform(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecae9635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:24:32.152229Z",
     "start_time": "2024-02-10T14:24:32.145428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4599773 , 0.5001998 , 0.52529293, ..., 0.4052878 , 0.5324256 ,\n",
       "        0.27569968],\n",
       "       [0.58168113, 0.47737575, 0.45707464, ..., 0.46718958, 0.4704046 ,\n",
       "        0.45043117],\n",
       "       [0.50672764, 0.6408195 , 0.54765874, ..., 0.5150039 , 0.45378554,\n",
       "        0.41897237],\n",
       "       ...,\n",
       "       [0.5929465 , 0.5836759 , 0.50009054, ..., 0.45045012, 0.60331523,\n",
       "        0.40922025],\n",
       "       [0.47044164, 0.4418217 , 0.43300474, ..., 0.31804413, 0.6307093 ,\n",
       "        0.43886894],\n",
       "       [0.54568547, 0.40901166, 0.44107127, ..., 0.32701153, 0.504436  ,\n",
       "        0.4860566 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94a65afb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:27.873703Z",
     "start_time": "2024-02-10T14:40:27.863182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arts, culture, entertainment and media' 'conflict, war and peace'\n",
      " 'crime, law and justice' 'disaster, accident and emergency incident'\n",
      " 'economy, business and finance' 'education' 'environment' 'health'\n",
      " 'human interest' 'labour' 'lifestyle and leisure' 'politics'\n",
      " 'religion and belief' 'science and technology' 'society' 'sport'\n",
      " 'weather']\n",
      "We did it boys , labels have been created\n"
     ]
    }
   ],
   "source": [
    "y1 = labeler(io[\"category_level_1\"])\n",
    "\n",
    "\n",
    "X = vect.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6467e6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:28.584075Z",
     "start_time": "2024-02-10T14:40:28.567075Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f59a3dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:29.524752Z",
     "start_time": "2024-02-10T14:40:29.519751Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train1 = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5bc8db44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:40:30.387999Z",
     "start_time": "2024-02-10T14:40:30.383017Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.layers import Flatten , Input , Conv2D , MaxPooling2D , BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eeb1ec26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:43:09.122049Z",
     "start_time": "2024-02-10T14:43:08.952920Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model = tf.keras.Sequential()\n",
    "cnn_model.add(tf.keras.layers.Conv1D(256,3,input_shape=(200,1), activation='leaky_relu'))\n",
    "#cnn_model.add(tf.keras.layers.MaxPool1D())\n",
    "cnn_model.add(tf.keras.layers.Conv1D(128,3,activation='leaky_relu'))\n",
    "#cnn_model.add(tf.keras.layers.MaxPool1D())\n",
    "cnn_model.add(tf.keras.layers.Conv1D(64,3,activation='leaky_relu')) \n",
    "cnn_model.add(tf.keras.layers.MaxPool1D())\n",
    "cnn_model.add(tf.keras.layers.BatchNormalization())\n",
    "cnn_model.add(tf.keras.layers.Flatten())\n",
    "cnn_model.add(tf.keras.layers.Dense(64,activation='leaky_relu'))\n",
    "cnn_model.add(tf.keras.layers.Dense(30,activation='leaky_relu'))\n",
    "cnn_model.add(tf.keras.layers.Dense(17,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fbda5aa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:43:09.753638Z",
     "start_time": "2024-02-10T14:43:09.728336Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer='adam', \n",
    "          loss = 'categorical_crossentropy',\n",
    "          metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                   tf.keras.metrics.Precision(name='precision') ,\n",
    "                   tf.keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7054f445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:43:10.348676Z",
     "start_time": "2024-02-10T14:43:10.325673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 198, 256)          1024      \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 196, 128)          98432     \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 194, 64)           24640     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 97, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 97, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6208)              0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 64)                397376    \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 30)                1950      \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 17)                527       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 524205 (2.00 MB)\n",
      "Trainable params: 524077 (2.00 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b3f8a509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:43:38.980295Z",
     "start_time": "2024-02-10T14:43:11.024661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "437/437 [==============================] - 15s 30ms/step - loss: 1.5424 - accuracy: 0.9535 - precision: 0.7200 - recall: 0.3432 - val_loss: 1.8982 - val_accuracy: 0.9416 - val_precision: 0.9286 - val_recall: 0.0074\n",
      "Epoch 2/2\n",
      "437/437 [==============================] - 13s 30ms/step - loss: 1.1716 - accuracy: 0.9607 - precision: 0.7518 - recall: 0.4948 - val_loss: 1.2370 - val_accuracy: 0.9600 - val_precision: 0.7672 - val_recall: 0.4585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18aefa92690>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(X_train, y_train1, batch_size = 16 ,epochs=2, validation_split=(0.2) ,verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d4327ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:43:49.391591Z",
     "start_time": "2024-02-10T14:43:48.129137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 1s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.32      0.44        68\n",
      "           1       0.69      0.75      0.72       176\n",
      "           2       0.45      0.89      0.59        97\n",
      "           3       0.56      0.78      0.65        90\n",
      "           4       0.67      0.48      0.56        93\n",
      "           5       0.68      0.67      0.67       108\n",
      "           6       0.90      0.52      0.66       126\n",
      "           7       0.72      0.68      0.70       136\n",
      "           8       0.74      0.45      0.56       122\n",
      "           9       0.76      0.64      0.69       155\n",
      "          10       0.41      0.79      0.54        61\n",
      "          11       0.44      0.67      0.53       172\n",
      "          12       0.74      0.68      0.71       182\n",
      "          13       0.59      0.56      0.57       151\n",
      "          14       0.56      0.34      0.42       200\n",
      "          15       0.76      0.82      0.79       169\n",
      "          16       0.90      0.83      0.86        77\n",
      "\n",
      "    accuracy                           0.63      2183\n",
      "   macro avg       0.66      0.64      0.63      2183\n",
      "weighted avg       0.66      0.63      0.63      2183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = np.argmax(cnn_model.predict(X_test), axis=-1)\n",
    "\n",
    "print(classification_report(y_test.values , predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c0eb4d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:47:36.865347Z",
     "start_time": "2024-02-10T14:44:26.203874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration  2  we get these data :\n",
      "110/110 [==============================] - 12s 104ms/step - loss: 0.9445 - accuracy: 0.9667 - precision: 0.7900 - recall: 0.5898 - val_loss: 1.2026 - val_accuracy: 0.9626 - val_precision: 0.7518 - val_recall: 0.5444\n",
      "69/69 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.41      0.49        68\n",
      "           1       0.84      0.58      0.69       176\n",
      "           2       0.73      0.60      0.66        97\n",
      "           3       0.78      0.58      0.66        90\n",
      "           4       0.77      0.52      0.62        93\n",
      "           5       0.66      0.64      0.65       108\n",
      "           6       0.80      0.78      0.79       126\n",
      "           7       0.69      0.77      0.73       136\n",
      "           8       0.59      0.74      0.65       122\n",
      "           9       0.80      0.54      0.65       155\n",
      "          10       0.67      0.57      0.62        61\n",
      "          11       0.51      0.65      0.57       172\n",
      "          12       0.70      0.72      0.71       182\n",
      "          13       0.55      0.63      0.59       151\n",
      "          14       0.43      0.60      0.50       200\n",
      "          15       0.83      0.82      0.82       169\n",
      "          16       0.82      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.66      2183\n",
      "   macro avg       0.69      0.65      0.66      2183\n",
      "weighted avg       0.68      0.66      0.66      2183\n",
      "\n",
      "At iteration  3  we get these data :\n",
      "110/110 [==============================] - 11s 101ms/step - loss: 0.8812 - accuracy: 0.9691 - precision: 0.8022 - recall: 0.6289 - val_loss: 1.1307 - val_accuracy: 0.9643 - val_precision: 0.7654 - val_recall: 0.5678\n",
      "69/69 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.40      0.49        68\n",
      "           1       0.74      0.77      0.76       176\n",
      "           2       0.56      0.77      0.65        97\n",
      "           3       0.67      0.67      0.67        90\n",
      "           4       0.78      0.43      0.56        93\n",
      "           5       0.66      0.73      0.70       108\n",
      "           6       0.84      0.78      0.81       126\n",
      "           7       0.73      0.70      0.71       136\n",
      "           8       0.69      0.57      0.62       122\n",
      "           9       0.72      0.65      0.68       155\n",
      "          10       0.51      0.69      0.58        61\n",
      "          11       0.62      0.57      0.59       172\n",
      "          12       0.72      0.77      0.74       182\n",
      "          13       0.76      0.46      0.58       151\n",
      "          14       0.48      0.64      0.55       200\n",
      "          15       0.77      0.91      0.83       169\n",
      "          16       0.86      0.92      0.89        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.69      0.67      0.67      2183\n",
      "weighted avg       0.69      0.68      0.68      2183\n",
      "\n",
      "At iteration  4  we get these data :\n",
      "110/110 [==============================] - 11s 102ms/step - loss: 0.8370 - accuracy: 0.9701 - precision: 0.8115 - recall: 0.6408 - val_loss: 1.1424 - val_accuracy: 0.9642 - val_precision: 0.7577 - val_recall: 0.5747\n",
      "69/69 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50        68\n",
      "           1       0.77      0.76      0.77       176\n",
      "           2       0.68      0.66      0.67        97\n",
      "           3       0.70      0.60      0.65        90\n",
      "           4       0.63      0.55      0.59        93\n",
      "           5       0.61      0.71      0.66       108\n",
      "           6       0.80      0.78      0.79       126\n",
      "           7       0.66      0.79      0.72       136\n",
      "           8       0.64      0.66      0.65       122\n",
      "           9       0.77      0.63      0.69       155\n",
      "          10       0.62      0.62      0.62        61\n",
      "          11       0.56      0.59      0.57       172\n",
      "          12       0.78      0.57      0.66       182\n",
      "          13       0.55      0.64      0.59       151\n",
      "          14       0.53      0.50      0.52       200\n",
      "          15       0.77      0.88      0.82       169\n",
      "          16       0.82      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.67      2183\n",
      "   macro avg       0.67      0.67      0.67      2183\n",
      "weighted avg       0.67      0.67      0.67      2183\n",
      "\n",
      "At iteration  5  we get these data :\n",
      "110/110 [==============================] - 11s 103ms/step - loss: 0.8167 - accuracy: 0.9704 - precision: 0.8102 - recall: 0.6490 - val_loss: 1.1881 - val_accuracy: 0.9637 - val_precision: 0.7376 - val_recall: 0.5936\n",
      "69/69 [==============================] - 1s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.53      0.57        68\n",
      "           1       0.75      0.76      0.76       176\n",
      "           2       0.70      0.69      0.69        97\n",
      "           3       0.66      0.63      0.64        90\n",
      "           4       0.81      0.31      0.45        93\n",
      "           5       0.68      0.69      0.68       108\n",
      "           6       0.82      0.79      0.80       126\n",
      "           7       0.68      0.75      0.71       136\n",
      "           8       0.60      0.77      0.67       122\n",
      "           9       0.80      0.52      0.63       155\n",
      "          10       0.70      0.61      0.65        61\n",
      "          11       0.52      0.65      0.58       172\n",
      "          12       0.76      0.69      0.72       182\n",
      "          13       0.68      0.55      0.61       151\n",
      "          14       0.46      0.66      0.54       200\n",
      "          15       0.82      0.82      0.82       169\n",
      "          16       0.86      0.90      0.88        77\n",
      "\n",
      "    accuracy                           0.67      2183\n",
      "   macro avg       0.70      0.66      0.67      2183\n",
      "weighted avg       0.69      0.67      0.67      2183\n",
      "\n",
      "At iteration  6  we get these data :\n",
      "110/110 [==============================] - 12s 109ms/step - loss: 0.7755 - accuracy: 0.9723 - precision: 0.8254 - recall: 0.6720 - val_loss: 1.2135 - val_accuracy: 0.9628 - val_precision: 0.7318 - val_recall: 0.5810\n",
      "69/69 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.47      0.52        68\n",
      "           1       0.81      0.69      0.74       176\n",
      "           2       0.67      0.63      0.65        97\n",
      "           3       0.60      0.68      0.64        90\n",
      "           4       0.74      0.45      0.56        93\n",
      "           5       0.78      0.59      0.67       108\n",
      "           6       0.83      0.76      0.79       126\n",
      "           7       0.59      0.83      0.69       136\n",
      "           8       0.64      0.74      0.69       122\n",
      "           9       0.75      0.68      0.71       155\n",
      "          10       0.71      0.64      0.67        61\n",
      "          11       0.60      0.61      0.61       172\n",
      "          12       0.66      0.75      0.70       182\n",
      "          13       0.71      0.53      0.61       151\n",
      "          14       0.47      0.64      0.54       200\n",
      "          15       0.86      0.80      0.83       169\n",
      "          16       0.89      0.92      0.90        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.70      0.67      0.68      2183\n",
      "weighted avg       0.69      0.68      0.68      2183\n",
      "\n",
      "At iteration  7  we get these data :\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.7361 - accuracy: 0.9735 - precision: 0.8325 - recall: 0.6888 - val_loss: 1.1786 - val_accuracy: 0.9640 - val_precision: 0.7478 - val_recall: 0.5856\n",
      "69/69 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57        68\n",
      "           1       0.83      0.68      0.75       176\n",
      "           2       0.58      0.81      0.68        97\n",
      "           3       0.64      0.63      0.64        90\n",
      "           4       0.61      0.62      0.62        93\n",
      "           5       0.70      0.69      0.70       108\n",
      "           6       0.74      0.82      0.77       126\n",
      "           7       0.76      0.68      0.72       136\n",
      "           8       0.70      0.65      0.67       122\n",
      "           9       0.61      0.79      0.69       155\n",
      "          10       0.73      0.66      0.69        61\n",
      "          11       0.56      0.55      0.55       172\n",
      "          12       0.74      0.72      0.73       182\n",
      "          13       0.61      0.64      0.62       151\n",
      "          14       0.66      0.44      0.53       200\n",
      "          15       0.76      0.85      0.80       169\n",
      "          16       0.87      0.95      0.91        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.69      0.69      0.68      2183\n",
      "weighted avg       0.69      0.68      0.68      2183\n",
      "\n",
      "At iteration  8  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 11s 105ms/step - loss: 0.7233 - accuracy: 0.9736 - precision: 0.8315 - recall: 0.6922 - val_loss: 1.2592 - val_accuracy: 0.9622 - val_precision: 0.7185 - val_recall: 0.5873\n",
      "69/69 [==============================] - 1s 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.34      0.44        68\n",
      "           1       0.79      0.72      0.75       176\n",
      "           2       0.73      0.57      0.64        97\n",
      "           3       0.62      0.61      0.62        90\n",
      "           4       0.76      0.42      0.54        93\n",
      "           5       0.68      0.64      0.66       108\n",
      "           6       0.81      0.72      0.76       126\n",
      "           7       0.54      0.84      0.65       136\n",
      "           8       0.64      0.64      0.64       122\n",
      "           9       0.84      0.49      0.62       155\n",
      "          10       0.61      0.72      0.66        61\n",
      "          11       0.46      0.67      0.55       172\n",
      "          12       0.70      0.74      0.72       182\n",
      "          13       0.54      0.63      0.58       151\n",
      "          14       0.54      0.46      0.50       200\n",
      "          15       0.79      0.82      0.80       169\n",
      "          16       0.78      0.99      0.87        77\n",
      "\n",
      "    accuracy                           0.65      2183\n",
      "   macro avg       0.68      0.65      0.65      2183\n",
      "weighted avg       0.67      0.65      0.65      2183\n",
      "\n",
      "At iteration  9  we get these data :\n",
      "110/110 [==============================] - 13s 116ms/step - loss: 0.6775 - accuracy: 0.9750 - precision: 0.8383 - recall: 0.7125 - val_loss: 1.2470 - val_accuracy: 0.9629 - val_precision: 0.7220 - val_recall: 0.6005\n",
      "69/69 [==============================] - 1s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.57      0.51        68\n",
      "           1       0.84      0.69      0.76       176\n",
      "           2       0.57      0.76      0.65        97\n",
      "           3       0.61      0.66      0.63        90\n",
      "           4       0.70      0.53      0.60        93\n",
      "           5       0.60      0.81      0.69       108\n",
      "           6       0.87      0.67      0.76       126\n",
      "           7       0.68      0.78      0.73       136\n",
      "           8       0.65      0.57      0.61       122\n",
      "           9       0.78      0.69      0.73       155\n",
      "          10       0.62      0.74      0.67        61\n",
      "          11       0.52      0.62      0.57       172\n",
      "          12       0.68      0.73      0.71       182\n",
      "          13       0.66      0.56      0.61       151\n",
      "          14       0.60      0.47      0.53       200\n",
      "          15       0.84      0.80      0.82       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.68      0.68      0.67      2183\n",
      "weighted avg       0.69      0.68      0.68      2183\n",
      "\n",
      "At iteration  10  we get these data :\n",
      "110/110 [==============================] - 11s 100ms/step - loss: 0.6487 - accuracy: 0.9757 - precision: 0.8430 - recall: 0.7203 - val_loss: 1.2833 - val_accuracy: 0.9621 - val_precision: 0.7142 - val_recall: 0.5936\n",
      "69/69 [==============================] - 1s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.41      0.49        68\n",
      "           1       0.85      0.51      0.63       176\n",
      "           2       0.69      0.66      0.67        97\n",
      "           3       0.54      0.67      0.60        90\n",
      "           4       0.67      0.52      0.58        93\n",
      "           5       0.64      0.71      0.68       108\n",
      "           6       0.78      0.75      0.76       126\n",
      "           7       0.60      0.84      0.70       136\n",
      "           8       0.70      0.59      0.64       122\n",
      "           9       0.76      0.65      0.70       155\n",
      "          10       0.58      0.74      0.65        61\n",
      "          11       0.44      0.73      0.55       172\n",
      "          12       0.67      0.76      0.71       182\n",
      "          13       0.63      0.50      0.56       151\n",
      "          14       0.59      0.40      0.48       200\n",
      "          15       0.79      0.83      0.81       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.65      2183\n",
      "   macro avg       0.67      0.66      0.65      2183\n",
      "weighted avg       0.67      0.65      0.65      2183\n",
      "\n",
      "At iteration  11  we get these data :\n",
      "110/110 [==============================] - 12s 109ms/step - loss: 0.6042 - accuracy: 0.9773 - precision: 0.8527 - recall: 0.7423 - val_loss: 1.3128 - val_accuracy: 0.9635 - val_precision: 0.7252 - val_recall: 0.6119\n",
      "69/69 [==============================] - 1s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.46      0.51        68\n",
      "           1       0.82      0.68      0.74       176\n",
      "           2       0.66      0.68      0.67        97\n",
      "           3       0.64      0.56      0.60        90\n",
      "           4       0.62      0.59      0.60        93\n",
      "           5       0.66      0.71      0.68       108\n",
      "           6       0.77      0.80      0.78       126\n",
      "           7       0.71      0.76      0.73       136\n",
      "           8       0.66      0.66      0.66       122\n",
      "           9       0.73      0.72      0.72       155\n",
      "          10       0.64      0.77      0.70        61\n",
      "          11       0.57      0.67      0.62       172\n",
      "          12       0.71      0.69      0.70       182\n",
      "          13       0.72      0.47      0.57       151\n",
      "          14       0.50      0.55      0.53       200\n",
      "          15       0.79      0.86      0.82       169\n",
      "          16       0.77      0.97      0.86        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.68      0.68      0.68      2183\n",
      "weighted avg       0.68      0.68      0.68      2183\n",
      "\n",
      "At iteration  12  we get these data :\n",
      "110/110 [==============================] - 12s 112ms/step - loss: 0.5746 - accuracy: 0.9780 - precision: 0.8549 - recall: 0.7535 - val_loss: 1.3932 - val_accuracy: 0.9615 - val_precision: 0.7005 - val_recall: 0.6050\n",
      "69/69 [==============================] - 1s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.59      0.52        68\n",
      "           1       0.82      0.64      0.72       176\n",
      "           2       0.63      0.70      0.66        97\n",
      "           3       0.73      0.60      0.66        90\n",
      "           4       0.72      0.39      0.50        93\n",
      "           5       0.63      0.77      0.69       108\n",
      "           6       0.65      0.87      0.74       126\n",
      "           7       0.71      0.76      0.73       136\n",
      "           8       0.69      0.69      0.69       122\n",
      "           9       0.66      0.79      0.72       155\n",
      "          10       0.74      0.57      0.65        61\n",
      "          11       0.49      0.63      0.55       172\n",
      "          12       0.63      0.76      0.69       182\n",
      "          13       0.62      0.52      0.57       151\n",
      "          14       0.64      0.45      0.52       200\n",
      "          15       0.88      0.75      0.81       169\n",
      "          16       0.91      0.91      0.91        77\n",
      "\n",
      "    accuracy                           0.67      2183\n",
      "   macro avg       0.68      0.67      0.67      2183\n",
      "weighted avg       0.68      0.67      0.67      2183\n",
      "\n",
      "At iteration  13  we get these data :\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.5549 - accuracy: 0.9794 - precision: 0.8650 - recall: 0.7698 - val_loss: 1.3345 - val_accuracy: 0.9631 - val_precision: 0.7188 - val_recall: 0.6131\n",
      "69/69 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.50      0.49        68\n",
      "           1       0.86      0.65      0.74       176\n",
      "           2       0.76      0.65      0.70        97\n",
      "           3       0.70      0.52      0.60        90\n",
      "           4       0.69      0.47      0.56        93\n",
      "           5       0.70      0.70      0.70       108\n",
      "           6       0.85      0.70      0.77       126\n",
      "           7       0.76      0.73      0.74       136\n",
      "           8       0.65      0.61      0.63       122\n",
      "           9       0.74      0.67      0.70       155\n",
      "          10       0.76      0.72      0.74        61\n",
      "          11       0.53      0.73      0.61       172\n",
      "          12       0.69      0.71      0.70       182\n",
      "          13       0.54      0.60      0.57       151\n",
      "          14       0.51      0.55      0.53       200\n",
      "          15       0.73      0.90      0.80       169\n",
      "          16       0.79      0.99      0.88        77\n",
      "\n",
      "    accuracy                           0.67      2183\n",
      "   macro avg       0.69      0.67      0.67      2183\n",
      "weighted avg       0.68      0.67      0.67      2183\n",
      "\n",
      "At iteration  14  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 11s 104ms/step - loss: 0.5258 - accuracy: 0.9800 - precision: 0.8688 - recall: 0.7784 - val_loss: 1.4644 - val_accuracy: 0.9616 - val_precision: 0.6969 - val_recall: 0.6159\n",
      "69/69 [==============================] - 1s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.41      0.50        68\n",
      "           1       0.77      0.70      0.73       176\n",
      "           2       0.80      0.57      0.66        97\n",
      "           3       0.61      0.69      0.65        90\n",
      "           4       0.70      0.48      0.57        93\n",
      "           5       0.59      0.68      0.63       108\n",
      "           6       0.82      0.64      0.72       126\n",
      "           7       0.64      0.78      0.70       136\n",
      "           8       0.66      0.69      0.67       122\n",
      "           9       0.80      0.60      0.69       155\n",
      "          10       0.82      0.67      0.74        61\n",
      "          11       0.49      0.73      0.58       172\n",
      "          12       0.65      0.75      0.70       182\n",
      "          13       0.56      0.58      0.57       151\n",
      "          14       0.62      0.52      0.57       200\n",
      "          15       0.77      0.89      0.83       169\n",
      "          16       0.89      0.87      0.88        77\n",
      "\n",
      "    accuracy                           0.67      2183\n",
      "   macro avg       0.70      0.66      0.67      2183\n",
      "weighted avg       0.69      0.67      0.67      2183\n",
      "\n",
      "At iteration  15  we get these data :\n",
      "110/110 [==============================] - 12s 111ms/step - loss: 0.4922 - accuracy: 0.9811 - precision: 0.8713 - recall: 0.7956 - val_loss: 1.4599 - val_accuracy: 0.9625 - val_precision: 0.7039 - val_recall: 0.6245\n",
      "69/69 [==============================] - 1s 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.51      0.54        68\n",
      "           1       0.79      0.69      0.73       176\n",
      "           2       0.55      0.73      0.63        97\n",
      "           3       0.65      0.60      0.62        90\n",
      "           4       0.67      0.55      0.60        93\n",
      "           5       0.62      0.76      0.68       108\n",
      "           6       0.78      0.75      0.77       126\n",
      "           7       0.82      0.62      0.70       136\n",
      "           8       0.71      0.56      0.62       122\n",
      "           9       0.75      0.68      0.71       155\n",
      "          10       0.66      0.84      0.74        61\n",
      "          11       0.54      0.62      0.58       172\n",
      "          12       0.68      0.71      0.69       182\n",
      "          13       0.63      0.55      0.59       151\n",
      "          14       0.50      0.60      0.54       200\n",
      "          15       0.83      0.81      0.82       169\n",
      "          16       0.88      0.96      0.92        77\n",
      "\n",
      "    accuracy                           0.67      2183\n",
      "   macro avg       0.68      0.68      0.68      2183\n",
      "weighted avg       0.68      0.67      0.67      2183\n",
      "\n",
      "At iteration  16  we get these data :\n",
      " 95/110 [========================>.....] - ETA: 1s - loss: 0.4405 - accuracy: 0.9827 - precision: 0.8842 - recall: 0.8123"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt iteration \u001b[39m\u001b[38;5;124m\"\u001b[39m  ,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m we get these data :\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m----> 5\u001b[0m     cnn_model\u001b[38;5;241m.\u001b[39mfit(X_train , y_train1 , batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m , epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m , validation_split\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.2\u001b[39m) ,verbose \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(cnn_model\u001b[38;5;241m.\u001b[39mpredict(X_test), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(y_test\u001b[38;5;241m.\u001b[39mvalues , predictions))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    print( \"At iteration \"  ,i+2 , \" we get these data :\" )\n",
    "    \n",
    "    cnn_model.fit(X_train , y_train1 , batch_size = 64 , epochs = 1 , validation_split=(0.2) ,verbose =1)\n",
    "\n",
    "    predictions = np.argmax(cnn_model.predict(X_test), axis=-1)\n",
    "\n",
    "    print(classification_report(y_test.values , predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26c7d6d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:56:43.286003Z",
     "start_time": "2024-02-10T13:56:08.291981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "437/437 [==============================] - 12s 28ms/step - loss: 0.9620 - accuracy: 0.9662 - precision: 0.7791 - recall: 0.5941 - val_loss: 1.2586 - val_accuracy: 0.9610 - val_precision: 0.7159 - val_recall: 0.5581\n",
      "Epoch 2/3\n",
      "437/437 [==============================] - 11s 26ms/step - loss: 0.8831 - accuracy: 0.9685 - precision: 0.7934 - recall: 0.6289 - val_loss: 1.1667 - val_accuracy: 0.9626 - val_precision: 0.7356 - val_recall: 0.5684\n",
      "Epoch 3/3\n",
      "437/437 [==============================] - 12s 26ms/step - loss: 0.8455 - accuracy: 0.9694 - precision: 0.7982 - recall: 0.6432 - val_loss: 1.1566 - val_accuracy: 0.9634 - val_precision: 0.7436 - val_recall: 0.5776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x263f5766b10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(X_train, y_train1, batch_size = 16 ,epochs=3, validation_split=(0.2) ,verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "223624ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:56:46.502604Z",
     "start_time": "2024-02-10T13:56:45.399715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 1s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.46      0.46        68\n",
      "           1       0.78      0.69      0.73       176\n",
      "           2       0.55      0.80      0.65        97\n",
      "           3       0.52      0.69      0.59        90\n",
      "           4       0.70      0.55      0.61        93\n",
      "           5       0.67      0.69      0.68       108\n",
      "           6       0.77      0.79      0.78       126\n",
      "           7       0.59      0.88      0.71       136\n",
      "           8       0.67      0.43      0.53       122\n",
      "           9       0.85      0.63      0.73       155\n",
      "          10       0.90      0.44      0.59        61\n",
      "          11       0.72      0.49      0.58       172\n",
      "          12       0.76      0.75      0.75       182\n",
      "          13       0.42      0.66      0.51       151\n",
      "          14       0.62      0.41      0.50       200\n",
      "          15       0.78      0.92      0.85       169\n",
      "          16       0.82      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.66      2183\n",
      "   macro avg       0.68      0.66      0.65      2183\n",
      "weighted avg       0.68      0.66      0.66      2183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(cnn_model.predict(X_test), axis=-1)\n",
    "\n",
    "print(classification_report(y_test.values , predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "342c8669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:57:02.985769Z",
     "start_time": "2024-02-10T13:56:49.800318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437/437 [==============================] - 12s 27ms/step - loss: 0.7857 - accuracy: 0.9709 - precision: 0.8068 - recall: 0.6644 - val_loss: 1.2116 - val_accuracy: 0.9619 - val_precision: 0.7202 - val_recall: 0.5747\n",
      "69/69 [==============================] - 1s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.25      0.39        68\n",
      "           1       0.83      0.69      0.75       176\n",
      "           2       0.69      0.65      0.67        97\n",
      "           3       0.88      0.49      0.63        90\n",
      "           4       0.51      0.63      0.56        93\n",
      "           5       0.62      0.83      0.71       108\n",
      "           6       0.87      0.71      0.78       126\n",
      "           7       0.62      0.82      0.70       136\n",
      "           8       0.42      0.84      0.56       122\n",
      "           9       0.86      0.62      0.72       155\n",
      "          10       0.71      0.44      0.55        61\n",
      "          11       0.65      0.52      0.58       172\n",
      "          12       0.74      0.72      0.73       182\n",
      "          13       0.80      0.40      0.54       151\n",
      "          14       0.45      0.62      0.52       200\n",
      "          15       0.85      0.83      0.84       169\n",
      "          16       0.71      0.99      0.83        77\n",
      "\n",
      "    accuracy                           0.66      2183\n",
      "   macro avg       0.71      0.65      0.65      2183\n",
      "weighted avg       0.71      0.66      0.66      2183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_model.fit(X_train, y_train1, batch_size = 16 ,epochs=1, validation_split=(0.2) ,verbose =1)\n",
    "\n",
    "predictions = np.argmax(cnn_model.predict(X_test), axis=-1)\n",
    "\n",
    "print(classification_report(y_test.values , predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a02b6788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T13:18:15.017701Z",
     "start_time": "2024-02-08T13:18:15.014220Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6 ---> 69%\n",
    "# 7 ---> 70%\n",
    "# 8 ---> 67%\n",
    "# 9 ---> 71\n",
    "# 10 ---> 64%\n",
    "#11 ---> 69%\n",
    "#12 ---> 69%\n",
    "#13 ---> 69%\n",
    "# 14 ---> 70 %\n",
    "# 15 ---> 71 %\n",
    "# 16 ---> 69 %\n",
    "# 17 --- > 71 %\n",
    "# 18 ---> 70 %\n",
    "# 19 ---> 70 %\n",
    "# 20 ---> 70 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19dfd5a",
   "metadata": {},
   "source": [
    "# MODEL FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "089abd7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:41:20.657314Z",
     "start_time": "2024-02-10T14:41:20.573232Z"
    }
   },
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Dense(100,input_dim=200,activation = \"leaky_relu\"))\n",
    "model6.add(Dense(180,activation = \"leaky_relu\"))\n",
    "model6.add(tf.keras.layers.Dropout(0.2))\n",
    "model6.add(Dense(60,activation = \"leaky_relu\"))\n",
    "#model6.add(Dense(30,activation = \"leaky_relu\"))\n",
    "model6.add(Dense(17,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07b64ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:41:20.932095Z",
     "start_time": "2024-02-10T14:41:20.926289Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.AdamW(learning_rate =0.007 , beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    use_ema=True,\n",
    "    ema_momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5616c058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:41:21.267726Z",
     "start_time": "2024-02-10T14:41:21.244760Z"
    }
   },
   "outputs": [],
   "source": [
    "model6.compile(optimizer = opt , \n",
    "              loss = 'categorical_crossentropy' ,\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7f0b3fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:41:29.200099Z",
     "start_time": "2024-02-10T14:41:21.765315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "437/437 [==============================] - 3s 3ms/step - loss: 1.6885 - accuracy: 0.9505 - precision: 0.7043 - recall: 0.2739 - val_loss: 1.3865 - val_accuracy: 0.9579 - val_precision: 0.7616 - val_recall: 0.4133\n",
      "Epoch 2/5\n",
      "437/437 [==============================] - 1s 3ms/step - loss: 1.3686 - accuracy: 0.9561 - precision: 0.7225 - recall: 0.4116 - val_loss: 1.4017 - val_accuracy: 0.9575 - val_precision: 0.7366 - val_recall: 0.4322\n",
      "Epoch 3/5\n",
      "437/437 [==============================] - 1s 3ms/step - loss: 1.2964 - accuracy: 0.9578 - precision: 0.7319 - recall: 0.4451 - val_loss: 1.3009 - val_accuracy: 0.9598 - val_precision: 0.7447 - val_recall: 0.4825\n",
      "Epoch 4/5\n",
      "437/437 [==============================] - 1s 3ms/step - loss: 1.2337 - accuracy: 0.9595 - precision: 0.7371 - recall: 0.4829 - val_loss: 1.3808 - val_accuracy: 0.9572 - val_precision: 0.7070 - val_recall: 0.4654\n",
      "Epoch 5/5\n",
      "437/437 [==============================] - 1s 3ms/step - loss: 1.1781 - accuracy: 0.9609 - precision: 0.7489 - recall: 0.5048 - val_loss: 1.2744 - val_accuracy: 0.9603 - val_precision: 0.7173 - val_recall: 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18adeca6590>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.fit(X_train , y_train1 , batch_size = 16 , epochs = 5 , validation_split=(0.2) ,verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c3a383e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:41:33.536586Z",
     "start_time": "2024-02-10T14:41:33.262569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14,  1, 16, ..., 11,  9, 12], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.argmax(model6.predict(X_test), axis=-1)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8eaea2c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:41:35.049677Z",
     "start_time": "2024-02-10T14:41:35.030675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50        68\n",
      "           1       0.77      0.72      0.74       176\n",
      "           2       0.69      0.78      0.73        97\n",
      "           3       0.76      0.61      0.68        90\n",
      "           4       0.82      0.43      0.56        93\n",
      "           5       0.66      0.67      0.66       108\n",
      "           6       0.73      0.84      0.78       126\n",
      "           7       0.69      0.70      0.70       136\n",
      "           8       0.78      0.44      0.57       122\n",
      "           9       0.78      0.63      0.70       155\n",
      "          10       0.55      0.66      0.60        61\n",
      "          11       0.49      0.66      0.56       172\n",
      "          12       0.75      0.75      0.75       182\n",
      "          13       0.73      0.50      0.60       151\n",
      "          14       0.46      0.60      0.52       200\n",
      "          15       0.76      0.84      0.80       169\n",
      "          16       0.80      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.67      2183\n",
      "   macro avg       0.69      0.66      0.67      2183\n",
      "weighted avg       0.69      0.67      0.67      2183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.values , predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "838635ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:42:57.085216Z",
     "start_time": "2024-02-10T14:41:35.806725Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration  6  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.9301 - accuracy: 0.9673 - precision: 0.7988 - recall: 0.5936 - val_loss: 1.1594 - val_accuracy: 0.9637 - val_precision: 0.7569 - val_recall: 0.5633\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.46      0.47        68\n",
      "           1       0.78      0.73      0.75       176\n",
      "           2       0.68      0.74      0.71        97\n",
      "           3       0.72      0.61      0.66        90\n",
      "           4       0.82      0.45      0.58        93\n",
      "           5       0.65      0.67      0.66       108\n",
      "           6       0.75      0.83      0.79       126\n",
      "           7       0.70      0.71      0.70       136\n",
      "           8       0.73      0.52      0.61       122\n",
      "           9       0.77      0.63      0.69       155\n",
      "          10       0.61      0.66      0.63        61\n",
      "          11       0.50      0.64      0.56       172\n",
      "          12       0.74      0.76      0.75       182\n",
      "          13       0.69      0.54      0.61       151\n",
      "          14       0.49      0.62      0.55       200\n",
      "          15       0.78      0.86      0.81       169\n",
      "          16       0.80      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.69      0.67      0.67      2183\n",
      "weighted avg       0.69      0.68      0.67      2183\n",
      "\n",
      "At iteration  7  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.9008 - accuracy: 0.9684 - precision: 0.8033 - recall: 0.6132 - val_loss: 1.1561 - val_accuracy: 0.9632 - val_precision: 0.7410 - val_recall: 0.5747\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.47      0.50        68\n",
      "           1       0.79      0.72      0.75       176\n",
      "           2       0.66      0.75      0.71        97\n",
      "           3       0.72      0.63      0.67        90\n",
      "           4       0.80      0.46      0.59        93\n",
      "           5       0.67      0.70      0.68       108\n",
      "           6       0.77      0.83      0.80       126\n",
      "           7       0.69      0.71      0.70       136\n",
      "           8       0.70      0.57      0.63       122\n",
      "           9       0.76      0.63      0.69       155\n",
      "          10       0.63      0.66      0.65        61\n",
      "          11       0.52      0.63      0.57       172\n",
      "          12       0.73      0.77      0.75       182\n",
      "          13       0.65      0.55      0.59       151\n",
      "          14       0.53      0.60      0.56       200\n",
      "          15       0.77      0.85      0.81       169\n",
      "          16       0.80      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.69      0.68      0.68      2183\n",
      "weighted avg       0.69      0.68      0.68      2183\n",
      "\n",
      "At iteration  8  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8825 - accuracy: 0.9685 - precision: 0.7973 - recall: 0.6226 - val_loss: 1.1615 - val_accuracy: 0.9631 - val_precision: 0.7455 - val_recall: 0.5667\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.49      0.52        68\n",
      "           1       0.80      0.73      0.76       176\n",
      "           2       0.69      0.75      0.72        97\n",
      "           3       0.73      0.67      0.70        90\n",
      "           4       0.80      0.47      0.59        93\n",
      "           5       0.66      0.71      0.68       108\n",
      "           6       0.77      0.81      0.79       126\n",
      "           7       0.71      0.71      0.71       136\n",
      "           8       0.71      0.55      0.62       122\n",
      "           9       0.76      0.63      0.69       155\n",
      "          10       0.65      0.66      0.65        61\n",
      "          11       0.52      0.63      0.57       172\n",
      "          12       0.72      0.77      0.75       182\n",
      "          13       0.65      0.56      0.60       151\n",
      "          14       0.50      0.59      0.55       200\n",
      "          15       0.78      0.86      0.81       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.69      0.68      0.68      2183\n",
      "weighted avg       0.69      0.68      0.68      2183\n",
      "\n",
      "At iteration  9  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8644 - accuracy: 0.9691 - precision: 0.8076 - recall: 0.6230 - val_loss: 1.1780 - val_accuracy: 0.9637 - val_precision: 0.7429 - val_recall: 0.5856\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.49      0.51        68\n",
      "           1       0.80      0.72      0.76       176\n",
      "           2       0.69      0.77      0.73        97\n",
      "           3       0.71      0.63      0.67        90\n",
      "           4       0.78      0.48      0.60        93\n",
      "           5       0.66      0.70      0.68       108\n",
      "           6       0.78      0.84      0.81       126\n",
      "           7       0.71      0.73      0.72       136\n",
      "           8       0.68      0.59      0.63       122\n",
      "           9       0.77      0.65      0.70       155\n",
      "          10       0.65      0.64      0.64        61\n",
      "          11       0.53      0.63      0.58       172\n",
      "          12       0.72      0.76      0.74       182\n",
      "          13       0.65      0.58      0.61       151\n",
      "          14       0.54      0.59      0.57       200\n",
      "          15       0.78      0.85      0.81       169\n",
      "          16       0.80      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.69      0.68      0.68      2183\n",
      "weighted avg       0.69      0.69      0.69      2183\n",
      "\n",
      "At iteration  10  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8500 - accuracy: 0.9694 - precision: 0.8042 - recall: 0.6338 - val_loss: 1.1922 - val_accuracy: 0.9634 - val_precision: 0.7414 - val_recall: 0.5810\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.49      0.52        68\n",
      "           1       0.81      0.73      0.76       176\n",
      "           2       0.69      0.75      0.72        97\n",
      "           3       0.71      0.64      0.67        90\n",
      "           4       0.78      0.49      0.61        93\n",
      "           5       0.64      0.71      0.68       108\n",
      "           6       0.78      0.83      0.80       126\n",
      "           7       0.71      0.70      0.70       136\n",
      "           8       0.71      0.60      0.65       122\n",
      "           9       0.76      0.64      0.69       155\n",
      "          10       0.65      0.64      0.64        61\n",
      "          11       0.53      0.64      0.58       172\n",
      "          12       0.73      0.78      0.76       182\n",
      "          13       0.67      0.58      0.62       151\n",
      "          14       0.52      0.59      0.56       200\n",
      "          15       0.78      0.87      0.82       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.69      0.69      2183\n",
      "\n",
      "At iteration  11  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8544 - accuracy: 0.9699 - precision: 0.8082 - recall: 0.6394 - val_loss: 1.1442 - val_accuracy: 0.9640 - val_precision: 0.7602 - val_recall: 0.5678\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.44      0.47        68\n",
      "           1       0.81      0.73      0.77       176\n",
      "           2       0.69      0.76      0.72        97\n",
      "           3       0.71      0.63      0.67        90\n",
      "           4       0.75      0.49      0.60        93\n",
      "           5       0.65      0.71      0.68       108\n",
      "           6       0.78      0.83      0.81       126\n",
      "           7       0.70      0.71      0.71       136\n",
      "           8       0.70      0.61      0.65       122\n",
      "           9       0.76      0.66      0.71       155\n",
      "          10       0.68      0.64      0.66        61\n",
      "          11       0.55      0.63      0.59       172\n",
      "          12       0.71      0.77      0.74       182\n",
      "          13       0.65      0.58      0.61       151\n",
      "          14       0.54      0.60      0.57       200\n",
      "          15       0.78      0.87      0.82       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.69      0.68      0.69      2183\n",
      "weighted avg       0.69      0.69      0.69      2183\n",
      "\n",
      "At iteration  12  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8346 - accuracy: 0.9700 - precision: 0.8086 - recall: 0.6422 - val_loss: 1.1878 - val_accuracy: 0.9639 - val_precision: 0.7504 - val_recall: 0.5781\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.47      0.51        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.68      0.75      0.71        97\n",
      "           3       0.72      0.62      0.67        90\n",
      "           4       0.77      0.51      0.61        93\n",
      "           5       0.64      0.70      0.67       108\n",
      "           6       0.79      0.83      0.81       126\n",
      "           7       0.71      0.70      0.71       136\n",
      "           8       0.72      0.60      0.65       122\n",
      "           9       0.77      0.66      0.71       155\n",
      "          10       0.67      0.66      0.66        61\n",
      "          11       0.55      0.65      0.60       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.66      0.59      0.62       151\n",
      "          14       0.52      0.60      0.56       200\n",
      "          15       0.78      0.86      0.82       169\n",
      "          16       0.81      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.69      0.69      0.69      2183\n",
      "\n",
      "At iteration  13  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8410 - accuracy: 0.9695 - precision: 0.8028 - recall: 0.6375 - val_loss: 1.1565 - val_accuracy: 0.9652 - val_precision: 0.7641 - val_recall: 0.5913\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.47      0.52        68\n",
      "           1       0.78      0.73      0.76       176\n",
      "           2       0.69      0.76      0.72        97\n",
      "           3       0.71      0.60      0.65        90\n",
      "           4       0.78      0.51      0.61        93\n",
      "           5       0.64      0.70      0.67       108\n",
      "           6       0.78      0.83      0.81       126\n",
      "           7       0.71      0.69      0.70       136\n",
      "           8       0.71      0.57      0.64       122\n",
      "           9       0.77      0.63      0.69       155\n",
      "          10       0.70      0.64      0.67        61\n",
      "          11       0.56      0.64      0.60       172\n",
      "          12       0.72      0.75      0.74       182\n",
      "          13       0.65      0.60      0.62       151\n",
      "          14       0.51      0.61      0.56       200\n",
      "          15       0.77      0.88      0.82       169\n",
      "          16       0.80      0.96      0.87        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.70      0.68      0.68      2183\n",
      "weighted avg       0.69      0.69      0.69      2183\n",
      "\n",
      "At iteration  14  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8115 - accuracy: 0.9702 - precision: 0.8063 - recall: 0.6494 - val_loss: 1.1761 - val_accuracy: 0.9624 - val_precision: 0.7330 - val_recall: 0.5673\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.49      0.51        68\n",
      "           1       0.79      0.74      0.77       176\n",
      "           2       0.69      0.75      0.72        97\n",
      "           3       0.69      0.61      0.65        90\n",
      "           4       0.78      0.48      0.60        93\n",
      "           5       0.64      0.70      0.67       108\n",
      "           6       0.79      0.84      0.82       126\n",
      "           7       0.71      0.72      0.71       136\n",
      "           8       0.72      0.61      0.66       122\n",
      "           9       0.77      0.63      0.70       155\n",
      "          10       0.68      0.64      0.66        61\n",
      "          11       0.56      0.63      0.59       172\n",
      "          12       0.72      0.74      0.73       182\n",
      "          13       0.66      0.59      0.62       151\n",
      "          14       0.53      0.62      0.57       200\n",
      "          15       0.77      0.87      0.82       169\n",
      "          16       0.80      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.70      0.68      0.69      2183\n",
      "weighted avg       0.69      0.69      0.69      2183\n",
      "\n",
      "At iteration  15  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8190 - accuracy: 0.9703 - precision: 0.8061 - recall: 0.6513 - val_loss: 1.1861 - val_accuracy: 0.9635 - val_precision: 0.7407 - val_recall: 0.5839\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.44      0.50        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.69      0.76      0.72        97\n",
      "           3       0.70      0.60      0.65        90\n",
      "           4       0.79      0.52      0.62        93\n",
      "           5       0.65      0.71      0.68       108\n",
      "           6       0.80      0.84      0.82       126\n",
      "           7       0.70      0.71      0.70       136\n",
      "           8       0.74      0.60      0.66       122\n",
      "           9       0.77      0.65      0.70       155\n",
      "          10       0.68      0.64      0.66        61\n",
      "          11       0.56      0.63      0.59       172\n",
      "          12       0.70      0.77      0.73       182\n",
      "          13       0.64      0.59      0.62       151\n",
      "          14       0.53      0.63      0.58       200\n",
      "          15       0.78      0.86      0.82       169\n",
      "          16       0.80      0.96      0.87        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.69      0.69      2183\n",
      "\n",
      "At iteration  16  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8149 - accuracy: 0.9703 - precision: 0.8055 - recall: 0.6534 - val_loss: 1.2096 - val_accuracy: 0.9630 - val_precision: 0.7365 - val_recall: 0.5776\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.49      0.52        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.71      0.76      0.74        97\n",
      "           3       0.71      0.60      0.65        90\n",
      "           4       0.77      0.54      0.63        93\n",
      "           5       0.66      0.72      0.69       108\n",
      "           6       0.80      0.82      0.81       126\n",
      "           7       0.69      0.70      0.69       136\n",
      "           8       0.72      0.61      0.66       122\n",
      "           9       0.77      0.63      0.69       155\n",
      "          10       0.68      0.64      0.66        61\n",
      "          11       0.58      0.65      0.61       172\n",
      "          12       0.70      0.76      0.73       182\n",
      "          13       0.64      0.58      0.61       151\n",
      "          14       0.53      0.63      0.57       200\n",
      "          15       0.78      0.87      0.82       169\n",
      "          16       0.80      0.96      0.87        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.69      0.69      2183\n",
      "\n",
      "At iteration  17  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.8080 - accuracy: 0.9705 - precision: 0.8086 - recall: 0.6537 - val_loss: 1.2447 - val_accuracy: 0.9618 - val_precision: 0.7186 - val_recall: 0.5758\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.47      0.51        68\n",
      "           1       0.79      0.74      0.76       176\n",
      "           2       0.72      0.75      0.74        97\n",
      "           3       0.68      0.60      0.64        90\n",
      "           4       0.78      0.54      0.64        93\n",
      "           5       0.67      0.72      0.70       108\n",
      "           6       0.81      0.83      0.82       126\n",
      "           7       0.71      0.71      0.71       136\n",
      "           8       0.74      0.61      0.67       122\n",
      "           9       0.78      0.65      0.71       155\n",
      "          10       0.71      0.64      0.67        61\n",
      "          11       0.56      0.65      0.60       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.67      0.58      0.62       151\n",
      "          14       0.52      0.64      0.58       200\n",
      "          15       0.78      0.87      0.82       169\n",
      "          16       0.80      0.96      0.87        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.69      0.69      2183\n",
      "\n",
      "At iteration  18  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7989 - accuracy: 0.9711 - precision: 0.8148 - recall: 0.6584 - val_loss: 1.2890 - val_accuracy: 0.9624 - val_precision: 0.7192 - val_recall: 0.5924\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.49      0.52        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.72      0.75      0.73        97\n",
      "           3       0.69      0.60      0.64        90\n",
      "           4       0.80      0.55      0.65        93\n",
      "           5       0.66      0.72      0.69       108\n",
      "           6       0.80      0.80      0.80       126\n",
      "           7       0.69      0.70      0.69       136\n",
      "           8       0.75      0.62      0.68       122\n",
      "           9       0.78      0.66      0.72       155\n",
      "          10       0.72      0.67      0.69        61\n",
      "          11       0.59      0.68      0.63       172\n",
      "          12       0.72      0.76      0.74       182\n",
      "          13       0.65      0.58      0.61       151\n",
      "          14       0.53      0.64      0.58       200\n",
      "          15       0.78      0.86      0.82       169\n",
      "          16       0.80      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  19  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7847 - accuracy: 0.9713 - precision: 0.8116 - recall: 0.6661 - val_loss: 1.2593 - val_accuracy: 0.9625 - val_precision: 0.7197 - val_recall: 0.5924\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.50      0.52        68\n",
      "           1       0.80      0.75      0.78       176\n",
      "           2       0.74      0.75      0.75        97\n",
      "           3       0.70      0.60      0.65        90\n",
      "           4       0.81      0.54      0.65        93\n",
      "           5       0.65      0.71      0.68       108\n",
      "           6       0.81      0.83      0.82       126\n",
      "           7       0.70      0.73      0.71       136\n",
      "           8       0.76      0.61      0.68       122\n",
      "           9       0.77      0.65      0.71       155\n",
      "          10       0.70      0.66      0.68        61\n",
      "          11       0.58      0.69      0.63       172\n",
      "          12       0.72      0.76      0.74       182\n",
      "          13       0.64      0.58      0.61       151\n",
      "          14       0.53      0.64      0.58       200\n",
      "          15       0.78      0.85      0.82       169\n",
      "          16       0.80      0.99      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.69      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  20  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7882 - accuracy: 0.9704 - precision: 0.8037 - recall: 0.6584 - val_loss: 1.2321 - val_accuracy: 0.9640 - val_precision: 0.7351 - val_recall: 0.6068\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.51      0.54        68\n",
      "           1       0.79      0.75      0.77       176\n",
      "           2       0.73      0.74      0.73        97\n",
      "           3       0.71      0.60      0.65        90\n",
      "           4       0.79      0.54      0.64        93\n",
      "           5       0.64      0.72      0.68       108\n",
      "           6       0.80      0.83      0.81       126\n",
      "           7       0.72      0.72      0.72       136\n",
      "           8       0.77      0.59      0.67       122\n",
      "           9       0.78      0.68      0.72       155\n",
      "          10       0.68      0.64      0.66        61\n",
      "          11       0.60      0.67      0.63       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.64      0.58      0.61       151\n",
      "          14       0.53      0.64      0.58       200\n",
      "          15       0.79      0.86      0.83       169\n",
      "          16       0.80      0.99      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.69      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  21  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7693 - accuracy: 0.9713 - precision: 0.8096 - recall: 0.6697 - val_loss: 1.2175 - val_accuracy: 0.9628 - val_precision: 0.7266 - val_recall: 0.5902\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.50      0.54        68\n",
      "           1       0.79      0.76      0.77       176\n",
      "           2       0.74      0.72      0.73        97\n",
      "           3       0.71      0.61      0.65        90\n",
      "           4       0.81      0.55      0.65        93\n",
      "           5       0.65      0.73      0.69       108\n",
      "           6       0.80      0.83      0.81       126\n",
      "           7       0.73      0.74      0.73       136\n",
      "           8       0.75      0.60      0.67       122\n",
      "           9       0.80      0.68      0.74       155\n",
      "          10       0.67      0.64      0.66        61\n",
      "          11       0.58      0.69      0.63       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.65      0.58      0.61       151\n",
      "          14       0.54      0.65      0.59       200\n",
      "          15       0.79      0.87      0.83       169\n",
      "          16       0.80      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  22  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7748 - accuracy: 0.9712 - precision: 0.8100 - recall: 0.6664 - val_loss: 1.3023 - val_accuracy: 0.9612 - val_precision: 0.7097 - val_recall: 0.5753\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.47      0.51        68\n",
      "           1       0.78      0.75      0.76       176\n",
      "           2       0.74      0.72      0.73        97\n",
      "           3       0.72      0.61      0.66        90\n",
      "           4       0.81      0.54      0.65        93\n",
      "           5       0.66      0.72      0.69       108\n",
      "           6       0.81      0.84      0.82       126\n",
      "           7       0.72      0.71      0.72       136\n",
      "           8       0.74      0.61      0.67       122\n",
      "           9       0.78      0.68      0.73       155\n",
      "          10       0.70      0.64      0.67        61\n",
      "          11       0.59      0.66      0.62       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.64      0.58      0.61       151\n",
      "          14       0.52      0.64      0.57       200\n",
      "          15       0.78      0.88      0.83       169\n",
      "          16       0.81      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.69      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  23  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7493 - accuracy: 0.9721 - precision: 0.8182 - recall: 0.6767 - val_loss: 1.2658 - val_accuracy: 0.9646 - val_precision: 0.7392 - val_recall: 0.6148\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.50      0.54        68\n",
      "           1       0.79      0.75      0.77       176\n",
      "           2       0.74      0.71      0.73        97\n",
      "           3       0.70      0.62      0.66        90\n",
      "           4       0.80      0.52      0.63        93\n",
      "           5       0.67      0.74      0.70       108\n",
      "           6       0.80      0.84      0.82       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.77      0.61      0.68       122\n",
      "           9       0.80      0.69      0.74       155\n",
      "          10       0.69      0.66      0.67        61\n",
      "          11       0.58      0.68      0.63       172\n",
      "          12       0.71      0.76      0.73       182\n",
      "          13       0.63      0.58      0.60       151\n",
      "          14       0.56      0.63      0.59       200\n",
      "          15       0.78      0.88      0.83       169\n",
      "          16       0.82      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.70      2183\n",
      "\n",
      "At iteration  24  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7446 - accuracy: 0.9724 - precision: 0.8183 - recall: 0.6827 - val_loss: 1.2680 - val_accuracy: 0.9630 - val_precision: 0.7298 - val_recall: 0.5890\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.50      0.53        68\n",
      "           1       0.78      0.74      0.76       176\n",
      "           2       0.73      0.75      0.74        97\n",
      "           3       0.70      0.63      0.67        90\n",
      "           4       0.82      0.54      0.65        93\n",
      "           5       0.65      0.75      0.70       108\n",
      "           6       0.82      0.87      0.84       126\n",
      "           7       0.73      0.74      0.74       136\n",
      "           8       0.72      0.61      0.66       122\n",
      "           9       0.80      0.69      0.74       155\n",
      "          10       0.68      0.62      0.65        61\n",
      "          11       0.58      0.66      0.62       172\n",
      "          12       0.72      0.75      0.73       182\n",
      "          13       0.65      0.58      0.61       151\n",
      "          14       0.56      0.62      0.59       200\n",
      "          15       0.78      0.87      0.82       169\n",
      "          16       0.82      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.70      2183\n",
      "\n",
      "At iteration  25  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7318 - accuracy: 0.9729 - precision: 0.8213 - recall: 0.6895 - val_loss: 1.2845 - val_accuracy: 0.9630 - val_precision: 0.7211 - val_recall: 0.6039\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.50      0.54        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.69      0.72      0.70        97\n",
      "           3       0.70      0.60      0.65        90\n",
      "           4       0.79      0.52      0.62        93\n",
      "           5       0.67      0.74      0.70       108\n",
      "           6       0.80      0.84      0.82       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.72      0.62      0.67       122\n",
      "           9       0.78      0.68      0.72       155\n",
      "          10       0.65      0.64      0.64        61\n",
      "          11       0.60      0.64      0.62       172\n",
      "          12       0.70      0.77      0.74       182\n",
      "          13       0.64      0.56      0.60       151\n",
      "          14       0.54      0.64      0.59       200\n",
      "          15       0.80      0.88      0.83       169\n",
      "          16       0.79      0.99      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  26  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7223 - accuracy: 0.9727 - precision: 0.8203 - recall: 0.6870 - val_loss: 1.3024 - val_accuracy: 0.9635 - val_precision: 0.7233 - val_recall: 0.6136\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54        68\n",
      "           1       0.79      0.76      0.77       176\n",
      "           2       0.71      0.70      0.70        97\n",
      "           3       0.68      0.61      0.64        90\n",
      "           4       0.78      0.53      0.63        93\n",
      "           5       0.69      0.76      0.72       108\n",
      "           6       0.80      0.83      0.81       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.75      0.61      0.68       122\n",
      "           9       0.79      0.67      0.72       155\n",
      "          10       0.68      0.64      0.66        61\n",
      "          11       0.59      0.65      0.62       172\n",
      "          12       0.71      0.77      0.74       182\n",
      "          13       0.66      0.57      0.61       151\n",
      "          14       0.55      0.64      0.59       200\n",
      "          15       0.79      0.88      0.83       169\n",
      "          16       0.81      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  27  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.7001 - accuracy: 0.9736 - precision: 0.8267 - recall: 0.6985 - val_loss: 1.2775 - val_accuracy: 0.9634 - val_precision: 0.7258 - val_recall: 0.6062\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.49      0.50        68\n",
      "           1       0.79      0.76      0.77       176\n",
      "           2       0.74      0.69      0.71        97\n",
      "           3       0.69      0.62      0.65        90\n",
      "           4       0.80      0.55      0.65        93\n",
      "           5       0.66      0.74      0.70       108\n",
      "           6       0.82      0.84      0.83       126\n",
      "           7       0.75      0.74      0.74       136\n",
      "           8       0.75      0.63      0.68       122\n",
      "           9       0.80      0.68      0.74       155\n",
      "          10       0.71      0.59      0.64        61\n",
      "          11       0.59      0.67      0.63       172\n",
      "          12       0.71      0.76      0.73       182\n",
      "          13       0.65      0.58      0.62       151\n",
      "          14       0.55      0.65      0.60       200\n",
      "          15       0.80      0.88      0.84       169\n",
      "          16       0.82      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  28  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6847 - accuracy: 0.9743 - precision: 0.8298 - recall: 0.7078 - val_loss: 1.3080 - val_accuracy: 0.9629 - val_precision: 0.7224 - val_recall: 0.5987\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.51      0.53        68\n",
      "           1       0.77      0.74      0.75       176\n",
      "           2       0.70      0.73      0.72        97\n",
      "           3       0.70      0.61      0.65        90\n",
      "           4       0.78      0.54      0.64        93\n",
      "           5       0.68      0.73      0.70       108\n",
      "           6       0.82      0.83      0.82       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.73      0.61      0.66       122\n",
      "           9       0.80      0.69      0.74       155\n",
      "          10       0.66      0.62      0.64        61\n",
      "          11       0.59      0.66      0.63       172\n",
      "          12       0.72      0.77      0.75       182\n",
      "          13       0.63      0.60      0.62       151\n",
      "          14       0.56      0.64      0.60       200\n",
      "          15       0.80      0.88      0.84       169\n",
      "          16       0.81      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  29  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.7157 - accuracy: 0.9736 - precision: 0.8241 - recall: 0.7002 - val_loss: 1.3519 - val_accuracy: 0.9621 - val_precision: 0.7074 - val_recall: 0.6062\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.50      0.51        68\n",
      "           1       0.79      0.74      0.77       176\n",
      "           2       0.69      0.71      0.70        97\n",
      "           3       0.70      0.62      0.66        90\n",
      "           4       0.78      0.49      0.61        93\n",
      "           5       0.67      0.75      0.71       108\n",
      "           6       0.80      0.85      0.82       126\n",
      "           7       0.72      0.76      0.74       136\n",
      "           8       0.75      0.61      0.67       122\n",
      "           9       0.77      0.68      0.73       155\n",
      "          10       0.64      0.64      0.64        61\n",
      "          11       0.59      0.66      0.62       172\n",
      "          12       0.71      0.77      0.74       182\n",
      "          13       0.66      0.59      0.62       151\n",
      "          14       0.56      0.61      0.59       200\n",
      "          15       0.80      0.88      0.83       169\n",
      "          16       0.81      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  30  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.9739 - precision: 0.8230 - recall: 0.7091 - val_loss: 1.2831 - val_accuracy: 0.9615 - val_precision: 0.7079 - val_recall: 0.5896\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.51      0.50        68\n",
      "           1       0.78      0.77      0.78       176\n",
      "           2       0.71      0.72      0.72        97\n",
      "           3       0.69      0.63      0.66        90\n",
      "           4       0.78      0.53      0.63        93\n",
      "           5       0.65      0.74      0.69       108\n",
      "           6       0.82      0.84      0.83       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.71      0.60      0.65       122\n",
      "           9       0.78      0.67      0.72       155\n",
      "          10       0.67      0.64      0.66        61\n",
      "          11       0.59      0.65      0.62       172\n",
      "          12       0.74      0.75      0.74       182\n",
      "          13       0.66      0.60      0.63       151\n",
      "          14       0.56      0.62      0.59       200\n",
      "          15       0.80      0.86      0.83       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  31  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.9737 - precision: 0.8212 - recall: 0.7061 - val_loss: 1.3852 - val_accuracy: 0.9607 - val_precision: 0.7037 - val_recall: 0.5736\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.54      0.55        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.72      0.73      0.73        97\n",
      "           3       0.68      0.60      0.64        90\n",
      "           4       0.77      0.54      0.63        93\n",
      "           5       0.64      0.74      0.69       108\n",
      "           6       0.81      0.83      0.82       126\n",
      "           7       0.75      0.76      0.75       136\n",
      "           8       0.74      0.60      0.66       122\n",
      "           9       0.78      0.70      0.73       155\n",
      "          10       0.66      0.66      0.66        61\n",
      "          11       0.58      0.66      0.62       172\n",
      "          12       0.72      0.75      0.74       182\n",
      "          13       0.66      0.60      0.63       151\n",
      "          14       0.58      0.64      0.61       200\n",
      "          15       0.81      0.86      0.84       169\n",
      "          16       0.81      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  32  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6538 - accuracy: 0.9754 - precision: 0.8361 - recall: 0.7228 - val_loss: 1.3461 - val_accuracy: 0.9624 - val_precision: 0.7136 - val_recall: 0.6033\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.51      0.53        68\n",
      "           1       0.81      0.76      0.78       176\n",
      "           2       0.71      0.73      0.72        97\n",
      "           3       0.69      0.61      0.65        90\n",
      "           4       0.78      0.53      0.63        93\n",
      "           5       0.65      0.74      0.69       108\n",
      "           6       0.82      0.84      0.83       126\n",
      "           7       0.74      0.76      0.75       136\n",
      "           8       0.71      0.58      0.64       122\n",
      "           9       0.78      0.70      0.74       155\n",
      "          10       0.68      0.67      0.68        61\n",
      "          11       0.59      0.65      0.62       172\n",
      "          12       0.73      0.75      0.74       182\n",
      "          13       0.67      0.57      0.61       151\n",
      "          14       0.56      0.66      0.60       200\n",
      "          15       0.80      0.87      0.83       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  33  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6737 - accuracy: 0.9745 - precision: 0.8281 - recall: 0.7144 - val_loss: 1.3902 - val_accuracy: 0.9616 - val_precision: 0.7088 - val_recall: 0.5907\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.50      0.52        68\n",
      "           1       0.80      0.76      0.78       176\n",
      "           2       0.72      0.70      0.71        97\n",
      "           3       0.69      0.63      0.66        90\n",
      "           4       0.80      0.57      0.67        93\n",
      "           5       0.65      0.73      0.69       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.71      0.59      0.65       122\n",
      "           9       0.80      0.72      0.76       155\n",
      "          10       0.67      0.66      0.66        61\n",
      "          11       0.59      0.66      0.63       172\n",
      "          12       0.71      0.76      0.74       182\n",
      "          13       0.64      0.60      0.62       151\n",
      "          14       0.58      0.64      0.61       200\n",
      "          15       0.82      0.89      0.85       169\n",
      "          16       0.81      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  34  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.9749 - precision: 0.8328 - recall: 0.7165 - val_loss: 1.4014 - val_accuracy: 0.9631 - val_precision: 0.7175 - val_recall: 0.6136\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51        68\n",
      "           1       0.80      0.76      0.78       176\n",
      "           2       0.70      0.71      0.71        97\n",
      "           3       0.69      0.66      0.67        90\n",
      "           4       0.79      0.54      0.64        93\n",
      "           5       0.65      0.73      0.69       108\n",
      "           6       0.82      0.82      0.82       126\n",
      "           7       0.75      0.76      0.75       136\n",
      "           8       0.69      0.61      0.64       122\n",
      "           9       0.79      0.70      0.74       155\n",
      "          10       0.65      0.64      0.64        61\n",
      "          11       0.56      0.66      0.61       172\n",
      "          12       0.72      0.73      0.72       182\n",
      "          13       0.63      0.60      0.61       151\n",
      "          14       0.58      0.62      0.60       200\n",
      "          15       0.82      0.86      0.84       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  35  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6532 - accuracy: 0.9752 - precision: 0.8322 - recall: 0.7250 - val_loss: 1.4188 - val_accuracy: 0.9625 - val_precision: 0.7185 - val_recall: 0.5947\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.49      0.50        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.71      0.69      0.70        97\n",
      "           3       0.71      0.61      0.65        90\n",
      "           4       0.80      0.56      0.66        93\n",
      "           5       0.64      0.72      0.68       108\n",
      "           6       0.83      0.85      0.84       126\n",
      "           7       0.75      0.76      0.75       136\n",
      "           8       0.74      0.57      0.65       122\n",
      "           9       0.78      0.72      0.74       155\n",
      "          10       0.66      0.69      0.67        61\n",
      "          11       0.57      0.65      0.61       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.65      0.62      0.63       151\n",
      "          14       0.57      0.63      0.60       200\n",
      "          15       0.82      0.87      0.84       169\n",
      "          16       0.81      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  36  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6455 - accuracy: 0.9754 - precision: 0.8321 - recall: 0.7287 - val_loss: 1.3800 - val_accuracy: 0.9616 - val_precision: 0.7047 - val_recall: 0.5982\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.51      0.52        68\n",
      "           1       0.79      0.73      0.76       176\n",
      "           2       0.70      0.71      0.71        97\n",
      "           3       0.68      0.62      0.65        90\n",
      "           4       0.78      0.55      0.65        93\n",
      "           5       0.66      0.73      0.69       108\n",
      "           6       0.80      0.82      0.81       126\n",
      "           7       0.76      0.74      0.75       136\n",
      "           8       0.75      0.59      0.66       122\n",
      "           9       0.80      0.69      0.74       155\n",
      "          10       0.66      0.62      0.64        61\n",
      "          11       0.58      0.66      0.62       172\n",
      "          12       0.72      0.75      0.73       182\n",
      "          13       0.62      0.62      0.62       151\n",
      "          14       0.58      0.65      0.61       200\n",
      "          15       0.80      0.88      0.84       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  37  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6247 - accuracy: 0.9759 - precision: 0.8350 - recall: 0.7354 - val_loss: 1.3563 - val_accuracy: 0.9627 - val_precision: 0.7122 - val_recall: 0.6148\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.50      0.52        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.71      0.70      0.70        97\n",
      "           3       0.69      0.63      0.66        90\n",
      "           4       0.79      0.56      0.65        93\n",
      "           5       0.65      0.73      0.69       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.74      0.73      0.74       136\n",
      "           8       0.72      0.60      0.65       122\n",
      "           9       0.79      0.72      0.75       155\n",
      "          10       0.66      0.64      0.65        61\n",
      "          11       0.58      0.65      0.61       172\n",
      "          12       0.71      0.76      0.74       182\n",
      "          13       0.64      0.60      0.62       151\n",
      "          14       0.57      0.66      0.61       200\n",
      "          15       0.82      0.87      0.84       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  38  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6171 - accuracy: 0.9762 - precision: 0.8382 - recall: 0.7377 - val_loss: 1.4009 - val_accuracy: 0.9619 - val_precision: 0.7024 - val_recall: 0.6119\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.50      0.52        68\n",
      "           1       0.80      0.73      0.76       176\n",
      "           2       0.71      0.70      0.70        97\n",
      "           3       0.67      0.62      0.64        90\n",
      "           4       0.78      0.55      0.65        93\n",
      "           5       0.64      0.73      0.68       108\n",
      "           6       0.81      0.84      0.82       126\n",
      "           7       0.74      0.72      0.73       136\n",
      "           8       0.71      0.59      0.65       122\n",
      "           9       0.79      0.68      0.73       155\n",
      "          10       0.65      0.66      0.65        61\n",
      "          11       0.58      0.65      0.61       172\n",
      "          12       0.70      0.75      0.72       182\n",
      "          13       0.65      0.60      0.62       151\n",
      "          14       0.57      0.65      0.61       200\n",
      "          15       0.81      0.89      0.85       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  39  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6137 - accuracy: 0.9762 - precision: 0.8363 - recall: 0.7410 - val_loss: 1.4061 - val_accuracy: 0.9613 - val_precision: 0.7005 - val_recall: 0.5970\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.50      0.52        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.73      0.68      0.70        97\n",
      "           3       0.67      0.62      0.65        90\n",
      "           4       0.78      0.54      0.64        93\n",
      "           5       0.63      0.74      0.68       108\n",
      "           6       0.81      0.83      0.82       126\n",
      "           7       0.75      0.74      0.75       136\n",
      "           8       0.72      0.60      0.65       122\n",
      "           9       0.78      0.70      0.73       155\n",
      "          10       0.62      0.62      0.62        61\n",
      "          11       0.56      0.65      0.60       172\n",
      "          12       0.71      0.76      0.73       182\n",
      "          13       0.66      0.60      0.62       151\n",
      "          14       0.58      0.65      0.61       200\n",
      "          15       0.82      0.88      0.85       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  40  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6272 - accuracy: 0.9761 - precision: 0.8351 - recall: 0.7393 - val_loss: 1.5126 - val_accuracy: 0.9573 - val_precision: 0.6608 - val_recall: 0.5633\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.70      0.67      0.68        97\n",
      "           3       0.69      0.63      0.66        90\n",
      "           4       0.80      0.57      0.67        93\n",
      "           5       0.64      0.71      0.67       108\n",
      "           6       0.82      0.84      0.83       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.74      0.61      0.67       122\n",
      "           9       0.79      0.68      0.73       155\n",
      "          10       0.68      0.69      0.68        61\n",
      "          11       0.56      0.63      0.60       172\n",
      "          12       0.72      0.75      0.74       182\n",
      "          13       0.64      0.61      0.62       151\n",
      "          14       0.59      0.68      0.63       200\n",
      "          15       0.82      0.88      0.85       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  41  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6331 - accuracy: 0.9755 - precision: 0.8325 - recall: 0.7310 - val_loss: 1.4724 - val_accuracy: 0.9606 - val_precision: 0.6879 - val_recall: 0.6045\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.49      0.50        68\n",
      "           1       0.78      0.76      0.77       176\n",
      "           2       0.70      0.69      0.69        97\n",
      "           3       0.68      0.58      0.62        90\n",
      "           4       0.79      0.56      0.65        93\n",
      "           5       0.67      0.74      0.70       108\n",
      "           6       0.82      0.84      0.83       126\n",
      "           7       0.76      0.74      0.75       136\n",
      "           8       0.68      0.59      0.63       122\n",
      "           9       0.78      0.68      0.73       155\n",
      "          10       0.66      0.64      0.65        61\n",
      "          11       0.55      0.62      0.58       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.63      0.58      0.60       151\n",
      "          14       0.56      0.65      0.60       200\n",
      "          15       0.81      0.88      0.84       169\n",
      "          16       0.80      0.96      0.87        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  42  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5885 - accuracy: 0.9774 - precision: 0.8453 - recall: 0.7530 - val_loss: 1.4536 - val_accuracy: 0.9622 - val_precision: 0.7053 - val_recall: 0.6125\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.51      0.55        68\n",
      "           1       0.79      0.76      0.77       176\n",
      "           2       0.70      0.70      0.70        97\n",
      "           3       0.68      0.62      0.65        90\n",
      "           4       0.77      0.52      0.62        93\n",
      "           5       0.65      0.73      0.69       108\n",
      "           6       0.81      0.83      0.82       126\n",
      "           7       0.75      0.74      0.75       136\n",
      "           8       0.72      0.58      0.64       122\n",
      "           9       0.76      0.68      0.72       155\n",
      "          10       0.67      0.66      0.66        61\n",
      "          11       0.57      0.65      0.61       172\n",
      "          12       0.72      0.75      0.73       182\n",
      "          13       0.64      0.61      0.62       151\n",
      "          14       0.57      0.65      0.61       200\n",
      "          15       0.80      0.87      0.84       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  43  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.9780 - precision: 0.8481 - recall: 0.7623 - val_loss: 1.4280 - val_accuracy: 0.9624 - val_precision: 0.7072 - val_recall: 0.6165\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.51      0.54        68\n",
      "           1       0.80      0.76      0.78       176\n",
      "           2       0.72      0.69      0.71        97\n",
      "           3       0.71      0.64      0.67        90\n",
      "           4       0.80      0.57      0.67        93\n",
      "           5       0.64      0.70      0.67       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.74      0.76      0.75       136\n",
      "           8       0.70      0.61      0.65       122\n",
      "           9       0.77      0.71      0.74       155\n",
      "          10       0.71      0.64      0.67        61\n",
      "          11       0.57      0.63      0.60       172\n",
      "          12       0.70      0.76      0.73       182\n",
      "          13       0.63      0.61      0.62       151\n",
      "          14       0.56      0.62      0.59       200\n",
      "          15       0.82      0.88      0.85       169\n",
      "          16       0.83      0.97      0.90        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  44  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5589 - accuracy: 0.9785 - precision: 0.8521 - recall: 0.7669 - val_loss: 1.4981 - val_accuracy: 0.9611 - val_precision: 0.6923 - val_recall: 0.6090\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.49      0.53        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.69      0.71      0.70        97\n",
      "           3       0.70      0.63      0.67        90\n",
      "           4       0.80      0.57      0.67        93\n",
      "           5       0.63      0.72      0.67       108\n",
      "           6       0.82      0.87      0.84       126\n",
      "           7       0.75      0.72      0.73       136\n",
      "           8       0.70      0.60      0.64       122\n",
      "           9       0.80      0.71      0.75       155\n",
      "          10       0.67      0.62      0.64        61\n",
      "          11       0.56      0.64      0.60       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.65      0.62      0.63       151\n",
      "          14       0.57      0.65      0.60       200\n",
      "          15       0.83      0.88      0.85       169\n",
      "          16       0.83      0.97      0.90        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.70      2183\n",
      "\n",
      "At iteration  45  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5494 - accuracy: 0.9785 - precision: 0.8513 - recall: 0.7695 - val_loss: 1.4062 - val_accuracy: 0.9625 - val_precision: 0.7093 - val_recall: 0.6131\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.51      0.55        68\n",
      "           1       0.80      0.73      0.76       176\n",
      "           2       0.70      0.71      0.71        97\n",
      "           3       0.71      0.63      0.67        90\n",
      "           4       0.80      0.56      0.66        93\n",
      "           5       0.63      0.71      0.67       108\n",
      "           6       0.82      0.86      0.84       126\n",
      "           7       0.74      0.75      0.75       136\n",
      "           8       0.67      0.62      0.65       122\n",
      "           9       0.77      0.70      0.73       155\n",
      "          10       0.69      0.62      0.66        61\n",
      "          11       0.56      0.66      0.61       172\n",
      "          12       0.72      0.74      0.73       182\n",
      "          13       0.63      0.61      0.62       151\n",
      "          14       0.58      0.64      0.61       200\n",
      "          15       0.83      0.88      0.85       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  46  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5589 - accuracy: 0.9789 - precision: 0.8565 - recall: 0.7705 - val_loss: 1.4836 - val_accuracy: 0.9612 - val_precision: 0.6938 - val_recall: 0.6108\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54        68\n",
      "           1       0.79      0.74      0.76       176\n",
      "           2       0.71      0.70      0.70        97\n",
      "           3       0.67      0.67      0.67        90\n",
      "           4       0.76      0.56      0.65        93\n",
      "           5       0.65      0.71      0.68       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.73      0.74      0.74       136\n",
      "           8       0.70      0.64      0.67       122\n",
      "           9       0.78      0.70      0.74       155\n",
      "          10       0.70      0.64      0.67        61\n",
      "          11       0.56      0.65      0.60       172\n",
      "          12       0.74      0.74      0.74       182\n",
      "          13       0.63      0.61      0.62       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.81      0.88      0.85       169\n",
      "          16       0.84      0.96      0.90        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  47  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5745 - accuracy: 0.9776 - precision: 0.8438 - recall: 0.7609 - val_loss: 1.5135 - val_accuracy: 0.9625 - val_precision: 0.7070 - val_recall: 0.6188\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.55        68\n",
      "           1       0.81      0.74      0.77       176\n",
      "           2       0.71      0.70      0.70        97\n",
      "           3       0.67      0.64      0.66        90\n",
      "           4       0.77      0.54      0.63        93\n",
      "           5       0.63      0.72      0.68       108\n",
      "           6       0.82      0.82      0.82       126\n",
      "           7       0.77      0.74      0.75       136\n",
      "           8       0.71      0.61      0.66       122\n",
      "           9       0.79      0.69      0.74       155\n",
      "          10       0.67      0.62      0.64        61\n",
      "          11       0.56      0.66      0.60       172\n",
      "          12       0.73      0.75      0.74       182\n",
      "          13       0.63      0.62      0.62       151\n",
      "          14       0.58      0.63      0.60       200\n",
      "          15       0.81      0.89      0.85       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  48  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5611 - accuracy: 0.9776 - precision: 0.8444 - recall: 0.7589 - val_loss: 1.4463 - val_accuracy: 0.9637 - val_precision: 0.7181 - val_recall: 0.6314\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.55        68\n",
      "           1       0.78      0.76      0.77       176\n",
      "           2       0.68      0.71      0.70        97\n",
      "           3       0.70      0.63      0.66        90\n",
      "           4       0.77      0.57      0.65        93\n",
      "           5       0.64      0.73      0.68       108\n",
      "           6       0.84      0.85      0.84       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.69      0.58      0.63       122\n",
      "           9       0.78      0.68      0.73       155\n",
      "          10       0.62      0.66      0.64        61\n",
      "          11       0.58      0.63      0.61       172\n",
      "          12       0.74      0.75      0.74       182\n",
      "          13       0.62      0.62      0.62       151\n",
      "          14       0.59      0.62      0.61       200\n",
      "          15       0.82      0.88      0.85       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  49  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5417 - accuracy: 0.9789 - precision: 0.8518 - recall: 0.7758 - val_loss: 1.5524 - val_accuracy: 0.9600 - val_precision: 0.6860 - val_recall: 0.5890\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54        68\n",
      "           1       0.79      0.75      0.77       176\n",
      "           2       0.70      0.72      0.71        97\n",
      "           3       0.69      0.66      0.67        90\n",
      "           4       0.76      0.56      0.65        93\n",
      "           5       0.68      0.73      0.70       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.73      0.75      0.74       136\n",
      "           8       0.67      0.57      0.62       122\n",
      "           9       0.80      0.70      0.75       155\n",
      "          10       0.68      0.66      0.67        61\n",
      "          11       0.60      0.65      0.62       172\n",
      "          12       0.71      0.74      0.73       182\n",
      "          13       0.66      0.63      0.64       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.82      0.89      0.86       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.71      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  50  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5398 - accuracy: 0.9792 - precision: 0.8562 - recall: 0.7768 - val_loss: 1.5248 - val_accuracy: 0.9607 - val_precision: 0.6908 - val_recall: 0.6010\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.51      0.55        68\n",
      "           1       0.79      0.76      0.77       176\n",
      "           2       0.71      0.74      0.72        97\n",
      "           3       0.64      0.64      0.64        90\n",
      "           4       0.76      0.56      0.65        93\n",
      "           5       0.64      0.69      0.67       108\n",
      "           6       0.83      0.82      0.82       126\n",
      "           7       0.73      0.74      0.74       136\n",
      "           8       0.68      0.58      0.63       122\n",
      "           9       0.78      0.69      0.73       155\n",
      "          10       0.69      0.67      0.68        61\n",
      "          11       0.59      0.65      0.62       172\n",
      "          12       0.72      0.74      0.73       182\n",
      "          13       0.65      0.63      0.64       151\n",
      "          14       0.56      0.64      0.60       200\n",
      "          15       0.83      0.87      0.85       169\n",
      "          16       0.84      0.96      0.90        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  51  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5551 - accuracy: 0.9783 - precision: 0.8469 - recall: 0.7707 - val_loss: 1.4743 - val_accuracy: 0.9612 - val_precision: 0.6961 - val_recall: 0.6045\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.51      0.54        68\n",
      "           1       0.79      0.75      0.77       176\n",
      "           2       0.69      0.70      0.70        97\n",
      "           3       0.66      0.66      0.66        90\n",
      "           4       0.75      0.55      0.63        93\n",
      "           5       0.64      0.73      0.68       108\n",
      "           6       0.83      0.84      0.83       126\n",
      "           7       0.75      0.76      0.75       136\n",
      "           8       0.70      0.61      0.65       122\n",
      "           9       0.78      0.69      0.73       155\n",
      "          10       0.67      0.70      0.69        61\n",
      "          11       0.61      0.63      0.62       172\n",
      "          12       0.70      0.74      0.72       182\n",
      "          13       0.66      0.62      0.64       151\n",
      "          14       0.58      0.65      0.61       200\n",
      "          15       0.84      0.87      0.85       169\n",
      "          16       0.81      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  52  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5156 - accuracy: 0.9800 - precision: 0.8610 - recall: 0.7867 - val_loss: 1.5268 - val_accuracy: 0.9614 - val_precision: 0.6953 - val_recall: 0.6125\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.69      0.71      0.70        97\n",
      "           3       0.68      0.62      0.65        90\n",
      "           4       0.78      0.53      0.63        93\n",
      "           5       0.65      0.73      0.69       108\n",
      "           6       0.83      0.84      0.84       126\n",
      "           7       0.74      0.76      0.75       136\n",
      "           8       0.69      0.61      0.65       122\n",
      "           9       0.78      0.69      0.73       155\n",
      "          10       0.69      0.67      0.68        61\n",
      "          11       0.58      0.62      0.60       172\n",
      "          12       0.71      0.74      0.73       182\n",
      "          13       0.64      0.63      0.64       151\n",
      "          14       0.58      0.66      0.62       200\n",
      "          15       0.84      0.87      0.85       169\n",
      "          16       0.81      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  53  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5247 - accuracy: 0.9800 - precision: 0.8621 - recall: 0.7858 - val_loss: 1.5786 - val_accuracy: 0.9619 - val_precision: 0.7011 - val_recall: 0.6136\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.51      0.53        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.69      0.70      0.70        97\n",
      "           3       0.65      0.63      0.64        90\n",
      "           4       0.75      0.53      0.62        93\n",
      "           5       0.64      0.72      0.68       108\n",
      "           6       0.84      0.82      0.83       126\n",
      "           7       0.75      0.74      0.75       136\n",
      "           8       0.70      0.60      0.65       122\n",
      "           9       0.77      0.70      0.73       155\n",
      "          10       0.69      0.69      0.69        61\n",
      "          11       0.56      0.65      0.60       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.64      0.63      0.64       151\n",
      "          14       0.59      0.61      0.60       200\n",
      "          15       0.80      0.88      0.84       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  54  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5823 - accuracy: 0.9774 - precision: 0.8423 - recall: 0.7581 - val_loss: 1.4884 - val_accuracy: 0.9620 - val_precision: 0.6966 - val_recall: 0.6256\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.55        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.68      0.71      0.70        97\n",
      "           3       0.67      0.60      0.63        90\n",
      "           4       0.75      0.55      0.63        93\n",
      "           5       0.64      0.73      0.68       108\n",
      "           6       0.83      0.79      0.81       126\n",
      "           7       0.74      0.76      0.75       136\n",
      "           8       0.67      0.57      0.62       122\n",
      "           9       0.78      0.70      0.73       155\n",
      "          10       0.70      0.66      0.68        61\n",
      "          11       0.56      0.63      0.60       172\n",
      "          12       0.70      0.75      0.72       182\n",
      "          13       0.63      0.64      0.64       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.82      0.85      0.83       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  55  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5301 - accuracy: 0.9795 - precision: 0.8569 - recall: 0.7818 - val_loss: 1.6698 - val_accuracy: 0.9601 - val_precision: 0.6799 - val_recall: 0.6090\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.54      0.53        68\n",
      "           1       0.79      0.74      0.76       176\n",
      "           2       0.68      0.70      0.69        97\n",
      "           3       0.68      0.64      0.66        90\n",
      "           4       0.76      0.54      0.63        93\n",
      "           5       0.65      0.71      0.68       108\n",
      "           6       0.83      0.79      0.81       126\n",
      "           7       0.74      0.76      0.75       136\n",
      "           8       0.69      0.62      0.66       122\n",
      "           9       0.76      0.68      0.72       155\n",
      "          10       0.73      0.66      0.69        61\n",
      "          11       0.57      0.60      0.58       172\n",
      "          12       0.72      0.75      0.74       182\n",
      "          13       0.64      0.64      0.64       151\n",
      "          14       0.57      0.65      0.61       200\n",
      "          15       0.82      0.86      0.84       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  56  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5249 - accuracy: 0.9793 - precision: 0.8544 - recall: 0.7820 - val_loss: 1.6296 - val_accuracy: 0.9612 - val_precision: 0.6868 - val_recall: 0.6262\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54        68\n",
      "           1       0.81      0.74      0.78       176\n",
      "           2       0.69      0.70      0.70        97\n",
      "           3       0.65      0.64      0.65        90\n",
      "           4       0.77      0.53      0.62        93\n",
      "           5       0.64      0.71      0.67       108\n",
      "           6       0.84      0.83      0.83       126\n",
      "           7       0.74      0.77      0.76       136\n",
      "           8       0.67      0.61      0.64       122\n",
      "           9       0.79      0.68      0.73       155\n",
      "          10       0.68      0.66      0.67        61\n",
      "          11       0.57      0.65      0.61       172\n",
      "          12       0.73      0.75      0.74       182\n",
      "          13       0.66      0.62      0.64       151\n",
      "          14       0.58      0.61      0.60       200\n",
      "          15       0.80      0.88      0.84       169\n",
      "          16       0.81      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  57  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4655 - accuracy: 0.9810 - precision: 0.8664 - recall: 0.8000 - val_loss: 1.6478 - val_accuracy: 0.9603 - val_precision: 0.6800 - val_recall: 0.6131\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54        68\n",
      "           1       0.79      0.74      0.77       176\n",
      "           2       0.71      0.72      0.71        97\n",
      "           3       0.67      0.64      0.66        90\n",
      "           4       0.76      0.56      0.65        93\n",
      "           5       0.65      0.73      0.69       108\n",
      "           6       0.83      0.80      0.82       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.67      0.63      0.65       122\n",
      "           9       0.79      0.69      0.74       155\n",
      "          10       0.71      0.69      0.70        61\n",
      "          11       0.57      0.64      0.60       172\n",
      "          12       0.73      0.73      0.73       182\n",
      "          13       0.68      0.62      0.65       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.80      0.89      0.84       169\n",
      "          16       0.81      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  58  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4649 - accuracy: 0.9814 - precision: 0.8700 - recall: 0.8039 - val_loss: 1.6364 - val_accuracy: 0.9612 - val_precision: 0.6896 - val_recall: 0.6193\n",
      "69/69 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.54      0.55        68\n",
      "           1       0.79      0.72      0.75       176\n",
      "           2       0.67      0.74      0.71        97\n",
      "           3       0.64      0.62      0.63        90\n",
      "           4       0.76      0.57      0.65        93\n",
      "           5       0.64      0.72      0.68       108\n",
      "           6       0.84      0.80      0.82       126\n",
      "           7       0.74      0.76      0.75       136\n",
      "           8       0.69      0.61      0.64       122\n",
      "           9       0.77      0.70      0.73       155\n",
      "          10       0.66      0.70      0.68        61\n",
      "          11       0.58      0.63      0.60       172\n",
      "          12       0.72      0.73      0.72       182\n",
      "          13       0.65      0.60      0.62       151\n",
      "          14       0.59      0.62      0.60       200\n",
      "          15       0.81      0.88      0.84       169\n",
      "          16       0.79      0.95      0.86        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  59  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.5317 - accuracy: 0.9793 - precision: 0.8527 - recall: 0.7830 - val_loss: 1.6590 - val_accuracy: 0.9604 - val_precision: 0.6843 - val_recall: 0.6056\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.56      0.56        68\n",
      "           1       0.78      0.74      0.76       176\n",
      "           2       0.69      0.73      0.71        97\n",
      "           3       0.65      0.63      0.64        90\n",
      "           4       0.75      0.55      0.63        93\n",
      "           5       0.63      0.71      0.67       108\n",
      "           6       0.82      0.82      0.82       126\n",
      "           7       0.74      0.75      0.75       136\n",
      "           8       0.68      0.59      0.63       122\n",
      "           9       0.79      0.70      0.74       155\n",
      "          10       0.65      0.70      0.68        61\n",
      "          11       0.58      0.63      0.61       172\n",
      "          12       0.74      0.74      0.74       182\n",
      "          13       0.66      0.62      0.64       151\n",
      "          14       0.59      0.65      0.62       200\n",
      "          15       0.83      0.86      0.85       169\n",
      "          16       0.81      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  60  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5205 - accuracy: 0.9794 - precision: 0.8559 - recall: 0.7817 - val_loss: 1.6627 - val_accuracy: 0.9628 - val_precision: 0.7012 - val_recall: 0.6394\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56        68\n",
      "           1       0.81      0.76      0.78       176\n",
      "           2       0.67      0.69      0.68        97\n",
      "           3       0.64      0.62      0.63        90\n",
      "           4       0.77      0.58      0.66        93\n",
      "           5       0.66      0.72      0.69       108\n",
      "           6       0.83      0.79      0.81       126\n",
      "           7       0.73      0.74      0.73       136\n",
      "           8       0.68      0.61      0.64       122\n",
      "           9       0.80      0.68      0.73       155\n",
      "          10       0.67      0.70      0.69        61\n",
      "          11       0.58      0.65      0.61       172\n",
      "          12       0.72      0.75      0.73       182\n",
      "          13       0.66      0.62      0.64       151\n",
      "          14       0.61      0.68      0.64       200\n",
      "          15       0.82      0.86      0.84       169\n",
      "          16       0.81      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  61  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4907 - accuracy: 0.9809 - precision: 0.8659 - recall: 0.7989 - val_loss: 1.5983 - val_accuracy: 0.9600 - val_precision: 0.6795 - val_recall: 0.6056\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.79      0.76      0.77       176\n",
      "           2       0.67      0.67      0.67        97\n",
      "           3       0.68      0.66      0.67        90\n",
      "           4       0.77      0.57      0.65        93\n",
      "           5       0.65      0.70      0.68       108\n",
      "           6       0.81      0.79      0.80       126\n",
      "           7       0.72      0.76      0.74       136\n",
      "           8       0.66      0.64      0.65       122\n",
      "           9       0.81      0.67      0.73       155\n",
      "          10       0.66      0.66      0.66        61\n",
      "          11       0.58      0.63      0.61       172\n",
      "          12       0.72      0.75      0.73       182\n",
      "          13       0.64      0.61      0.62       151\n",
      "          14       0.60      0.65      0.62       200\n",
      "          15       0.82      0.86      0.84       169\n",
      "          16       0.84      0.96      0.90        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  62  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4767 - accuracy: 0.9807 - precision: 0.8632 - recall: 0.7991 - val_loss: 1.7346 - val_accuracy: 0.9602 - val_precision: 0.6752 - val_recall: 0.6222\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54        68\n",
      "           1       0.80      0.76      0.78       176\n",
      "           2       0.69      0.68      0.69        97\n",
      "           3       0.70      0.64      0.67        90\n",
      "           4       0.76      0.59      0.67        93\n",
      "           5       0.65      0.69      0.67       108\n",
      "           6       0.83      0.80      0.81       126\n",
      "           7       0.73      0.76      0.75       136\n",
      "           8       0.68      0.62      0.65       122\n",
      "           9       0.79      0.68      0.73       155\n",
      "          10       0.69      0.67      0.68        61\n",
      "          11       0.57      0.63      0.60       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.65      0.63      0.64       151\n",
      "          14       0.61      0.66      0.63       200\n",
      "          15       0.81      0.88      0.84       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  63  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4653 - accuracy: 0.9813 - precision: 0.8706 - recall: 0.8016 - val_loss: 1.6352 - val_accuracy: 0.9610 - val_precision: 0.6826 - val_recall: 0.6291\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.57      0.58        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.68      0.71      0.69        97\n",
      "           3       0.65      0.61      0.63        90\n",
      "           4       0.75      0.56      0.64        93\n",
      "           5       0.62      0.69      0.65       108\n",
      "           6       0.82      0.77      0.79       126\n",
      "           7       0.75      0.77      0.76       136\n",
      "           8       0.70      0.60      0.65       122\n",
      "           9       0.79      0.66      0.72       155\n",
      "          10       0.73      0.67      0.70        61\n",
      "          11       0.58      0.63      0.60       172\n",
      "          12       0.73      0.76      0.74       182\n",
      "          13       0.65      0.64      0.64       151\n",
      "          14       0.57      0.66      0.61       200\n",
      "          15       0.81      0.88      0.84       169\n",
      "          16       0.81      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  64  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4506 - accuracy: 0.9826 - precision: 0.8779 - recall: 0.8176 - val_loss: 1.7234 - val_accuracy: 0.9591 - val_precision: 0.6736 - val_recall: 0.5907\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57        68\n",
      "           1       0.81      0.74      0.77       176\n",
      "           2       0.70      0.70      0.70        97\n",
      "           3       0.64      0.62      0.63        90\n",
      "           4       0.73      0.55      0.63        93\n",
      "           5       0.61      0.69      0.65       108\n",
      "           6       0.81      0.77      0.79       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.68      0.61      0.64       122\n",
      "           9       0.80      0.66      0.73       155\n",
      "          10       0.68      0.69      0.68        61\n",
      "          11       0.56      0.62      0.59       172\n",
      "          12       0.72      0.75      0.73       182\n",
      "          13       0.65      0.62      0.64       151\n",
      "          14       0.57      0.68      0.62       200\n",
      "          15       0.81      0.85      0.83       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  65  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.4708 - accuracy: 0.9809 - precision: 0.8648 - recall: 0.8013 - val_loss: 1.6814 - val_accuracy: 0.9615 - val_precision: 0.6899 - val_recall: 0.6291\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54        68\n",
      "           1       0.81      0.73      0.77       176\n",
      "           2       0.65      0.67      0.66        97\n",
      "           3       0.65      0.59      0.62        90\n",
      "           4       0.74      0.55      0.63        93\n",
      "           5       0.60      0.68      0.64       108\n",
      "           6       0.81      0.79      0.80       126\n",
      "           7       0.73      0.77      0.75       136\n",
      "           8       0.69      0.58      0.63       122\n",
      "           9       0.77      0.68      0.72       155\n",
      "          10       0.70      0.70      0.70        61\n",
      "          11       0.56      0.61      0.58       172\n",
      "          12       0.70      0.74      0.72       182\n",
      "          13       0.65      0.60      0.63       151\n",
      "          14       0.58      0.65      0.61       200\n",
      "          15       0.82      0.89      0.85       169\n",
      "          16       0.80      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.69      0.69      0.69      2183\n",
      "weighted avg       0.70      0.69      0.69      2183\n",
      "\n",
      "At iteration  66  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4373 - accuracy: 0.9827 - precision: 0.8759 - recall: 0.8228 - val_loss: 1.7979 - val_accuracy: 0.9599 - val_precision: 0.6733 - val_recall: 0.6193\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54        68\n",
      "           1       0.81      0.74      0.77       176\n",
      "           2       0.69      0.66      0.67        97\n",
      "           3       0.67      0.60      0.63        90\n",
      "           4       0.74      0.56      0.64        93\n",
      "           5       0.62      0.68      0.65       108\n",
      "           6       0.80      0.80      0.80       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.70      0.64      0.67       122\n",
      "           9       0.80      0.68      0.74       155\n",
      "          10       0.67      0.67      0.67        61\n",
      "          11       0.55      0.63      0.59       172\n",
      "          12       0.70      0.75      0.73       182\n",
      "          13       0.64      0.62      0.63       151\n",
      "          14       0.59      0.63      0.61       200\n",
      "          15       0.83      0.89      0.86       169\n",
      "          16       0.80      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  67  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4383 - accuracy: 0.9825 - precision: 0.8781 - recall: 0.8156 - val_loss: 1.7711 - val_accuracy: 0.9618 - val_precision: 0.6920 - val_recall: 0.6314\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.56      0.55        68\n",
      "           1       0.81      0.75      0.78       176\n",
      "           2       0.69      0.67      0.68        97\n",
      "           3       0.64      0.59      0.61        90\n",
      "           4       0.73      0.52      0.60        93\n",
      "           5       0.62      0.69      0.66       108\n",
      "           6       0.82      0.79      0.80       126\n",
      "           7       0.72      0.73      0.72       136\n",
      "           8       0.67      0.66      0.66       122\n",
      "           9       0.78      0.69      0.73       155\n",
      "          10       0.70      0.69      0.69        61\n",
      "          11       0.56      0.62      0.59       172\n",
      "          12       0.72      0.75      0.74       182\n",
      "          13       0.64      0.60      0.62       151\n",
      "          14       0.60      0.64      0.62       200\n",
      "          15       0.83      0.89      0.86       169\n",
      "          16       0.78      0.95      0.86        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  68  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4452 - accuracy: 0.9824 - precision: 0.8742 - recall: 0.8188 - val_loss: 1.7949 - val_accuracy: 0.9598 - val_precision: 0.6738 - val_recall: 0.6136\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56        68\n",
      "           1       0.82      0.74      0.78       176\n",
      "           2       0.67      0.66      0.67        97\n",
      "           3       0.65      0.64      0.65        90\n",
      "           4       0.72      0.54      0.62        93\n",
      "           5       0.62      0.68      0.65       108\n",
      "           6       0.84      0.78      0.81       126\n",
      "           7       0.73      0.74      0.73       136\n",
      "           8       0.65      0.66      0.65       122\n",
      "           9       0.80      0.68      0.73       155\n",
      "          10       0.67      0.67      0.67        61\n",
      "          11       0.55      0.64      0.59       172\n",
      "          12       0.71      0.76      0.73       182\n",
      "          13       0.65      0.60      0.62       151\n",
      "          14       0.59      0.65      0.62       200\n",
      "          15       0.83      0.87      0.85       169\n",
      "          16       0.81      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  69  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4274 - accuracy: 0.9828 - precision: 0.8783 - recall: 0.8213 - val_loss: 1.7857 - val_accuracy: 0.9607 - val_precision: 0.6795 - val_recall: 0.6285\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.56      0.55        68\n",
      "           1       0.82      0.74      0.78       176\n",
      "           2       0.69      0.68      0.68        97\n",
      "           3       0.67      0.64      0.66        90\n",
      "           4       0.73      0.56      0.63        93\n",
      "           5       0.61      0.69      0.65       108\n",
      "           6       0.79      0.76      0.77       126\n",
      "           7       0.73      0.75      0.74       136\n",
      "           8       0.68      0.66      0.67       122\n",
      "           9       0.78      0.70      0.74       155\n",
      "          10       0.71      0.67      0.69        61\n",
      "          11       0.57      0.60      0.58       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.66      0.62      0.64       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.83      0.89      0.86       169\n",
      "          16       0.84      0.95      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  70  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.4631 - accuracy: 0.9810 - precision: 0.8649 - recall: 0.8029 - val_loss: 1.7999 - val_accuracy: 0.9596 - val_precision: 0.6696 - val_recall: 0.6193\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.58        68\n",
      "           1       0.81      0.74      0.78       176\n",
      "           2       0.71      0.67      0.69        97\n",
      "           3       0.68      0.66      0.67        90\n",
      "           4       0.73      0.52      0.60        93\n",
      "           5       0.60      0.69      0.64       108\n",
      "           6       0.80      0.76      0.78       126\n",
      "           7       0.76      0.75      0.75       136\n",
      "           8       0.69      0.61      0.65       122\n",
      "           9       0.79      0.69      0.74       155\n",
      "          10       0.68      0.70      0.69        61\n",
      "          11       0.54      0.63      0.58       172\n",
      "          12       0.71      0.74      0.73       182\n",
      "          13       0.65      0.62      0.64       151\n",
      "          14       0.58      0.63      0.60       200\n",
      "          15       0.81      0.89      0.85       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  71  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3973 - accuracy: 0.9840 - precision: 0.8870 - recall: 0.8351 - val_loss: 1.8396 - val_accuracy: 0.9602 - val_precision: 0.6726 - val_recall: 0.6314\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57        68\n",
      "           1       0.82      0.76      0.79       176\n",
      "           2       0.71      0.67      0.69        97\n",
      "           3       0.67      0.67      0.67        90\n",
      "           4       0.75      0.57      0.65        93\n",
      "           5       0.60      0.71      0.65       108\n",
      "           6       0.81      0.79      0.80       126\n",
      "           7       0.76      0.74      0.75       136\n",
      "           8       0.64      0.63      0.64       122\n",
      "           9       0.81      0.68      0.74       155\n",
      "          10       0.68      0.70      0.69        61\n",
      "          11       0.58      0.65      0.62       172\n",
      "          12       0.70      0.76      0.73       182\n",
      "          13       0.66      0.62      0.64       151\n",
      "          14       0.59      0.62      0.61       200\n",
      "          15       0.83      0.88      0.85       169\n",
      "          16       0.84      0.95      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  72  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3740 - accuracy: 0.9852 - precision: 0.8962 - recall: 0.8468 - val_loss: 1.8699 - val_accuracy: 0.9602 - val_precision: 0.6749 - val_recall: 0.6239\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.82      0.76      0.79       176\n",
      "           2       0.68      0.67      0.67        97\n",
      "           3       0.68      0.64      0.66        90\n",
      "           4       0.75      0.56      0.64        93\n",
      "           5       0.60      0.69      0.64       108\n",
      "           6       0.81      0.77      0.79       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.66      0.64      0.65       122\n",
      "           9       0.80      0.70      0.74       155\n",
      "          10       0.71      0.69      0.70        61\n",
      "          11       0.60      0.62      0.61       172\n",
      "          12       0.71      0.75      0.73       182\n",
      "          13       0.64      0.63      0.64       151\n",
      "          14       0.59      0.65      0.62       200\n",
      "          15       0.82      0.88      0.85       169\n",
      "          16       0.82      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  73  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.4509 - accuracy: 0.9816 - precision: 0.8669 - recall: 0.8129 - val_loss: 1.9260 - val_accuracy: 0.9594 - val_precision: 0.6661 - val_recall: 0.6199\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.81      0.75      0.78       176\n",
      "           2       0.75      0.65      0.70        97\n",
      "           3       0.64      0.64      0.64        90\n",
      "           4       0.75      0.55      0.63        93\n",
      "           5       0.59      0.68      0.63       108\n",
      "           6       0.82      0.80      0.81       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.67      0.64      0.65       122\n",
      "           9       0.79      0.67      0.73       155\n",
      "          10       0.67      0.69      0.68        61\n",
      "          11       0.57      0.65      0.61       172\n",
      "          12       0.73      0.75      0.74       182\n",
      "          13       0.65      0.64      0.64       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.82      0.86      0.84       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  74  we get these data :\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.4510 - accuracy: 0.9819 - precision: 0.8680 - recall: 0.8163 - val_loss: 1.7941 - val_accuracy: 0.9616 - val_precision: 0.6872 - val_recall: 0.6388\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.57      0.58        68\n",
      "           1       0.81      0.74      0.78       176\n",
      "           2       0.69      0.70      0.69        97\n",
      "           3       0.66      0.63      0.64        90\n",
      "           4       0.76      0.54      0.63        93\n",
      "           5       0.57      0.68      0.62       108\n",
      "           6       0.79      0.75      0.77       126\n",
      "           7       0.75      0.74      0.74       136\n",
      "           8       0.67      0.62      0.64       122\n",
      "           9       0.79      0.69      0.74       155\n",
      "          10       0.67      0.66      0.66        61\n",
      "          11       0.57      0.63      0.60       172\n",
      "          12       0.69      0.73      0.71       182\n",
      "          13       0.66      0.63      0.64       151\n",
      "          14       0.60      0.65      0.62       200\n",
      "          15       0.83      0.88      0.85       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  75  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3811 - accuracy: 0.9846 - precision: 0.8887 - recall: 0.8447 - val_loss: 1.8605 - val_accuracy: 0.9592 - val_precision: 0.6654 - val_recall: 0.6171\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.81      0.73      0.77       176\n",
      "           2       0.67      0.65      0.66        97\n",
      "           3       0.65      0.61      0.63        90\n",
      "           4       0.76      0.58      0.66        93\n",
      "           5       0.56      0.67      0.61       108\n",
      "           6       0.82      0.79      0.80       126\n",
      "           7       0.73      0.72      0.72       136\n",
      "           8       0.64      0.62      0.63       122\n",
      "           9       0.82      0.68      0.75       155\n",
      "          10       0.65      0.67      0.66        61\n",
      "          11       0.55      0.63      0.59       172\n",
      "          12       0.71      0.74      0.72       182\n",
      "          13       0.66      0.60      0.63       151\n",
      "          14       0.58      0.63      0.60       200\n",
      "          15       0.83      0.88      0.86       169\n",
      "          16       0.81      0.95      0.87        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.69      0.69      2183\n",
      "\n",
      "At iteration  76  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.4446 - accuracy: 0.9828 - precision: 0.8752 - recall: 0.8256 - val_loss: 1.9136 - val_accuracy: 0.9590 - val_precision: 0.6632 - val_recall: 0.6142\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.55        68\n",
      "           1       0.81      0.73      0.77       176\n",
      "           2       0.67      0.67      0.67        97\n",
      "           3       0.67      0.64      0.66        90\n",
      "           4       0.73      0.56      0.63        93\n",
      "           5       0.57      0.68      0.62       108\n",
      "           6       0.78      0.80      0.79       126\n",
      "           7       0.78      0.73      0.75       136\n",
      "           8       0.70      0.63      0.66       122\n",
      "           9       0.80      0.68      0.74       155\n",
      "          10       0.70      0.74      0.72        61\n",
      "          11       0.56      0.61      0.59       172\n",
      "          12       0.70      0.73      0.72       182\n",
      "          13       0.67      0.64      0.65       151\n",
      "          14       0.56      0.63      0.59       200\n",
      "          15       0.81      0.89      0.85       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  77  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3851 - accuracy: 0.9840 - precision: 0.8839 - recall: 0.8374 - val_loss: 1.8901 - val_accuracy: 0.9596 - val_precision: 0.6693 - val_recall: 0.6199\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56        68\n",
      "           1       0.81      0.74      0.77       176\n",
      "           2       0.68      0.67      0.67        97\n",
      "           3       0.68      0.64      0.66        90\n",
      "           4       0.75      0.56      0.64        93\n",
      "           5       0.60      0.69      0.64       108\n",
      "           6       0.81      0.80      0.80       126\n",
      "           7       0.75      0.75      0.75       136\n",
      "           8       0.67      0.62      0.65       122\n",
      "           9       0.82      0.68      0.74       155\n",
      "          10       0.70      0.70      0.70        61\n",
      "          11       0.57      0.63      0.60       172\n",
      "          12       0.70      0.74      0.72       182\n",
      "          13       0.65      0.63      0.64       151\n",
      "          14       0.59      0.65      0.61       200\n",
      "          15       0.84      0.88      0.86       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  78  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3653 - accuracy: 0.9852 - precision: 0.8956 - recall: 0.8475 - val_loss: 1.9701 - val_accuracy: 0.9612 - val_precision: 0.6802 - val_recall: 0.6440\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.57      0.57        68\n",
      "           1       0.82      0.74      0.78       176\n",
      "           2       0.72      0.64      0.68        97\n",
      "           3       0.64      0.62      0.63        90\n",
      "           4       0.76      0.55      0.64        93\n",
      "           5       0.59      0.69      0.64       108\n",
      "           6       0.79      0.79      0.79       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.68      0.62      0.65       122\n",
      "           9       0.81      0.70      0.75       155\n",
      "          10       0.71      0.74      0.73        61\n",
      "          11       0.56      0.63      0.60       172\n",
      "          12       0.70      0.73      0.72       182\n",
      "          13       0.62      0.63      0.62       151\n",
      "          14       0.59      0.62      0.60       200\n",
      "          15       0.84      0.89      0.86       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  79  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3769 - accuracy: 0.9846 - precision: 0.8892 - recall: 0.8437 - val_loss: 1.9370 - val_accuracy: 0.9591 - val_precision: 0.6640 - val_recall: 0.6165\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.57      0.57        68\n",
      "           1       0.81      0.74      0.78       176\n",
      "           2       0.72      0.67      0.70        97\n",
      "           3       0.68      0.63      0.66        90\n",
      "           4       0.76      0.55      0.64        93\n",
      "           5       0.58      0.68      0.63       108\n",
      "           6       0.80      0.77      0.78       126\n",
      "           7       0.76      0.71      0.74       136\n",
      "           8       0.65      0.66      0.66       122\n",
      "           9       0.80      0.70      0.75       155\n",
      "          10       0.72      0.70      0.71        61\n",
      "          11       0.57      0.65      0.61       172\n",
      "          12       0.72      0.74      0.73       182\n",
      "          13       0.64      0.62      0.63       151\n",
      "          14       0.59      0.65      0.62       200\n",
      "          15       0.83      0.89      0.86       169\n",
      "          16       0.84      0.95      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  80  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3741 - accuracy: 0.9851 - precision: 0.8917 - recall: 0.8490 - val_loss: 1.9841 - val_accuracy: 0.9576 - val_precision: 0.6520 - val_recall: 0.6005\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.81      0.73      0.77       176\n",
      "           2       0.72      0.64      0.68        97\n",
      "           3       0.66      0.62      0.64        90\n",
      "           4       0.74      0.55      0.63        93\n",
      "           5       0.62      0.68      0.65       108\n",
      "           6       0.78      0.79      0.78       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.68      0.63      0.65       122\n",
      "           9       0.79      0.69      0.74       155\n",
      "          10       0.67      0.67      0.67        61\n",
      "          11       0.55      0.65      0.60       172\n",
      "          12       0.68      0.73      0.70       182\n",
      "          13       0.64      0.62      0.63       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.82      0.88      0.85       169\n",
      "          16       0.84      0.95      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  81  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3633 - accuracy: 0.9853 - precision: 0.8937 - recall: 0.8505 - val_loss: 2.0780 - val_accuracy: 0.9586 - val_precision: 0.6585 - val_recall: 0.6148\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.54      0.57        68\n",
      "           1       0.81      0.74      0.77       176\n",
      "           2       0.71      0.69      0.70        97\n",
      "           3       0.65      0.62      0.64        90\n",
      "           4       0.76      0.57      0.65        93\n",
      "           5       0.61      0.68      0.64       108\n",
      "           6       0.80      0.79      0.80       126\n",
      "           7       0.74      0.75      0.75       136\n",
      "           8       0.66      0.64      0.65       122\n",
      "           9       0.77      0.71      0.74       155\n",
      "          10       0.70      0.70      0.70        61\n",
      "          11       0.59      0.65      0.62       172\n",
      "          12       0.72      0.74      0.73       182\n",
      "          13       0.64      0.62      0.63       151\n",
      "          14       0.58      0.64      0.61       200\n",
      "          15       0.83      0.88      0.85       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  82  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.9827 - precision: 0.8727 - recall: 0.8258 - val_loss: 1.8309 - val_accuracy: 0.9622 - val_precision: 0.6922 - val_recall: 0.6422\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.57      0.60        68\n",
      "           1       0.82      0.74      0.78       176\n",
      "           2       0.70      0.66      0.68        97\n",
      "           3       0.65      0.60      0.62        90\n",
      "           4       0.72      0.56      0.63        93\n",
      "           5       0.60      0.68      0.63       108\n",
      "           6       0.81      0.79      0.80       126\n",
      "           7       0.73      0.74      0.74       136\n",
      "           8       0.64      0.62      0.63       122\n",
      "           9       0.79      0.70      0.74       155\n",
      "          10       0.69      0.70      0.70        61\n",
      "          11       0.57      0.64      0.60       172\n",
      "          12       0.72      0.73      0.73       182\n",
      "          13       0.64      0.64      0.64       151\n",
      "          14       0.59      0.66      0.62       200\n",
      "          15       0.82      0.88      0.85       169\n",
      "          16       0.82      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  83  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3989 - accuracy: 0.9838 - precision: 0.8808 - recall: 0.8382 - val_loss: 1.9663 - val_accuracy: 0.9606 - val_precision: 0.6760 - val_recall: 0.6331\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.60      0.62        68\n",
      "           1       0.80      0.77      0.79       176\n",
      "           2       0.73      0.68      0.71        97\n",
      "           3       0.67      0.62      0.64        90\n",
      "           4       0.74      0.58      0.65        93\n",
      "           5       0.62      0.67      0.64       108\n",
      "           6       0.82      0.79      0.80       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.66      0.63      0.64       122\n",
      "           9       0.79      0.70      0.74       155\n",
      "          10       0.67      0.67      0.67        61\n",
      "          11       0.58      0.66      0.62       172\n",
      "          12       0.71      0.74      0.72       182\n",
      "          13       0.63      0.63      0.63       151\n",
      "          14       0.61      0.63      0.62       200\n",
      "          15       0.81      0.88      0.85       169\n",
      "          16       0.84      0.95      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  84  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3523 - accuracy: 0.9857 - precision: 0.8966 - recall: 0.8564 - val_loss: 2.0164 - val_accuracy: 0.9595 - val_precision: 0.6627 - val_recall: 0.6342\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.57      0.60        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.70      0.67      0.68        97\n",
      "           3       0.66      0.62      0.64        90\n",
      "           4       0.74      0.55      0.63        93\n",
      "           5       0.61      0.69      0.65       108\n",
      "           6       0.80      0.79      0.80       126\n",
      "           7       0.75      0.73      0.74       136\n",
      "           8       0.64      0.63      0.63       122\n",
      "           9       0.79      0.70      0.74       155\n",
      "          10       0.72      0.69      0.71        61\n",
      "          11       0.59      0.64      0.62       172\n",
      "          12       0.70      0.75      0.73       182\n",
      "          13       0.64      0.63      0.63       151\n",
      "          14       0.60      0.65      0.62       200\n",
      "          15       0.82      0.89      0.85       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  85  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3412 - accuracy: 0.9862 - precision: 0.8978 - recall: 0.8630 - val_loss: 2.0195 - val_accuracy: 0.9583 - val_precision: 0.6560 - val_recall: 0.6125\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57        68\n",
      "           1       0.81      0.75      0.78       176\n",
      "           2       0.68      0.67      0.67        97\n",
      "           3       0.66      0.62      0.64        90\n",
      "           4       0.74      0.56      0.64        93\n",
      "           5       0.62      0.69      0.65       108\n",
      "           6       0.80      0.79      0.80       126\n",
      "           7       0.75      0.72      0.74       136\n",
      "           8       0.65      0.63      0.64       122\n",
      "           9       0.77      0.71      0.74       155\n",
      "          10       0.68      0.70      0.69        61\n",
      "          11       0.56      0.65      0.60       172\n",
      "          12       0.72      0.71      0.72       182\n",
      "          13       0.66      0.64      0.65       151\n",
      "          14       0.59      0.62      0.61       200\n",
      "          15       0.83      0.88      0.85       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  86  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4274 - accuracy: 0.9830 - precision: 0.8766 - recall: 0.8281 - val_loss: 1.8953 - val_accuracy: 0.9606 - val_precision: 0.6780 - val_recall: 0.6279\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.71      0.69      0.70        97\n",
      "           3       0.64      0.60      0.62        90\n",
      "           4       0.74      0.55      0.63        93\n",
      "           5       0.61      0.67      0.63       108\n",
      "           6       0.82      0.81      0.81       126\n",
      "           7       0.77      0.70      0.73       136\n",
      "           8       0.67      0.63      0.65       122\n",
      "           9       0.77      0.70      0.73       155\n",
      "          10       0.73      0.67      0.70        61\n",
      "          11       0.57      0.65      0.61       172\n",
      "          12       0.72      0.74      0.73       182\n",
      "          13       0.64      0.66      0.65       151\n",
      "          14       0.58      0.64      0.61       200\n",
      "          15       0.83      0.87      0.85       169\n",
      "          16       0.81      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  87  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.9851 - precision: 0.8902 - recall: 0.8517 - val_loss: 1.7929 - val_accuracy: 0.9603 - val_precision: 0.6768 - val_recall: 0.6234\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.81      0.76      0.78       176\n",
      "           2       0.70      0.69      0.69        97\n",
      "           3       0.65      0.61      0.63        90\n",
      "           4       0.76      0.54      0.63        93\n",
      "           5       0.61      0.68      0.64       108\n",
      "           6       0.80      0.79      0.80       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.66      0.66      0.66       122\n",
      "           9       0.80      0.69      0.74       155\n",
      "          10       0.71      0.66      0.68        61\n",
      "          11       0.58      0.64      0.61       172\n",
      "          12       0.72      0.75      0.73       182\n",
      "          13       0.65      0.64      0.64       151\n",
      "          14       0.58      0.64      0.61       200\n",
      "          15       0.84      0.88      0.86       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  88  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3723 - accuracy: 0.9849 - precision: 0.8890 - recall: 0.8494 - val_loss: 1.9449 - val_accuracy: 0.9597 - val_precision: 0.6689 - val_recall: 0.6245\n",
      "69/69 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.57      0.60        68\n",
      "           1       0.82      0.76      0.79       176\n",
      "           2       0.68      0.68      0.68        97\n",
      "           3       0.64      0.61      0.63        90\n",
      "           4       0.73      0.52      0.60        93\n",
      "           5       0.59      0.69      0.63       108\n",
      "           6       0.83      0.79      0.81       126\n",
      "           7       0.77      0.72      0.74       136\n",
      "           8       0.65      0.65      0.65       122\n",
      "           9       0.76      0.66      0.71       155\n",
      "          10       0.70      0.72      0.71        61\n",
      "          11       0.58      0.63      0.60       172\n",
      "          12       0.74      0.73      0.73       182\n",
      "          13       0.67      0.64      0.65       151\n",
      "          14       0.56      0.66      0.60       200\n",
      "          15       0.81      0.86      0.83       169\n",
      "          16       0.82      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  89  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.9863 - precision: 0.8996 - recall: 0.8643 - val_loss: 2.0540 - val_accuracy: 0.9584 - val_precision: 0.6567 - val_recall: 0.6153\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.56      0.58        68\n",
      "           1       0.82      0.76      0.79       176\n",
      "           2       0.69      0.68      0.68        97\n",
      "           3       0.63      0.61      0.62        90\n",
      "           4       0.74      0.54      0.62        93\n",
      "           5       0.58      0.69      0.63       108\n",
      "           6       0.82      0.79      0.80       126\n",
      "           7       0.74      0.71      0.73       136\n",
      "           8       0.64      0.65      0.64       122\n",
      "           9       0.78      0.66      0.72       155\n",
      "          10       0.66      0.67      0.67        61\n",
      "          11       0.58      0.64      0.61       172\n",
      "          12       0.71      0.70      0.71       182\n",
      "          13       0.65      0.63      0.64       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.80      0.88      0.84       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  90  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3990 - accuracy: 0.9837 - precision: 0.8803 - recall: 0.8374 - val_loss: 1.9990 - val_accuracy: 0.9601 - val_precision: 0.6716 - val_recall: 0.6285\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58        68\n",
      "           1       0.81      0.76      0.78       176\n",
      "           2       0.69      0.70      0.70        97\n",
      "           3       0.67      0.63      0.65        90\n",
      "           4       0.78      0.57      0.66        93\n",
      "           5       0.58      0.69      0.63       108\n",
      "           6       0.82      0.81      0.82       126\n",
      "           7       0.74      0.71      0.73       136\n",
      "           8       0.68      0.65      0.66       122\n",
      "           9       0.77      0.68      0.72       155\n",
      "          10       0.68      0.70      0.69        61\n",
      "          11       0.58      0.60      0.59       172\n",
      "          12       0.72      0.72      0.72       182\n",
      "          13       0.62      0.64      0.63       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.82      0.87      0.84       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  91  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3226 - accuracy: 0.9870 - precision: 0.9049 - recall: 0.8700 - val_loss: 2.0238 - val_accuracy: 0.9619 - val_precision: 0.6878 - val_recall: 0.6445\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.57      0.58        68\n",
      "           1       0.81      0.76      0.78       176\n",
      "           2       0.71      0.68      0.69        97\n",
      "           3       0.62      0.59      0.60        90\n",
      "           4       0.75      0.56      0.64        93\n",
      "           5       0.58      0.68      0.63       108\n",
      "           6       0.81      0.79      0.80       126\n",
      "           7       0.74      0.72      0.73       136\n",
      "           8       0.67      0.66      0.67       122\n",
      "           9       0.78      0.68      0.72       155\n",
      "          10       0.71      0.67      0.69        61\n",
      "          11       0.58      0.63      0.60       172\n",
      "          12       0.72      0.72      0.72       182\n",
      "          13       0.65      0.64      0.64       151\n",
      "          14       0.59      0.65      0.62       200\n",
      "          15       0.82      0.88      0.85       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  92  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3411 - accuracy: 0.9863 - precision: 0.8988 - recall: 0.8644 - val_loss: 2.0229 - val_accuracy: 0.9598 - val_precision: 0.6673 - val_recall: 0.6325\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.80      0.76      0.78       176\n",
      "           2       0.71      0.70      0.70        97\n",
      "           3       0.68      0.60      0.64        90\n",
      "           4       0.76      0.56      0.65        93\n",
      "           5       0.60      0.69      0.64       108\n",
      "           6       0.81      0.79      0.80       126\n",
      "           7       0.74      0.73      0.74       136\n",
      "           8       0.69      0.63      0.66       122\n",
      "           9       0.78      0.68      0.73       155\n",
      "          10       0.68      0.69      0.68        61\n",
      "          11       0.58      0.66      0.62       172\n",
      "          12       0.71      0.73      0.72       182\n",
      "          13       0.64      0.64      0.64       151\n",
      "          14       0.61      0.66      0.63       200\n",
      "          15       0.83      0.89      0.86       169\n",
      "          16       0.82      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  93  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3499 - accuracy: 0.9859 - precision: 0.8981 - recall: 0.8576 - val_loss: 2.0190 - val_accuracy: 0.9582 - val_precision: 0.6541 - val_recall: 0.6148\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.56      0.55        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.71      0.69      0.70        97\n",
      "           3       0.69      0.61      0.65        90\n",
      "           4       0.74      0.53      0.62        93\n",
      "           5       0.60      0.69      0.64       108\n",
      "           6       0.81      0.79      0.80       126\n",
      "           7       0.72      0.71      0.71       136\n",
      "           8       0.70      0.63      0.66       122\n",
      "           9       0.78      0.67      0.72       155\n",
      "          10       0.67      0.67      0.67        61\n",
      "          11       0.56      0.62      0.59       172\n",
      "          12       0.72      0.72      0.72       182\n",
      "          13       0.65      0.64      0.64       151\n",
      "          14       0.60      0.66      0.62       200\n",
      "          15       0.80      0.89      0.85       169\n",
      "          16       0.82      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  94  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3500 - accuracy: 0.9854 - precision: 0.8928 - recall: 0.8553 - val_loss: 2.0577 - val_accuracy: 0.9593 - val_precision: 0.6673 - val_recall: 0.6153\n",
      "69/69 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.80      0.74      0.77       176\n",
      "           2       0.74      0.67      0.70        97\n",
      "           3       0.69      0.59      0.63        90\n",
      "           4       0.74      0.56      0.64        93\n",
      "           5       0.58      0.70      0.64       108\n",
      "           6       0.80      0.78      0.79       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.68      0.62      0.65       122\n",
      "           9       0.78      0.67      0.72       155\n",
      "          10       0.72      0.70      0.71        61\n",
      "          11       0.55      0.63      0.59       172\n",
      "          12       0.72      0.74      0.73       182\n",
      "          13       0.62      0.62      0.62       151\n",
      "          14       0.60      0.64      0.62       200\n",
      "          15       0.83      0.91      0.86       169\n",
      "          16       0.82      0.94      0.87        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  95  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3133 - accuracy: 0.9874 - precision: 0.9082 - recall: 0.8737 - val_loss: 2.2225 - val_accuracy: 0.9601 - val_precision: 0.6709 - val_recall: 0.6302\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.57      0.59        68\n",
      "           1       0.79      0.75      0.77       176\n",
      "           2       0.69      0.70      0.70        97\n",
      "           3       0.66      0.59      0.62        90\n",
      "           4       0.75      0.55      0.63        93\n",
      "           5       0.58      0.68      0.63       108\n",
      "           6       0.80      0.77      0.79       126\n",
      "           7       0.72      0.74      0.73       136\n",
      "           8       0.66      0.64      0.65       122\n",
      "           9       0.80      0.67      0.73       155\n",
      "          10       0.71      0.69      0.70        61\n",
      "          11       0.58      0.63      0.60       172\n",
      "          12       0.71      0.74      0.73       182\n",
      "          13       0.62      0.62      0.62       151\n",
      "          14       0.59      0.62      0.61       200\n",
      "          15       0.81      0.90      0.85       169\n",
      "          16       0.82      0.94      0.87        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  96  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.9859 - precision: 0.8948 - recall: 0.8607 - val_loss: 1.9732 - val_accuracy: 0.9610 - val_precision: 0.6786 - val_recall: 0.6394\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58        68\n",
      "           1       0.79      0.74      0.77       176\n",
      "           2       0.69      0.68      0.69        97\n",
      "           3       0.65      0.60      0.62        90\n",
      "           4       0.76      0.55      0.64        93\n",
      "           5       0.60      0.68      0.64       108\n",
      "           6       0.79      0.76      0.77       126\n",
      "           7       0.75      0.74      0.74       136\n",
      "           8       0.66      0.63      0.65       122\n",
      "           9       0.77      0.66      0.71       155\n",
      "          10       0.68      0.66      0.67        61\n",
      "          11       0.58      0.65      0.61       172\n",
      "          12       0.72      0.73      0.72       182\n",
      "          13       0.64      0.62      0.63       151\n",
      "          14       0.58      0.65      0.61       200\n",
      "          15       0.80      0.88      0.84       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  97  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4227 - accuracy: 0.9832 - precision: 0.8764 - recall: 0.8315 - val_loss: 2.0637 - val_accuracy: 0.9599 - val_precision: 0.6695 - val_recall: 0.6285\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.53      0.57        68\n",
      "           1       0.79      0.75      0.77       176\n",
      "           2       0.70      0.67      0.68        97\n",
      "           3       0.70      0.60      0.65        90\n",
      "           4       0.75      0.54      0.62        93\n",
      "           5       0.60      0.69      0.64       108\n",
      "           6       0.81      0.77      0.79       126\n",
      "           7       0.72      0.74      0.73       136\n",
      "           8       0.64      0.63      0.64       122\n",
      "           9       0.77      0.68      0.72       155\n",
      "          10       0.70      0.70      0.70        61\n",
      "          11       0.58      0.67      0.62       172\n",
      "          12       0.71      0.73      0.72       182\n",
      "          13       0.63      0.58      0.60       151\n",
      "          14       0.56      0.61      0.59       200\n",
      "          15       0.81      0.91      0.86       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.69      2183\n",
      "\n",
      "At iteration  98  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3158 - accuracy: 0.9876 - precision: 0.9097 - recall: 0.8757 - val_loss: 2.1454 - val_accuracy: 0.9588 - val_precision: 0.6592 - val_recall: 0.6199\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.53      0.56        68\n",
      "           1       0.80      0.75      0.77       176\n",
      "           2       0.70      0.66      0.68        97\n",
      "           3       0.67      0.60      0.63        90\n",
      "           4       0.73      0.55      0.63        93\n",
      "           5       0.60      0.70      0.65       108\n",
      "           6       0.80      0.77      0.78       126\n",
      "           7       0.72      0.73      0.73       136\n",
      "           8       0.67      0.62      0.65       122\n",
      "           9       0.78      0.68      0.73       155\n",
      "          10       0.74      0.69      0.71        61\n",
      "          11       0.58      0.65      0.62       172\n",
      "          12       0.70      0.74      0.72       182\n",
      "          13       0.62      0.58      0.60       151\n",
      "          14       0.59      0.67      0.63       200\n",
      "          15       0.81      0.91      0.86       169\n",
      "          16       0.83      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  99  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.9874 - precision: 0.9082 - recall: 0.8739 - val_loss: 2.0586 - val_accuracy: 0.9593 - val_precision: 0.6640 - val_recall: 0.6222\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.51      0.56        68\n",
      "           1       0.81      0.76      0.78       176\n",
      "           2       0.70      0.68      0.69        97\n",
      "           3       0.65      0.59      0.62        90\n",
      "           4       0.75      0.55      0.63        93\n",
      "           5       0.60      0.69      0.64       108\n",
      "           6       0.80      0.76      0.78       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.63      0.64      0.64       122\n",
      "           9       0.77      0.70      0.73       155\n",
      "          10       0.72      0.67      0.69        61\n",
      "          11       0.59      0.63      0.61       172\n",
      "          12       0.70      0.73      0.71       182\n",
      "          13       0.63      0.60      0.62       151\n",
      "          14       0.60      0.67      0.63       200\n",
      "          15       0.82      0.89      0.86       169\n",
      "          16       0.82      0.95      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  100  we get these data :\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3886 - accuracy: 0.9845 - precision: 0.8829 - recall: 0.8482 - val_loss: 2.1737 - val_accuracy: 0.9614 - val_precision: 0.6809 - val_recall: 0.6474\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.54      0.58        68\n",
      "           1       0.82      0.76      0.79       176\n",
      "           2       0.68      0.67      0.68        97\n",
      "           3       0.68      0.60      0.64        90\n",
      "           4       0.78      0.56      0.65        93\n",
      "           5       0.61      0.68      0.64       108\n",
      "           6       0.77      0.76      0.76       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.65      0.62      0.64       122\n",
      "           9       0.78      0.69      0.73       155\n",
      "          10       0.67      0.66      0.66        61\n",
      "          11       0.56      0.63      0.59       172\n",
      "          12       0.72      0.74      0.73       182\n",
      "          13       0.63      0.61      0.62       151\n",
      "          14       0.59      0.67      0.63       200\n",
      "          15       0.82      0.89      0.85       169\n",
      "          16       0.85      0.96      0.90        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  101  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3051 - accuracy: 0.9875 - precision: 0.9066 - recall: 0.8782 - val_loss: 2.1695 - val_accuracy: 0.9599 - val_precision: 0.6710 - val_recall: 0.6245\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.57      0.59        68\n",
      "           1       0.80      0.76      0.78       176\n",
      "           2       0.70      0.68      0.69        97\n",
      "           3       0.66      0.60      0.63        90\n",
      "           4       0.73      0.56      0.63        93\n",
      "           5       0.63      0.68      0.65       108\n",
      "           6       0.78      0.77      0.77       126\n",
      "           7       0.72      0.74      0.73       136\n",
      "           8       0.67      0.59      0.63       122\n",
      "           9       0.78      0.69      0.73       155\n",
      "          10       0.71      0.72      0.72        61\n",
      "          11       0.56      0.62      0.59       172\n",
      "          12       0.71      0.74      0.72       182\n",
      "          13       0.63      0.61      0.62       151\n",
      "          14       0.58      0.64      0.61       200\n",
      "          15       0.82      0.90      0.86       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  102  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3138 - accuracy: 0.9875 - precision: 0.9069 - recall: 0.8785 - val_loss: 2.2441 - val_accuracy: 0.9581 - val_precision: 0.6540 - val_recall: 0.6113\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.57      0.59        68\n",
      "           1       0.80      0.76      0.78       176\n",
      "           2       0.69      0.68      0.69        97\n",
      "           3       0.65      0.60      0.62        90\n",
      "           4       0.74      0.57      0.64        93\n",
      "           5       0.59      0.69      0.63       108\n",
      "           6       0.77      0.75      0.76       126\n",
      "           7       0.77      0.75      0.76       136\n",
      "           8       0.64      0.61      0.62       122\n",
      "           9       0.80      0.68      0.74       155\n",
      "          10       0.71      0.75      0.73        61\n",
      "          11       0.56      0.62      0.59       172\n",
      "          12       0.71      0.72      0.72       182\n",
      "          13       0.63      0.59      0.61       151\n",
      "          14       0.57      0.64      0.61       200\n",
      "          15       0.84      0.88      0.86       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  103  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.9870 - precision: 0.9033 - recall: 0.8716 - val_loss: 2.2526 - val_accuracy: 0.9592 - val_precision: 0.6609 - val_recall: 0.6302\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58        68\n",
      "           1       0.80      0.75      0.78       176\n",
      "           2       0.68      0.67      0.68        97\n",
      "           3       0.68      0.61      0.64        90\n",
      "           4       0.75      0.57      0.65        93\n",
      "           5       0.57      0.68      0.62       108\n",
      "           6       0.79      0.75      0.77       126\n",
      "           7       0.74      0.74      0.74       136\n",
      "           8       0.64      0.64      0.64       122\n",
      "           9       0.79      0.68      0.73       155\n",
      "          10       0.68      0.67      0.68        61\n",
      "          11       0.57      0.60      0.59       172\n",
      "          12       0.70      0.72      0.71       182\n",
      "          13       0.63      0.60      0.61       151\n",
      "          14       0.59      0.65      0.61       200\n",
      "          15       0.85      0.91      0.87       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  104  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3578 - accuracy: 0.9855 - precision: 0.8919 - recall: 0.8576 - val_loss: 2.0016 - val_accuracy: 0.9596 - val_precision: 0.6681 - val_recall: 0.6211\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.54      0.57        68\n",
      "           1       0.80      0.76      0.78       176\n",
      "           2       0.67      0.68      0.68        97\n",
      "           3       0.66      0.60      0.63        90\n",
      "           4       0.74      0.55      0.63        93\n",
      "           5       0.60      0.68      0.64       108\n",
      "           6       0.78      0.75      0.76       126\n",
      "           7       0.74      0.75      0.75       136\n",
      "           8       0.66      0.63      0.64       122\n",
      "           9       0.76      0.71      0.74       155\n",
      "          10       0.67      0.67      0.67        61\n",
      "          11       0.58      0.65      0.61       172\n",
      "          12       0.69      0.72      0.71       182\n",
      "          13       0.64      0.60      0.62       151\n",
      "          14       0.59      0.62      0.61       200\n",
      "          15       0.85      0.90      0.87       169\n",
      "          16       0.84      0.96      0.90        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  105  we get these data :\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3338 - accuracy: 0.9865 - precision: 0.8997 - recall: 0.8669 - val_loss: 2.1473 - val_accuracy: 0.9611 - val_precision: 0.6807 - val_recall: 0.6394\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.56      0.58        68\n",
      "           1       0.80      0.77      0.79       176\n",
      "           2       0.68      0.70      0.69        97\n",
      "           3       0.68      0.61      0.64        90\n",
      "           4       0.73      0.52      0.60        93\n",
      "           5       0.59      0.69      0.64       108\n",
      "           6       0.82      0.75      0.78       126\n",
      "           7       0.75      0.74      0.74       136\n",
      "           8       0.66      0.64      0.65       122\n",
      "           9       0.80      0.68      0.74       155\n",
      "          10       0.68      0.67      0.68        61\n",
      "          11       0.58      0.64      0.61       172\n",
      "          12       0.70      0.72      0.71       182\n",
      "          13       0.61      0.60      0.60       151\n",
      "          14       0.59      0.65      0.62       200\n",
      "          15       0.85      0.91      0.88       169\n",
      "          16       0.81      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    print( \"At iteration \"  ,i+6 , \" we get these data :\" )\n",
    "    \n",
    "    model6.fit(X_train , y_train1 , batch_size = 64 , epochs = 1 , validation_split=(0.2) ,verbose =1)\n",
    "\n",
    "    predictions = np.argmax(model6.predict(X_test), axis=-1)\n",
    "\n",
    "    print(classification_report(y_test.values , predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a10566",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dff4882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:03:29.978318Z",
     "start_time": "2024-02-10T14:03:29.883312Z"
    }
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(100,input_dim=200,activation = \"leaky_relu\"))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(Dense(80,activation = \"leaky_relu\"))\n",
    "model2.add(Dense(40,activation = \"leaky_relu\"))\n",
    "model2.add(Dense(25,activation = \"leaky_relu\"))\n",
    "model2.add(Dense(17,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72082ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:03:41.810625Z",
     "start_time": "2024-02-10T14:03:41.804725Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.AdamW(learning_rate =0.007 , beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    use_ema=True,\n",
    "    ema_momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef62c895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:03:51.587915Z",
     "start_time": "2024-02-10T14:03:51.565393Z"
    }
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer = opt , \n",
    "              loss = 'categorical_crossentropy' ,\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "704d4616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:05:01.948184Z",
     "start_time": "2024-02-10T14:04:15.655677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration  0  we get these data :\n",
      "219/219 [==============================] - 2s 4ms/step - loss: 1.7634 - accuracy: 0.9491 - precision: 0.6970 - recall: 0.2378 - val_loss: 1.3285 - val_accuracy: 0.9583 - val_precision: 0.7867 - val_recall: 0.3990\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.10      0.18        68\n",
      "           1       0.66      0.72      0.69       176\n",
      "           2       0.79      0.56      0.65        97\n",
      "           3       0.80      0.50      0.62        90\n",
      "           4       0.70      0.42      0.52        93\n",
      "           5       0.66      0.66      0.66       108\n",
      "           6       0.71      0.81      0.76       126\n",
      "           7       0.63      0.74      0.68       136\n",
      "           8       0.41      0.32      0.36       122\n",
      "           9       0.70      0.60      0.65       155\n",
      "          10       0.78      0.23      0.35        61\n",
      "          11       0.45      0.58      0.51       172\n",
      "          12       0.61      0.76      0.67       182\n",
      "          13       0.54      0.60      0.56       151\n",
      "          14       0.40      0.41      0.41       200\n",
      "          15       0.72      0.94      0.81       169\n",
      "          16       0.77      0.99      0.86        77\n",
      "\n",
      "    accuracy                           0.61      2183\n",
      "   macro avg       0.65      0.58      0.59      2183\n",
      "weighted avg       0.62      0.61      0.60      2183\n",
      "\n",
      "At iteration  1  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.3660 - accuracy: 0.9555 - precision: 0.7226 - recall: 0.3946 - val_loss: 1.2695 - val_accuracy: 0.9594 - val_precision: 0.7498 - val_recall: 0.4648\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.24      0.32        68\n",
      "           1       0.70      0.66      0.68       176\n",
      "           2       0.68      0.75      0.71        97\n",
      "           3       0.73      0.60      0.66        90\n",
      "           4       0.75      0.45      0.56        93\n",
      "           5       0.65      0.67      0.66       108\n",
      "           6       0.67      0.83      0.74       126\n",
      "           7       0.66      0.73      0.69       136\n",
      "           8       0.47      0.37      0.41       122\n",
      "           9       0.77      0.57      0.66       155\n",
      "          10       0.53      0.52      0.53        61\n",
      "          11       0.49      0.62      0.54       172\n",
      "          12       0.71      0.79      0.74       182\n",
      "          13       0.57      0.60      0.58       151\n",
      "          14       0.47      0.46      0.46       200\n",
      "          15       0.75      0.87      0.80       169\n",
      "          16       0.82      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.64      2183\n",
      "   macro avg       0.64      0.63      0.63      2183\n",
      "weighted avg       0.64      0.64      0.63      2183\n",
      "\n",
      "At iteration  2  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.2755 - accuracy: 0.9568 - precision: 0.7209 - recall: 0.4334 - val_loss: 1.2209 - val_accuracy: 0.9605 - val_precision: 0.7410 - val_recall: 0.5060\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.21      0.30        68\n",
      "           1       0.71      0.68      0.69       176\n",
      "           2       0.65      0.77      0.71        97\n",
      "           3       0.75      0.61      0.67        90\n",
      "           4       0.75      0.48      0.59        93\n",
      "           5       0.69      0.73      0.71       108\n",
      "           6       0.71      0.83      0.76       126\n",
      "           7       0.66      0.80      0.72       136\n",
      "           8       0.52      0.44      0.48       122\n",
      "           9       0.83      0.57      0.67       155\n",
      "          10       0.57      0.52      0.55        61\n",
      "          11       0.49      0.62      0.55       172\n",
      "          12       0.72      0.79      0.76       182\n",
      "          13       0.65      0.54      0.59       151\n",
      "          14       0.45      0.51      0.48       200\n",
      "          15       0.75      0.86      0.80       169\n",
      "          16       0.81      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.65      2183\n",
      "   macro avg       0.66      0.64      0.64      2183\n",
      "weighted avg       0.66      0.65      0.65      2183\n",
      "\n",
      "At iteration  3  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.2322 - accuracy: 0.9586 - precision: 0.7349 - recall: 0.4639 - val_loss: 1.2599 - val_accuracy: 0.9587 - val_precision: 0.7342 - val_recall: 0.4665\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.26      0.36        68\n",
      "           1       0.77      0.66      0.71       176\n",
      "           2       0.66      0.75      0.70        97\n",
      "           3       0.78      0.57      0.66        90\n",
      "           4       0.75      0.51      0.60        93\n",
      "           5       0.70      0.77      0.73       108\n",
      "           6       0.74      0.83      0.79       126\n",
      "           7       0.72      0.74      0.73       136\n",
      "           8       0.54      0.43      0.48       122\n",
      "           9       0.80      0.61      0.69       155\n",
      "          10       0.56      0.51      0.53        61\n",
      "          11       0.51      0.60      0.55       172\n",
      "          12       0.71      0.82      0.76       182\n",
      "          13       0.65      0.59      0.62       151\n",
      "          14       0.47      0.58      0.52       200\n",
      "          15       0.74      0.88      0.80       169\n",
      "          16       0.80      0.99      0.88        77\n",
      "\n",
      "    accuracy                           0.67      2183\n",
      "   macro avg       0.68      0.65      0.65      2183\n",
      "weighted avg       0.67      0.67      0.66      2183\n",
      "\n",
      "At iteration  4  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.1804 - accuracy: 0.9603 - precision: 0.7475 - recall: 0.4908 - val_loss: 1.2296 - val_accuracy: 0.9610 - val_precision: 0.7734 - val_recall: 0.4768\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.40      0.50        68\n",
      "           1       0.73      0.72      0.72       176\n",
      "           2       0.65      0.74      0.69        97\n",
      "           3       0.73      0.61      0.67        90\n",
      "           4       0.72      0.53      0.61        93\n",
      "           5       0.67      0.79      0.72       108\n",
      "           6       0.75      0.83      0.79       126\n",
      "           7       0.68      0.76      0.72       136\n",
      "           8       0.57      0.57      0.57       122\n",
      "           9       0.78      0.63      0.70       155\n",
      "          10       0.55      0.51      0.53        61\n",
      "          11       0.53      0.59      0.56       172\n",
      "          12       0.71      0.83      0.77       182\n",
      "          13       0.64      0.58      0.61       151\n",
      "          14       0.55      0.47      0.51       200\n",
      "          15       0.75      0.87      0.81       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.68      0.67      0.67      2183\n",
      "weighted avg       0.67      0.68      0.67      2183\n",
      "\n",
      "At iteration  5  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.1330 - accuracy: 0.9610 - precision: 0.7458 - recall: 0.5104 - val_loss: 1.2153 - val_accuracy: 0.9620 - val_precision: 0.7522 - val_recall: 0.5266\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.46      0.53        68\n",
      "           1       0.74      0.68      0.71       176\n",
      "           2       0.65      0.74      0.70        97\n",
      "           3       0.79      0.66      0.72        90\n",
      "           4       0.73      0.52      0.60        93\n",
      "           5       0.68      0.78      0.72       108\n",
      "           6       0.77      0.82      0.79       126\n",
      "           7       0.69      0.75      0.72       136\n",
      "           8       0.65      0.51      0.57       122\n",
      "           9       0.82      0.63      0.71       155\n",
      "          10       0.57      0.57      0.57        61\n",
      "          11       0.51      0.62      0.56       172\n",
      "          12       0.75      0.79      0.77       182\n",
      "          13       0.61      0.63      0.62       151\n",
      "          14       0.55      0.55      0.55       200\n",
      "          15       0.77      0.86      0.81       169\n",
      "          16       0.83      0.99      0.90        77\n",
      "\n",
      "    accuracy                           0.68      2183\n",
      "   macro avg       0.69      0.68      0.68      2183\n",
      "weighted avg       0.69      0.68      0.68      2183\n",
      "\n",
      "At iteration  6  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 1s 3ms/step - loss: 1.0936 - accuracy: 0.9624 - precision: 0.7555 - recall: 0.5331 - val_loss: 1.1132 - val_accuracy: 0.9643 - val_precision: 0.7810 - val_recall: 0.5472\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.43      0.48        68\n",
      "           1       0.73      0.73      0.73       176\n",
      "           2       0.69      0.78      0.73        97\n",
      "           3       0.79      0.63      0.70        90\n",
      "           4       0.74      0.53      0.62        93\n",
      "           5       0.68      0.75      0.71       108\n",
      "           6       0.78      0.83      0.80       126\n",
      "           7       0.68      0.79      0.73       136\n",
      "           8       0.69      0.46      0.55       122\n",
      "           9       0.82      0.65      0.73       155\n",
      "          10       0.57      0.66      0.61        61\n",
      "          11       0.53      0.56      0.54       172\n",
      "          12       0.73      0.80      0.76       182\n",
      "          13       0.63      0.61      0.62       151\n",
      "          14       0.52      0.56      0.54       200\n",
      "          15       0.78      0.86      0.82       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.69      2183\n",
      "   macro avg       0.69      0.68      0.68      2183\n",
      "weighted avg       0.69      0.69      0.68      2183\n",
      "\n",
      "At iteration  7  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.0782 - accuracy: 0.9632 - precision: 0.7625 - recall: 0.5450 - val_loss: 1.1745 - val_accuracy: 0.9627 - val_precision: 0.7608 - val_recall: 0.5335\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.44      0.49        68\n",
      "           1       0.74      0.76      0.75       176\n",
      "           2       0.73      0.72      0.73        97\n",
      "           3       0.84      0.63      0.72        90\n",
      "           4       0.72      0.55      0.62        93\n",
      "           5       0.68      0.75      0.71       108\n",
      "           6       0.77      0.82      0.79       126\n",
      "           7       0.69      0.80      0.74       136\n",
      "           8       0.70      0.55      0.61       122\n",
      "           9       0.84      0.63      0.72       155\n",
      "          10       0.63      0.56      0.59        61\n",
      "          11       0.54      0.60      0.57       172\n",
      "          12       0.74      0.81      0.77       182\n",
      "          13       0.66      0.60      0.63       151\n",
      "          14       0.57      0.62      0.59       200\n",
      "          15       0.76      0.89      0.82       169\n",
      "          16       0.81      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  8  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.0539 - accuracy: 0.9636 - precision: 0.7635 - recall: 0.5528 - val_loss: 1.1669 - val_accuracy: 0.9627 - val_precision: 0.7568 - val_recall: 0.5381\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.46      0.52        68\n",
      "           1       0.78      0.69      0.73       176\n",
      "           2       0.70      0.75      0.72        97\n",
      "           3       0.78      0.68      0.73        90\n",
      "           4       0.74      0.52      0.61        93\n",
      "           5       0.69      0.75      0.72       108\n",
      "           6       0.79      0.84      0.81       126\n",
      "           7       0.72      0.76      0.74       136\n",
      "           8       0.70      0.53      0.60       122\n",
      "           9       0.81      0.66      0.73       155\n",
      "          10       0.59      0.66      0.62        61\n",
      "          11       0.51      0.65      0.57       172\n",
      "          12       0.75      0.80      0.77       182\n",
      "          13       0.66      0.63      0.64       151\n",
      "          14       0.55      0.58      0.57       200\n",
      "          15       0.78      0.85      0.81       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.69      2183\n",
      "\n",
      "At iteration  9  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.0322 - accuracy: 0.9643 - precision: 0.7690 - recall: 0.5623 - val_loss: 1.2162 - val_accuracy: 0.9619 - val_precision: 0.7312 - val_recall: 0.5575\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.47      0.55        68\n",
      "           1       0.76      0.74      0.75       176\n",
      "           2       0.73      0.73      0.73        97\n",
      "           3       0.78      0.66      0.71        90\n",
      "           4       0.72      0.51      0.59        93\n",
      "           5       0.68      0.76      0.72       108\n",
      "           6       0.78      0.82      0.80       126\n",
      "           7       0.71      0.76      0.73       136\n",
      "           8       0.69      0.61      0.64       122\n",
      "           9       0.82      0.66      0.73       155\n",
      "          10       0.62      0.61      0.61        61\n",
      "          11       0.54      0.61      0.58       172\n",
      "          12       0.73      0.81      0.77       182\n",
      "          13       0.69      0.60      0.64       151\n",
      "          14       0.55      0.65      0.60       200\n",
      "          15       0.78      0.85      0.82       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.71      0.69      0.70      2183\n",
      "weighted avg       0.71      0.70      0.70      2183\n",
      "\n",
      "At iteration  10  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.0162 - accuracy: 0.9648 - precision: 0.7714 - recall: 0.5699 - val_loss: 1.1896 - val_accuracy: 0.9618 - val_precision: 0.7214 - val_recall: 0.5707\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.49      0.53        68\n",
      "           1       0.75      0.75      0.75       176\n",
      "           2       0.71      0.75      0.73        97\n",
      "           3       0.78      0.68      0.73        90\n",
      "           4       0.69      0.56      0.62        93\n",
      "           5       0.67      0.77      0.72       108\n",
      "           6       0.77      0.84      0.81       126\n",
      "           7       0.70      0.75      0.73       136\n",
      "           8       0.70      0.58      0.63       122\n",
      "           9       0.82      0.66      0.73       155\n",
      "          10       0.60      0.52      0.56        61\n",
      "          11       0.57      0.61      0.59       172\n",
      "          12       0.76      0.79      0.77       182\n",
      "          13       0.70      0.59      0.64       151\n",
      "          14       0.56      0.64      0.60       200\n",
      "          15       0.78      0.86      0.82       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  11  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.9897 - accuracy: 0.9652 - precision: 0.7740 - recall: 0.5772 - val_loss: 1.1271 - val_accuracy: 0.9639 - val_precision: 0.7573 - val_recall: 0.5678\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.50      0.55        68\n",
      "           1       0.75      0.74      0.75       176\n",
      "           2       0.70      0.77      0.74        97\n",
      "           3       0.81      0.62      0.70        90\n",
      "           4       0.72      0.53      0.61        93\n",
      "           5       0.67      0.79      0.73       108\n",
      "           6       0.77      0.85      0.81       126\n",
      "           7       0.70      0.74      0.72       136\n",
      "           8       0.68      0.52      0.59       122\n",
      "           9       0.82      0.68      0.74       155\n",
      "          10       0.58      0.61      0.59        61\n",
      "          11       0.54      0.60      0.57       172\n",
      "          12       0.72      0.80      0.76       182\n",
      "          13       0.68      0.61      0.64       151\n",
      "          14       0.58      0.60      0.59       200\n",
      "          15       0.79      0.85      0.82       169\n",
      "          16       0.80      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.69      0.69      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  12  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 1s 3ms/step - loss: 0.9770 - accuracy: 0.9659 - precision: 0.7771 - recall: 0.5900 - val_loss: 1.1619 - val_accuracy: 0.9625 - val_precision: 0.7411 - val_recall: 0.5570\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53        68\n",
      "           1       0.77      0.76      0.76       176\n",
      "           2       0.73      0.74      0.74        97\n",
      "           3       0.74      0.61      0.67        90\n",
      "           4       0.70      0.55      0.61        93\n",
      "           5       0.69      0.79      0.74       108\n",
      "           6       0.79      0.83      0.81       126\n",
      "           7       0.72      0.71      0.71       136\n",
      "           8       0.70      0.57      0.63       122\n",
      "           9       0.81      0.63      0.71       155\n",
      "          10       0.62      0.62      0.62        61\n",
      "          11       0.55      0.62      0.58       172\n",
      "          12       0.73      0.77      0.75       182\n",
      "          13       0.66      0.64      0.65       151\n",
      "          14       0.58      0.64      0.61       200\n",
      "          15       0.78      0.88      0.83       169\n",
      "          16       0.81      0.99      0.89        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  13  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.9680 - accuracy: 0.9662 - precision: 0.7815 - recall: 0.5908 - val_loss: 1.1755 - val_accuracy: 0.9612 - val_precision: 0.7280 - val_recall: 0.5438\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.53      0.56        68\n",
      "           1       0.74      0.77      0.75       176\n",
      "           2       0.75      0.73      0.74        97\n",
      "           3       0.74      0.64      0.69        90\n",
      "           4       0.68      0.60      0.64        93\n",
      "           5       0.68      0.76      0.72       108\n",
      "           6       0.82      0.83      0.83       126\n",
      "           7       0.73      0.74      0.73       136\n",
      "           8       0.71      0.61      0.66       122\n",
      "           9       0.80      0.64      0.71       155\n",
      "          10       0.59      0.61      0.60        61\n",
      "          11       0.60      0.58      0.59       172\n",
      "          12       0.72      0.80      0.76       182\n",
      "          13       0.66      0.60      0.63       151\n",
      "          14       0.57      0.63      0.60       200\n",
      "          15       0.77      0.85      0.81       169\n",
      "          16       0.81      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.70      2183\n",
      "   macro avg       0.70      0.70      0.70      2183\n",
      "weighted avg       0.70      0.70      0.70      2183\n",
      "\n",
      "At iteration  14  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.9533 - accuracy: 0.9665 - precision: 0.7801 - recall: 0.5993 - val_loss: 1.0879 - val_accuracy: 0.9670 - val_precision: 0.7720 - val_recall: 0.6239\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.51      0.55        68\n",
      "           1       0.76      0.76      0.76       176\n",
      "           2       0.71      0.72      0.72        97\n",
      "           3       0.79      0.67      0.72        90\n",
      "           4       0.67      0.58      0.62        93\n",
      "           5       0.68      0.76      0.72       108\n",
      "           6       0.80      0.83      0.81       126\n",
      "           7       0.71      0.74      0.72       136\n",
      "           8       0.68      0.62      0.65       122\n",
      "           9       0.83      0.65      0.73       155\n",
      "          10       0.62      0.62      0.62        61\n",
      "          11       0.57      0.65      0.61       172\n",
      "          12       0.73      0.80      0.77       182\n",
      "          13       0.68      0.60      0.64       151\n",
      "          14       0.59      0.60      0.60       200\n",
      "          15       0.78      0.85      0.81       169\n",
      "          16       0.84      0.96      0.90        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.70      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  15  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.9369 - accuracy: 0.9668 - precision: 0.7789 - recall: 0.6083 - val_loss: 1.1278 - val_accuracy: 0.9659 - val_precision: 0.7710 - val_recall: 0.5976\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.51      0.53        68\n",
      "           1       0.76      0.76      0.76       176\n",
      "           2       0.74      0.75      0.75        97\n",
      "           3       0.74      0.70      0.72        90\n",
      "           4       0.70      0.56      0.62        93\n",
      "           5       0.66      0.77      0.71       108\n",
      "           6       0.82      0.83      0.83       126\n",
      "           7       0.73      0.75      0.74       136\n",
      "           8       0.70      0.54      0.61       122\n",
      "           9       0.81      0.68      0.74       155\n",
      "          10       0.63      0.62      0.63        61\n",
      "          11       0.58      0.64      0.61       172\n",
      "          12       0.73      0.80      0.76       182\n",
      "          13       0.69      0.65      0.67       151\n",
      "          14       0.57      0.60      0.59       200\n",
      "          15       0.79      0.85      0.82       169\n",
      "          16       0.85      0.96      0.90        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.70      0.71      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  16  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.9153 - accuracy: 0.9673 - precision: 0.7844 - recall: 0.6115 - val_loss: 1.1503 - val_accuracy: 0.9641 - val_precision: 0.7457 - val_recall: 0.5924\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.50      0.51        68\n",
      "           1       0.77      0.77      0.77       176\n",
      "           2       0.70      0.77      0.74        97\n",
      "           3       0.77      0.69      0.73        90\n",
      "           4       0.74      0.56      0.64        93\n",
      "           5       0.67      0.77      0.72       108\n",
      "           6       0.80      0.84      0.82       126\n",
      "           7       0.73      0.74      0.74       136\n",
      "           8       0.75      0.61      0.67       122\n",
      "           9       0.79      0.70      0.74       155\n",
      "          10       0.68      0.62      0.65        61\n",
      "          11       0.60      0.61      0.60       172\n",
      "          12       0.76      0.81      0.78       182\n",
      "          13       0.66      0.66      0.66       151\n",
      "          14       0.60      0.61      0.61       200\n",
      "          15       0.79      0.85      0.82       169\n",
      "          16       0.83      0.97      0.90        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.71      0.71      2183\n",
      "weighted avg       0.72      0.72      0.71      2183\n",
      "\n",
      "At iteration  17  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.9114 - accuracy: 0.9680 - precision: 0.7901 - recall: 0.6208 - val_loss: 1.1421 - val_accuracy: 0.9652 - val_precision: 0.7647 - val_recall: 0.5896\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55        68\n",
      "           1       0.76      0.77      0.76       176\n",
      "           2       0.70      0.73      0.72        97\n",
      "           3       0.77      0.66      0.71        90\n",
      "           4       0.68      0.56      0.62        93\n",
      "           5       0.66      0.77      0.71       108\n",
      "           6       0.83      0.84      0.83       126\n",
      "           7       0.70      0.79      0.74       136\n",
      "           8       0.73      0.59      0.65       122\n",
      "           9       0.81      0.66      0.73       155\n",
      "          10       0.66      0.66      0.66        61\n",
      "          11       0.59      0.59      0.59       172\n",
      "          12       0.73      0.80      0.76       182\n",
      "          13       0.69      0.65      0.67       151\n",
      "          14       0.61      0.62      0.62       200\n",
      "          15       0.78      0.86      0.82       169\n",
      "          16       0.83      0.97      0.90        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.71      0.71      0.71      2183\n",
      "weighted avg       0.71      0.71      0.71      2183\n",
      "\n",
      "At iteration  18  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8937 - accuracy: 0.9687 - precision: 0.7959 - recall: 0.6291 - val_loss: 1.1693 - val_accuracy: 0.9634 - val_precision: 0.7344 - val_recall: 0.5919\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.55        68\n",
      "           1       0.76      0.79      0.77       176\n",
      "           2       0.71      0.75      0.73        97\n",
      "           3       0.77      0.64      0.70        90\n",
      "           4       0.73      0.56      0.63        93\n",
      "           5       0.67      0.77      0.72       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.71      0.74      0.72       136\n",
      "           8       0.75      0.61      0.67       122\n",
      "           9       0.81      0.67      0.73       155\n",
      "          10       0.68      0.72      0.70        61\n",
      "          11       0.60      0.66      0.63       172\n",
      "          12       0.77      0.81      0.79       182\n",
      "          13       0.69      0.61      0.65       151\n",
      "          14       0.58      0.62      0.60       200\n",
      "          15       0.78      0.86      0.82       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.71      0.71      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  19  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8975 - accuracy: 0.9685 - precision: 0.7948 - recall: 0.6271 - val_loss: 1.1491 - val_accuracy: 0.9643 - val_precision: 0.7575 - val_recall: 0.5793\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.54      0.56        68\n",
      "           1       0.77      0.77      0.77       176\n",
      "           2       0.70      0.75      0.73        97\n",
      "           3       0.75      0.64      0.69        90\n",
      "           4       0.70      0.55      0.61        93\n",
      "           5       0.68      0.79      0.73       108\n",
      "           6       0.85      0.83      0.84       126\n",
      "           7       0.70      0.76      0.73       136\n",
      "           8       0.72      0.64      0.68       122\n",
      "           9       0.83      0.67      0.74       155\n",
      "          10       0.70      0.69      0.69        61\n",
      "          11       0.58      0.60      0.59       172\n",
      "          12       0.76      0.81      0.78       182\n",
      "          13       0.68      0.64      0.66       151\n",
      "          14       0.60      0.62      0.61       200\n",
      "          15       0.81      0.88      0.84       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.71      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  20  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.9016 - accuracy: 0.9679 - precision: 0.7873 - recall: 0.6233 - val_loss: 1.0792 - val_accuracy: 0.9655 - val_precision: 0.7603 - val_recall: 0.6027\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.55        68\n",
      "           1       0.76      0.78      0.77       176\n",
      "           2       0.73      0.74      0.74        97\n",
      "           3       0.78      0.63      0.70        90\n",
      "           4       0.71      0.55      0.62        93\n",
      "           5       0.67      0.79      0.73       108\n",
      "           6       0.80      0.85      0.83       126\n",
      "           7       0.74      0.72      0.73       136\n",
      "           8       0.73      0.62      0.67       122\n",
      "           9       0.79      0.64      0.71       155\n",
      "          10       0.77      0.61      0.68        61\n",
      "          11       0.59      0.63      0.61       172\n",
      "          12       0.77      0.80      0.78       182\n",
      "          13       0.64      0.64      0.64       151\n",
      "          14       0.59      0.64      0.61       200\n",
      "          15       0.77      0.89      0.83       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.72      0.71      0.71      2183\n",
      "weighted avg       0.72      0.71      0.71      2183\n",
      "\n",
      "At iteration  21  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8791 - accuracy: 0.9690 - precision: 0.7949 - recall: 0.6382 - val_loss: 1.0967 - val_accuracy: 0.9644 - val_precision: 0.7578 - val_recall: 0.5804\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.57      0.56        68\n",
      "           1       0.77      0.78      0.78       176\n",
      "           2       0.68      0.79      0.73        97\n",
      "           3       0.73      0.64      0.69        90\n",
      "           4       0.74      0.56      0.64        93\n",
      "           5       0.67      0.77      0.72       108\n",
      "           6       0.83      0.84      0.84       126\n",
      "           7       0.77      0.73      0.75       136\n",
      "           8       0.72      0.61      0.66       122\n",
      "           9       0.83      0.63      0.71       155\n",
      "          10       0.67      0.69      0.68        61\n",
      "          11       0.61      0.62      0.61       172\n",
      "          12       0.75      0.80      0.78       182\n",
      "          13       0.64      0.64      0.64       151\n",
      "          14       0.61      0.64      0.62       200\n",
      "          15       0.79      0.86      0.82       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.71      0.71      2183\n",
      "weighted avg       0.72      0.72      0.71      2183\n",
      "\n",
      "At iteration  22  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8543 - accuracy: 0.9694 - precision: 0.7963 - recall: 0.6448 - val_loss: 1.1202 - val_accuracy: 0.9654 - val_precision: 0.7503 - val_recall: 0.6159\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56        68\n",
      "           1       0.78      0.77      0.77       176\n",
      "           2       0.70      0.75      0.72        97\n",
      "           3       0.77      0.62      0.69        90\n",
      "           4       0.73      0.56      0.63        93\n",
      "           5       0.67      0.78      0.72       108\n",
      "           6       0.84      0.85      0.84       126\n",
      "           7       0.73      0.76      0.74       136\n",
      "           8       0.75      0.65      0.69       122\n",
      "           9       0.82      0.65      0.73       155\n",
      "          10       0.70      0.70      0.70        61\n",
      "          11       0.58      0.61      0.59       172\n",
      "          12       0.74      0.81      0.77       182\n",
      "          13       0.66      0.64      0.65       151\n",
      "          14       0.60      0.64      0.62       200\n",
      "          15       0.79      0.85      0.82       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  23  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8428 - accuracy: 0.9697 - precision: 0.7989 - recall: 0.6472 - val_loss: 1.1211 - val_accuracy: 0.9664 - val_precision: 0.7579 - val_recall: 0.6291\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54        68\n",
      "           1       0.76      0.79      0.78       176\n",
      "           2       0.73      0.77      0.75        97\n",
      "           3       0.76      0.61      0.68        90\n",
      "           4       0.68      0.58      0.63        93\n",
      "           5       0.69      0.79      0.73       108\n",
      "           6       0.82      0.86      0.84       126\n",
      "           7       0.73      0.71      0.72       136\n",
      "           8       0.75      0.63      0.69       122\n",
      "           9       0.80      0.66      0.73       155\n",
      "          10       0.69      0.72      0.70        61\n",
      "          11       0.59      0.60      0.60       172\n",
      "          12       0.75      0.80      0.77       182\n",
      "          13       0.68      0.64      0.66       151\n",
      "          14       0.58      0.62      0.60       200\n",
      "          15       0.79      0.86      0.82       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.71      0.71      2183\n",
      "weighted avg       0.72      0.72      0.71      2183\n",
      "\n",
      "At iteration  24  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8518 - accuracy: 0.9695 - precision: 0.7978 - recall: 0.6440 - val_loss: 1.1143 - val_accuracy: 0.9648 - val_precision: 0.7456 - val_recall: 0.6090\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56        68\n",
      "           1       0.76      0.77      0.77       176\n",
      "           2       0.76      0.76      0.76        97\n",
      "           3       0.74      0.63      0.68        90\n",
      "           4       0.69      0.58      0.63        93\n",
      "           5       0.68      0.80      0.73       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.71      0.77      0.74       136\n",
      "           8       0.78      0.61      0.68       122\n",
      "           9       0.80      0.64      0.71       155\n",
      "          10       0.65      0.72      0.68        61\n",
      "          11       0.62      0.60      0.61       172\n",
      "          12       0.73      0.79      0.76       182\n",
      "          13       0.68      0.63      0.65       151\n",
      "          14       0.57      0.63      0.60       200\n",
      "          15       0.80      0.86      0.83       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.71      2183\n",
      "   macro avg       0.72      0.71      0.71      2183\n",
      "weighted avg       0.72      0.71      0.71      2183\n",
      "\n",
      "At iteration  25  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8476 - accuracy: 0.9694 - precision: 0.7953 - recall: 0.6458 - val_loss: 1.1450 - val_accuracy: 0.9638 - val_precision: 0.7328 - val_recall: 0.6045\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.53      0.57        68\n",
      "           1       0.74      0.76      0.75       176\n",
      "           2       0.74      0.76      0.75        97\n",
      "           3       0.72      0.66      0.69        90\n",
      "           4       0.71      0.57      0.63        93\n",
      "           5       0.65      0.80      0.71       108\n",
      "           6       0.84      0.83      0.84       126\n",
      "           7       0.69      0.76      0.72       136\n",
      "           8       0.74      0.66      0.70       122\n",
      "           9       0.81      0.68      0.74       155\n",
      "          10       0.72      0.69      0.71        61\n",
      "          11       0.62      0.65      0.63       172\n",
      "          12       0.76      0.79      0.77       182\n",
      "          13       0.72      0.62      0.67       151\n",
      "          14       0.60      0.61      0.61       200\n",
      "          15       0.80      0.87      0.83       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  26  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8160 - accuracy: 0.9709 - precision: 0.8049 - recall: 0.6663 - val_loss: 1.1598 - val_accuracy: 0.9656 - val_precision: 0.7582 - val_recall: 0.6085\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.54      0.57        68\n",
      "           1       0.75      0.77      0.76       176\n",
      "           2       0.75      0.76      0.76        97\n",
      "           3       0.72      0.66      0.69        90\n",
      "           4       0.72      0.59      0.65        93\n",
      "           5       0.65      0.78      0.71       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.73      0.73      0.73       136\n",
      "           8       0.75      0.65      0.70       122\n",
      "           9       0.80      0.66      0.73       155\n",
      "          10       0.73      0.74      0.73        61\n",
      "          11       0.60      0.63      0.62       172\n",
      "          12       0.75      0.80      0.77       182\n",
      "          13       0.67      0.62      0.65       151\n",
      "          14       0.58      0.60      0.59       200\n",
      "          15       0.79      0.86      0.82       169\n",
      "          16       0.81      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  27  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8172 - accuracy: 0.9706 - precision: 0.8057 - recall: 0.6583 - val_loss: 1.1654 - val_accuracy: 0.9642 - val_precision: 0.7306 - val_recall: 0.6193\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.51      0.56        68\n",
      "           1       0.78      0.78      0.78       176\n",
      "           2       0.70      0.76      0.73        97\n",
      "           3       0.72      0.64      0.68        90\n",
      "           4       0.70      0.59      0.64        93\n",
      "           5       0.64      0.81      0.71       108\n",
      "           6       0.80      0.83      0.81       126\n",
      "           7       0.72      0.78      0.75       136\n",
      "           8       0.79      0.65      0.71       122\n",
      "           9       0.80      0.68      0.73       155\n",
      "          10       0.71      0.67      0.69        61\n",
      "          11       0.60      0.60      0.60       172\n",
      "          12       0.73      0.81      0.77       182\n",
      "          13       0.70      0.61      0.65       151\n",
      "          14       0.61      0.60      0.61       200\n",
      "          15       0.79      0.87      0.83       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.71      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  28  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8084 - accuracy: 0.9713 - precision: 0.8094 - recall: 0.6699 - val_loss: 1.0905 - val_accuracy: 0.9666 - val_precision: 0.7669 - val_recall: 0.6216\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.54      0.56        68\n",
      "           1       0.78      0.80      0.79       176\n",
      "           2       0.70      0.76      0.73        97\n",
      "           3       0.74      0.67      0.70        90\n",
      "           4       0.72      0.59      0.65        93\n",
      "           5       0.67      0.78      0.72       108\n",
      "           6       0.82      0.84      0.83       126\n",
      "           7       0.73      0.76      0.75       136\n",
      "           8       0.74      0.65      0.69       122\n",
      "           9       0.83      0.68      0.74       155\n",
      "          10       0.75      0.69      0.72        61\n",
      "          11       0.60      0.65      0.62       172\n",
      "          12       0.72      0.82      0.77       182\n",
      "          13       0.68      0.62      0.65       151\n",
      "          14       0.63      0.56      0.60       200\n",
      "          15       0.78      0.89      0.83       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  29  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7956 - accuracy: 0.9709 - precision: 0.8050 - recall: 0.6673 - val_loss: 1.0932 - val_accuracy: 0.9661 - val_precision: 0.7658 - val_recall: 0.6102\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.57      0.59        68\n",
      "           1       0.77      0.78      0.77       176\n",
      "           2       0.69      0.77      0.73        97\n",
      "           3       0.76      0.66      0.70        90\n",
      "           4       0.67      0.60      0.64        93\n",
      "           5       0.64      0.80      0.71       108\n",
      "           6       0.82      0.82      0.82       126\n",
      "           7       0.75      0.74      0.74       136\n",
      "           8       0.76      0.66      0.71       122\n",
      "           9       0.78      0.66      0.72       155\n",
      "          10       0.69      0.74      0.71        61\n",
      "          11       0.64      0.62      0.63       172\n",
      "          12       0.73      0.81      0.77       182\n",
      "          13       0.68      0.64      0.66       151\n",
      "          14       0.63      0.60      0.62       200\n",
      "          15       0.82      0.86      0.84       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  30  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7743 - accuracy: 0.9723 - precision: 0.8164 - recall: 0.6819 - val_loss: 1.1925 - val_accuracy: 0.9656 - val_precision: 0.7514 - val_recall: 0.6211\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        68\n",
      "           1       0.79      0.78      0.79       176\n",
      "           2       0.71      0.79      0.75        97\n",
      "           3       0.70      0.63      0.67        90\n",
      "           4       0.68      0.59      0.63        93\n",
      "           5       0.63      0.80      0.70       108\n",
      "           6       0.84      0.86      0.85       126\n",
      "           7       0.75      0.74      0.74       136\n",
      "           8       0.72      0.61      0.66       122\n",
      "           9       0.79      0.67      0.73       155\n",
      "          10       0.71      0.66      0.68        61\n",
      "          11       0.62      0.59      0.60       172\n",
      "          12       0.73      0.82      0.77       182\n",
      "          13       0.65      0.64      0.64       151\n",
      "          14       0.64      0.59      0.62       200\n",
      "          15       0.80      0.87      0.83       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.71      0.72      0.71      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  31  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8118 - accuracy: 0.9708 - precision: 0.8048 - recall: 0.6639 - val_loss: 1.0636 - val_accuracy: 0.9673 - val_precision: 0.7746 - val_recall: 0.6256\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.57      0.57        68\n",
      "           1       0.80      0.77      0.78       176\n",
      "           2       0.72      0.76      0.74        97\n",
      "           3       0.70      0.66      0.68        90\n",
      "           4       0.67      0.61      0.64        93\n",
      "           5       0.65      0.80      0.71       108\n",
      "           6       0.84      0.83      0.84       126\n",
      "           7       0.73      0.77      0.75       136\n",
      "           8       0.77      0.64      0.70       122\n",
      "           9       0.84      0.68      0.75       155\n",
      "          10       0.74      0.74      0.74        61\n",
      "          11       0.59      0.64      0.61       172\n",
      "          12       0.74      0.80      0.77       182\n",
      "          13       0.70      0.62      0.66       151\n",
      "          14       0.63      0.60      0.62       200\n",
      "          15       0.79      0.88      0.83       169\n",
      "          16       0.83      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.73      0.72      2183\n",
      "weighted avg       0.73      0.72      0.72      2183\n",
      "\n",
      "At iteration  32  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7686 - accuracy: 0.9726 - precision: 0.8191 - recall: 0.6856 - val_loss: 1.1466 - val_accuracy: 0.9648 - val_precision: 0.7348 - val_recall: 0.6297\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.59      0.62        68\n",
      "           1       0.79      0.78      0.79       176\n",
      "           2       0.71      0.77      0.74        97\n",
      "           3       0.74      0.63      0.68        90\n",
      "           4       0.71      0.57      0.63        93\n",
      "           5       0.67      0.81      0.74       108\n",
      "           6       0.83      0.83      0.83       126\n",
      "           7       0.73      0.76      0.75       136\n",
      "           8       0.73      0.69      0.71       122\n",
      "           9       0.80      0.68      0.74       155\n",
      "          10       0.72      0.70      0.71        61\n",
      "          11       0.64      0.62      0.63       172\n",
      "          12       0.74      0.81      0.77       182\n",
      "          13       0.70      0.62      0.66       151\n",
      "          14       0.61      0.64      0.62       200\n",
      "          15       0.82      0.87      0.84       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.73      2183\n",
      "   macro avg       0.73      0.73      0.73      2183\n",
      "weighted avg       0.73      0.73      0.73      2183\n",
      "\n",
      "At iteration  33  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7656 - accuracy: 0.9720 - precision: 0.8107 - recall: 0.6829 - val_loss: 1.1262 - val_accuracy: 0.9657 - val_precision: 0.7549 - val_recall: 0.6171\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.59      0.58        68\n",
      "           1       0.81      0.80      0.81       176\n",
      "           2       0.72      0.77      0.75        97\n",
      "           3       0.71      0.61      0.65        90\n",
      "           4       0.71      0.57      0.63        93\n",
      "           5       0.64      0.77      0.70       108\n",
      "           6       0.84      0.81      0.83       126\n",
      "           7       0.74      0.75      0.75       136\n",
      "           8       0.75      0.70      0.72       122\n",
      "           9       0.81      0.66      0.73       155\n",
      "          10       0.73      0.70      0.72        61\n",
      "          11       0.61      0.63      0.62       172\n",
      "          12       0.74      0.80      0.77       182\n",
      "          13       0.63      0.62      0.62       151\n",
      "          14       0.61      0.61      0.61       200\n",
      "          15       0.80      0.86      0.83       169\n",
      "          16       0.81      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  34  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7645 - accuracy: 0.9723 - precision: 0.8152 - recall: 0.6833 - val_loss: 1.1567 - val_accuracy: 0.9645 - val_precision: 0.7344 - val_recall: 0.6205\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57        68\n",
      "           1       0.78      0.76      0.77       176\n",
      "           2       0.70      0.75      0.72        97\n",
      "           3       0.72      0.64      0.68        90\n",
      "           4       0.71      0.57      0.63        93\n",
      "           5       0.66      0.80      0.72       108\n",
      "           6       0.84      0.82      0.83       126\n",
      "           7       0.73      0.78      0.75       136\n",
      "           8       0.72      0.69      0.70       122\n",
      "           9       0.80      0.68      0.74       155\n",
      "          10       0.70      0.70      0.70        61\n",
      "          11       0.61      0.61      0.61       172\n",
      "          12       0.72      0.84      0.78       182\n",
      "          13       0.70      0.61      0.65       151\n",
      "          14       0.63      0.60      0.62       200\n",
      "          15       0.81      0.87      0.84       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  35  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7633 - accuracy: 0.9722 - precision: 0.8142 - recall: 0.6827 - val_loss: 1.1711 - val_accuracy: 0.9652 - val_precision: 0.7397 - val_recall: 0.6297\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54        68\n",
      "           1       0.79      0.77      0.78       176\n",
      "           2       0.71      0.81      0.76        97\n",
      "           3       0.73      0.68      0.70        90\n",
      "           4       0.69      0.60      0.64        93\n",
      "           5       0.67      0.77      0.72       108\n",
      "           6       0.84      0.81      0.83       126\n",
      "           7       0.74      0.77      0.76       136\n",
      "           8       0.76      0.64      0.69       122\n",
      "           9       0.79      0.67      0.73       155\n",
      "          10       0.72      0.69      0.71        61\n",
      "          11       0.61      0.60      0.61       172\n",
      "          12       0.74      0.82      0.78       182\n",
      "          13       0.69      0.60      0.64       151\n",
      "          14       0.60      0.64      0.62       200\n",
      "          15       0.81      0.88      0.84       169\n",
      "          16       0.83      0.97      0.90        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  36  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7585 - accuracy: 0.9722 - precision: 0.8115 - recall: 0.6878 - val_loss: 1.1698 - val_accuracy: 0.9654 - val_precision: 0.7500 - val_recall: 0.6182\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.56      0.56        68\n",
      "           1       0.77      0.80      0.78       176\n",
      "           2       0.71      0.77      0.74        97\n",
      "           3       0.72      0.64      0.68        90\n",
      "           4       0.69      0.58      0.63        93\n",
      "           5       0.69      0.74      0.71       108\n",
      "           6       0.87      0.82      0.84       126\n",
      "           7       0.74      0.77      0.76       136\n",
      "           8       0.76      0.69      0.72       122\n",
      "           9       0.82      0.66      0.73       155\n",
      "          10       0.74      0.74      0.74        61\n",
      "          11       0.63      0.56      0.59       172\n",
      "          12       0.71      0.81      0.76       182\n",
      "          13       0.70      0.62      0.65       151\n",
      "          14       0.60      0.67      0.63       200\n",
      "          15       0.80      0.89      0.84       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.73      0.72      0.72      2183\n",
      "weighted avg       0.73      0.72      0.72      2183\n",
      "\n",
      "At iteration  37  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7451 - accuracy: 0.9732 - precision: 0.8211 - recall: 0.6952 - val_loss: 1.1298 - val_accuracy: 0.9659 - val_precision: 0.7471 - val_recall: 0.6359\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60        68\n",
      "           1       0.78      0.80      0.79       176\n",
      "           2       0.71      0.75      0.73        97\n",
      "           3       0.72      0.64      0.68        90\n",
      "           4       0.71      0.57      0.63        93\n",
      "           5       0.70      0.79      0.74       108\n",
      "           6       0.86      0.83      0.85       126\n",
      "           7       0.70      0.77      0.74       136\n",
      "           8       0.75      0.68      0.71       122\n",
      "           9       0.81      0.64      0.71       155\n",
      "          10       0.74      0.69      0.71        61\n",
      "          11       0.60      0.60      0.60       172\n",
      "          12       0.75      0.80      0.77       182\n",
      "          13       0.66      0.61      0.63       151\n",
      "          14       0.58      0.65      0.61       200\n",
      "          15       0.83      0.88      0.85       169\n",
      "          16       0.82      0.97      0.89        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  38  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7244 - accuracy: 0.9732 - precision: 0.8165 - recall: 0.7026 - val_loss: 1.1817 - val_accuracy: 0.9644 - val_precision: 0.7391 - val_recall: 0.6096\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.54      0.58        68\n",
      "           1       0.80      0.78      0.79       176\n",
      "           2       0.72      0.79      0.75        97\n",
      "           3       0.71      0.64      0.67        90\n",
      "           4       0.76      0.57      0.65        93\n",
      "           5       0.64      0.78      0.70       108\n",
      "           6       0.85      0.83      0.84       126\n",
      "           7       0.71      0.79      0.75       136\n",
      "           8       0.73      0.66      0.69       122\n",
      "           9       0.79      0.67      0.73       155\n",
      "          10       0.73      0.70      0.72        61\n",
      "          11       0.63      0.62      0.63       172\n",
      "          12       0.72      0.80      0.76       182\n",
      "          13       0.66      0.62      0.64       151\n",
      "          14       0.62      0.62      0.62       200\n",
      "          15       0.81      0.87      0.84       169\n",
      "          16       0.81      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.72      2183\n",
      "   macro avg       0.72      0.72      0.72      2183\n",
      "weighted avg       0.72      0.72      0.72      2183\n",
      "\n",
      "At iteration  39  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7296 - accuracy: 0.9728 - precision: 0.8166 - recall: 0.6931 - val_loss: 1.1922 - val_accuracy: 0.9640 - val_precision: 0.7266 - val_recall: 0.6222\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.54      0.60        68\n",
      "           1       0.79      0.78      0.79       176\n",
      "           2       0.69      0.77      0.73        97\n",
      "           3       0.73      0.62      0.67        90\n",
      "           4       0.73      0.56      0.63        93\n",
      "           5       0.66      0.79      0.72       108\n",
      "           6       0.84      0.84      0.84       126\n",
      "           7       0.74      0.77      0.76       136\n",
      "           8       0.73      0.68      0.71       122\n",
      "           9       0.82      0.68      0.74       155\n",
      "          10       0.76      0.67      0.71        61\n",
      "          11       0.63      0.60      0.61       172\n",
      "          12       0.73      0.81      0.77       182\n",
      "          13       0.64      0.64      0.64       151\n",
      "          14       0.63      0.68      0.65       200\n",
      "          15       0.81      0.88      0.84       169\n",
      "          16       0.82      0.96      0.89        77\n",
      "\n",
      "    accuracy                           0.73      2183\n",
      "   macro avg       0.73      0.72      0.72      2183\n",
      "weighted avg       0.73      0.73      0.72      2183\n",
      "\n",
      "At iteration  40  we get these data :\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7232 - accuracy: 0.9734 - precision: 0.8177 - recall: 0.7039 - val_loss: 1.1297 - val_accuracy: 0.9668 - val_precision: 0.7550 - val_recall: 0.6457\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.63        68\n",
      "           1       0.78      0.77      0.78       176\n",
      "           2       0.67      0.78      0.72        97\n",
      "           3       0.68      0.59      0.63        90\n",
      "           4       0.66      0.58      0.62        93\n",
      "           5       0.64      0.79      0.71       108\n",
      "           6       0.86      0.86      0.86       126\n",
      "           7       0.76      0.79      0.77       136\n",
      "           8       0.77      0.70      0.73       122\n",
      "           9       0.82      0.66      0.73       155\n",
      "          10       0.73      0.72      0.73        61\n",
      "          11       0.64      0.63      0.63       172\n",
      "          12       0.73      0.81      0.77       182\n",
      "          13       0.65      0.60      0.63       151\n",
      "          14       0.64      0.64      0.64       200\n",
      "          15       0.80      0.89      0.84       169\n",
      "          16       0.81      0.96      0.88        77\n",
      "\n",
      "    accuracy                           0.73      2183\n",
      "   macro avg       0.73      0.73      0.72      2183\n",
      "weighted avg       0.73      0.73      0.72      2183\n",
      "\n",
      "At iteration  41  we get these data :\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7221 - accuracy: 0.9740 - precision: 0.8257 - recall: 0.7074 - val_loss: 1.1548 - val_accuracy: 0.9652 - val_precision: 0.7424 - val_recall: 0.6251\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.57      0.59        68\n",
      "           1       0.80      0.78      0.79       176\n",
      "           2       0.73      0.79      0.76        97\n",
      "           3       0.70      0.62      0.66        90\n",
      "           4       0.71      0.58      0.64        93\n",
      "           5       0.66      0.79      0.72       108\n",
      "           6       0.85      0.84      0.84       126\n",
      "           7       0.77      0.79      0.78       136\n",
      "           8       0.79      0.66      0.72       122\n",
      "           9       0.81      0.71      0.76       155\n",
      "          10       0.70      0.77      0.73        61\n",
      "          11       0.65      0.63      0.64       172\n",
      "          12       0.74      0.80      0.77       182\n",
      "          13       0.67      0.63      0.65       151\n",
      "          14       0.64      0.66      0.65       200\n",
      "          15       0.82      0.89      0.85       169\n",
      "          16       0.80      0.97      0.88        77\n",
      "\n",
      "    accuracy                           0.74      2183\n",
      "   macro avg       0.73      0.73      0.73      2183\n",
      "weighted avg       0.74      0.74      0.73      2183\n",
      "\n",
      "At iteration  42  we get these data :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/219 [=========================>....] - ETA: 0s - loss: 0.7144 - accuracy: 0.9740 - precision: 0.8261 - recall: 0.7061"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt iteration \u001b[39m\u001b[38;5;124m\"\u001b[39m  ,i , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m we get these data :\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m----> 5\u001b[0m     model2\u001b[38;5;241m.\u001b[39mfit(X_train , y_train1 , batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m , epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m , validation_split\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.2\u001b[39m) ,verbose \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(model2\u001b[38;5;241m.\u001b[39mpredict(X_test), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(y_test\u001b[38;5;241m.\u001b[39mvalues , predictions))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1856\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1842\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1843\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1854\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1855\u001b[0m     )\n\u001b[1;32m-> 1856\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m   1857\u001b[0m     x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1858\u001b[0m     y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   1859\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight,\n\u001b[0;32m   1860\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mvalidation_batch_size \u001b[38;5;129;01mor\u001b[39;00m batch_size,\n\u001b[0;32m   1861\u001b[0m     steps\u001b[38;5;241m=\u001b[39mvalidation_steps,\n\u001b[0;32m   1862\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1863\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[0;32m   1864\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[0;32m   1865\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[0;32m   1866\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1867\u001b[0m     _use_cached_eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1868\u001b[0m )\n\u001b[0;32m   1869\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1871\u001b[0m }\n\u001b[0;32m   1872\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:2296\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   2293\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2294\u001b[0m             ):\n\u001b[0;32m   2295\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2296\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m test_function_runner\u001b[38;5;241m.\u001b[39mrun_step(\n\u001b[0;32m   2297\u001b[0m                     dataset_or_iterator,\n\u001b[0;32m   2298\u001b[0m                     data_handler,\n\u001b[0;32m   2299\u001b[0m                     step,\n\u001b[0;32m   2300\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   2301\u001b[0m                 )\n\u001b[0;32m   2303\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2304\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:4108\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[1;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[0;32m   4107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[1;32m-> 4108\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function(dataset_or_iterator)\n\u001b[0;32m   4109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   4110\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    878\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    print( \"At iteration \"  ,i , \" we get these data :\" )\n",
    "    \n",
    "    model2.fit(X_train , y_train1 , batch_size = 32 , epochs = 1 , validation_split=(0.2) ,verbose =1)\n",
    "\n",
    "    predictions = np.argmax(model2.predict(X_test), axis=-1)\n",
    "\n",
    "    print(classification_report(y_test.values , predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7833fed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:17:17.244873Z",
     "start_time": "2024-02-10T14:17:17.234658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9caca87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:18:02.422237Z",
     "start_time": "2024-02-10T14:18:02.416490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said',\n",
       " 'would',\n",
       " 'even',\n",
       " 'according',\n",
       " 'could',\n",
       " 'year',\n",
       " 'years',\n",
       " 'also',\n",
       " 'new',\n",
       " 'people',\n",
       " 'old,one',\n",
       " 'two',\n",
       " 'time',\n",
       " 'first',\n",
       " 'last',\n",
       " 'say',\n",
       " 'make',\n",
       " 'best',\n",
       " 'get',\n",
       " 'three',\n",
       " 'make',\n",
       " 'year old',\n",
       " 'told',\n",
       " 'made',\n",
       " 'like',\n",
       " 'take',\n",
       " 'many',\n",
       " 'set',\n",
       " 'number',\n",
       " 'month',\n",
       " 'week',\n",
       " 'well',\n",
       " 'back']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c063d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:39:57.690443Z",
     "start_time": "2024-02-10T14:39:57.677445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>category_level_1</th>\n",
       "      <th>category_level_2</th>\n",
       "      <th>Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1809</td>\n",
       "      <td>crime, law and justice</td>\n",
       "      <td>crime</td>\n",
       "      <td>virginia woman whose 2 old son found trash inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980</td>\n",
       "      <td>crime, law and justice</td>\n",
       "      <td>crime</td>\n",
       "      <td>author tri determin anyon help inmat escap cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995</td>\n",
       "      <td>crime, law and justice</td>\n",
       "      <td>crime</td>\n",
       "      <td>13 old suspect doubl homicid escap custodi ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2740</td>\n",
       "      <td>crime, law and justice</td>\n",
       "      <td>crime</td>\n",
       "      <td>mother young children found hang pennsylvania ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7038</td>\n",
       "      <td>crime, law and justice</td>\n",
       "      <td>crime</td>\n",
       "      <td>one famili member derek violent attack rex pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10910</th>\n",
       "      <td>907640</td>\n",
       "      <td>conflict, war and peace</td>\n",
       "      <td>post-war reconstruction</td>\n",
       "      <td>origin publish site beirut lebanon 10 30 deput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10911</th>\n",
       "      <td>892720</td>\n",
       "      <td>conflict, war and peace</td>\n",
       "      <td>post-war reconstruction</td>\n",
       "      <td>origin publish site kiev octob 12 tass david a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912</th>\n",
       "      <td>870499</td>\n",
       "      <td>conflict, war and peace</td>\n",
       "      <td>post-war reconstruction</td>\n",
       "      <td>detail 2019 07 06 600319 iran support iraq rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913</th>\n",
       "      <td>887334</td>\n",
       "      <td>conflict, war and peace</td>\n",
       "      <td>post-war reconstruction</td>\n",
       "      <td>detail 2019 09 25 607114 iraq salih terror ori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10914</th>\n",
       "      <td>885988</td>\n",
       "      <td>conflict, war and peace</td>\n",
       "      <td>post-war reconstruction</td>\n",
       "      <td>http tass com polit 1079130 origin publish sit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10915 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       data_id         category_level_1         category_level_2  \\\n",
       "0         1809   crime, law and justice                    crime   \n",
       "1         1980   crime, law and justice                    crime   \n",
       "2         1995   crime, law and justice                    crime   \n",
       "3         2740   crime, law and justice                    crime   \n",
       "4         7038   crime, law and justice                    crime   \n",
       "...        ...                      ...                      ...   \n",
       "10910   907640  conflict, war and peace  post-war reconstruction   \n",
       "10911   892720  conflict, war and peace  post-war reconstruction   \n",
       "10912   870499  conflict, war and peace  post-war reconstruction   \n",
       "10913   887334  conflict, war and peace  post-war reconstruction   \n",
       "10914   885988  conflict, war and peace  post-war reconstruction   \n",
       "\n",
       "                                                 Cleaned  \n",
       "0      virginia woman whose 2 old son found trash inc...  \n",
       "1      author tri determin anyon help inmat escap cal...  \n",
       "2      13 old suspect doubl homicid escap custodi ret...  \n",
       "3      mother young children found hang pennsylvania ...  \n",
       "4      one famili member derek violent attack rex pas...  \n",
       "...                                                  ...  \n",
       "10910  origin publish site beirut lebanon 10 30 deput...  \n",
       "10911  origin publish site kiev octob 12 tass david a...  \n",
       "10912  detail 2019 07 06 600319 iran support iraq rec...  \n",
       "10913  detail 2019 09 25 607114 iraq salih terror ori...  \n",
       "10914  http tass com polit 1079130 origin publish sit...  \n",
       "\n",
       "[10915 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89381eac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:21:11.830005Z",
     "start_time": "2024-02-10T14:21:11.820005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'said',\n",
       " 'would',\n",
       " 'even',\n",
       " 'according',\n",
       " 'could',\n",
       " 'year',\n",
       " 'years',\n",
       " 'also',\n",
       " 'new',\n",
       " 'people',\n",
       " 'old,one',\n",
       " 'two',\n",
       " 'time',\n",
       " 'first',\n",
       " 'last',\n",
       " 'say',\n",
       " 'make',\n",
       " 'best',\n",
       " 'get',\n",
       " 'three',\n",
       " 'make',\n",
       " 'year old',\n",
       " 'told',\n",
       " 'made',\n",
       " 'like',\n",
       " 'take',\n",
       " 'many',\n",
       " 'set',\n",
       " 'number',\n",
       " 'month',\n",
       " 'week',\n",
       " 'well',\n",
       " 'back',\n",
       " 'post',\n",
       " 'http',\n",
       " 'www',\n",
       " 'presstv',\n",
       " 'ir']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad621b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
